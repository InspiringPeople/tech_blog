{
  
    
        "post0": {
            "title": "Bert",
            "content": "BERT . By Google (2018.10 논문이 공개, 11월에 오픈 소스로 공개 됨) . | BERT는 Bidirectional Encoder Representations from Transformers . | “Attention is all you need” (2017년 논문)에서 소개한 Transformer의 Encoder 부분 사용 . | Github : https://github.com/google-research/bert . | Paper : https://arxiv.org/abs/1810.04805 . | . . [Transformer Architecture] . 1) Transformer Encoder . 2) Transformer Decoder . Bert에서는 Transformer의 Encoder Block만을 사용 . Bert base 모델은 Encoder Block을 12개, Bert large 모델은 24개 쌓아서 만듬. . (Transformer의 Decoder Block을 쓰는 모델은 GPT 계열 모델, Encoder / Decoder Block을 모두 쓰는 모델은 T5 등이 있다) . 주요 개념 . Bert의 자세한 내용을 살펴보기 전에, Bert를 이해하기 위한 주요 개념들에 대해 소개한다. . Transfer Learning . . 패러다임의 전환! . BERT는 기본적으로, wiki나 book data와 같은 대용량 unlabeled data로 모델을 미리 학습 시킨 후, 특정 task를 가지고 있는 labeled data로 transfer learning을 하는 모델이다. 이때 unlabeled data로 학습하는 과정을 pre-training (upstream task), labeled data로 학습하는 과정을 fine-tuning (downstream task)라고 지칭한다. | . . Bert의 Pre-training / Fine-tuning . Bert는 wiki, book corpus 데이터로 pre-training을 수행하여 General purpose의 언어모델 (Language Model)을 학습하고, 다양한 task에 따라 (Glue, Super Glue 등) fine-tuning 모델을 생성하는 방식이다. 이로써 적은 labeling data로도 SOTA 모델을 만들 수 있다. . Language Model은 어떤 방식으로 문맥을 학습할까? (Transformer 계열 모델) . Attention . Attention이란? . 인간은 정보처리를 할 때 모든 sequence를 고려하지 않음. 찾아야 하는 주요 정보가 무엇인지 알 수 있다면 이에 가중치를 주어, 주요 정보를 중심으로 맥락을 파악할 수 있음. Attention의 모티브는 자신에게 의미 있는 feature만 골라내서 중요하게 판단하겠다는 것임. . 논문 : Neural Machine Translation by Jointly Learning to Align and Translate (https://arxiv.org/abs/1409.0473), 조경현 교수님 . | 어텐션 매커니즘은 기계번역(machine translation)을 위한 sequence-to-sequence 모델(S2S)에 처음 도입 . | 번역 시 소스랭귀지와 타겟랭귀지의 길이가 길어질 수록 모델의 성능이 나빠짐, 이를 방지하기 위해 모델로 하여금 ‘중요한 부분만 집중(attention)하게 만들자’는 것이 어텐션 매커니즘의 핵심 아이디어 . | 어텐션은 weights의 중요도 벡터, 이미지의 픽셀값이나 문장에서 단어 등 어떤 요소를 예측하거나 추정하기 위해, 다른 요소들과 얼마나 강하게 연관되어 있는지 확인하고 어텐션 백터로 가중 합산된 값의 합계를 타겟값으로 추정 . | . Input 중에서 무엇이 중요한지는 attention weight로 학습이 가능하며, input과 output이 어떻게 연결되어 있는지 시각화가 가능함. (어떤 feature들이 연결되어 있는지 알 수 있음) . . 기존 Attention 방식의 단점 . 번역모델(Seq2Seq) Attention 메커니즘의 핵심은 decoder의 특정 time-step의 output이 encoder의 모든 time-step의 output 중 어떤 time-step과 가장 연관이 있는가임. 일반적인 Seq2Seq-Attention 모델에서의 번역 태스크의 문제는 원본 언어(Source Language), 번역된 언어(Target Language)간의 대응 여부는 Attention을 통해 찾을 수 있었음. (input 나는 소년이다 &lt;-&gt; output I am a boy 간에 ‘나’ 와 ‘I’, ‘소년’과 ‘boy’의 대응관계) . | 그러나 각 문장에서의 관계를 나타낼 수 없음. ‘I love tiger but it is scare’에서 ‘it’이 무엇을 나타내는지 와 같은 문제는 기존 Encoder-Decoder 기반의 Attention메커니즘에서는 찾을 수 없었음. . | Attention 모델은 Seq2seq의 성능을 높였으나, RNN이 순차로 이뤄져 연산이 느리다는 구조적인 문제점이 있음. 또한 병렬처리가 불가능함. 이런 단점을 해결하기 위해 RNN을 없애는 아이디어가 나오게 됨. . | . Self-attention . Self-attention이란? . 2017년 구글이 발표한 논문인 “Attention is all you need” (https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)에서 제안된 transformer model에서 소개된 방법 . | 기존의 seq2seq의 구조인 인코더-디코더 따르면서도 순환신경만 기반으로 구성된 기존 모델과 다르게 단순히 어텐션 구조만으로 전체 모델을 만들어 어텐션 기법의 중요성을 강조 . | 문장 내에서 token들이 스스로를 잘 표현하는 context vector를 갖게 만듦. . | . . 밑줄 친 부분에 따라 it에 대한 가중치가 달라짐 . Bert has multi-head attention : Bert is multi-headed beast! . Bert base 모델의 경우 Layer 12, attention head 12개 architecture를 가지고 있다. . | 따라서 12*12 = 144, 144개의 distinct attention을 갖게 됨 . | . . L2, H0: Attention to next word . . L6, H11: Attention to previous word . . L2, H1: Attention to other words predictive of word . 기존의 word2vec 같은 경우 하나의 단어는 하나의 vector 값만을 표현하였지만, transformer 계열 모델의 경우 multi-headed attention을 이용함으로써 하나의 단어가 여러 개의 “representation 공간”을 가질 수 있게 됨. . 2. Pre-training . Bert의 pre-training 과정은 크게 Input Embedding, MLM, NSP 3가지 과정으로 나눠진다. . Pre-training data (BERT base) . BookCorpus (Zhu et al., 2015) (800M words) + English Wikipedia (2,500M words) . | Unsupervised Learning으로 진행됨 . | long contiguous sequence만을 학습시키기 위해 Wikipedia는 text passage만 추출하여 사용 . | . . BERT의 Pre-training과 Fine-tuning . BERT의 Pre-training은 tokenization을 수행하여 embedding 된 Sentence 2개를 Input Data로 사용한다. (Input Data 생성에 대해서는 후술) Input 값은 Transformer layer를 거치며 문장 내에서 단어(token)스스로를 가장 잘 표현하는 Self-attention된 token vector(Contextual representation of token), 그리고 후술할 NSP(Next Sentence Prediction)와 MLM(Masked Language Model)을 거치며 언어의 맥락 정보를 저장한 모델을 구축함. . 이 후 Fine-tuning 과정에서는 pre-training 된 모델을 로드하여, MNLI, NER, SQuAD와 같은 다양한 NLP Classification task를 수행하는 labeled train data를 학습시켜 분류를 하게 됨. . 2) Tokenization . Subword segmentation . 단어 분리(Subword segmenation) 작업은 하나의 단어는 (단어보다 작은 단위의) 의미있는 여러 내부 단어들(subwords)의 조합으로 구성된 경우가 많기 때문에, 하나의 단어를 여러 내부 단어로 분리해서 단어를 이해해보겠다는 의도를 가진 전처리 작업 . | 더 적은 vocab으로 더 많은 단어를 표현하고 out of vocabulary 방지 . | . Bert의 Tokenization은 Subword segmentation의 하나인 WPM(word piece model, by Google) 기반으로 tokenization 된다. . Bert Tokenizer 모음 . Tokenization 과정 . Space 단위로 token 분리 . | Pretrained 된 WPM tokenizer 기준으로 subwords segmentation . | 문장 시작에 [CLS], 문장 끝에 [SEP], max_seq_len에 따라 [PAD] special token 붙이기 . | . 3) Input Embedding . Tokenization된 문장은 Token Embeddings, Sentence Embeddings, Positional Embedings를 합쳐 모델에 입력할 수 있는 Input Embedding 값으로 변환된다. . . BERT는 Input data로 3 가지의 vector들이 합쳐서 만들어짐 . 1) Token Embeddings : Input에 들어갈 2개의 문장을 단어 token 단위로 자르고 token을 수학적으로 표현함. 이후 NSL와 MLM을 실시하기 위해, sentence 순서를 50% 확률로 변경하고, 15%의 token을 masking함. Masked token의 80%는 masked([MSK])되고, 10%는 그대로, 10%는 다른 token으로 치환하게 됨. special token으로는 문장 가장 앞([CLS])과 문장이 분리되는 부분([SEP])에 들어가게 됨. . 2) Segment Embeddings : 문장의 순서를 Embedding함. 그림 상에서 A, B로 표현됨. . 3) Position Embeddings : 단어의 순서를 Embedding함. (Transformer는 RNN이나 CNN처럼 data를 순차적으로 연산하지 않기 때문에 단어와 문장의 순서정보를 token에 별도로 넣어줘야 함) 또한 같은 단어라도 쓰인 위치에 따라 다른 임베딩 값을 가질 수 있음 . . 4 )MLM (Masked Language Model) . token embedding 과정에서 각 문장 token을 15% 확률로 masked 하게 되며, (문맥을 계산하여) masked token이 무엇인지를 예측하는 학습을 진행함. . . 입력 token의 15%를 선택하여 80% [MASK], 10% 랜덤토큰대체, 10% 유지 . . 5) NSP (Next Sentence Prediction) . NSP를 위해 corpus에서 실제 next sentence와 random sentence를 50% 확률로 가져오게 되며, 이를 가지고 IsNext인지 NotNext인지를 예측하는 학습을 진행. (Segment embedding으로 문장의 순서 정보를 보존) . sentence_a = “신용 대출을 신청 하고 싶어요” . sentence_b = “대출 관련 서류를 준비해 오셨어요?” . Label: IsNext . sentence_a = “신용 대출을 신청 하고 싶어요” . sentence_b = ”어떤 팀이 올해 프로야구 우승 했나요?” . Label: NotNext . | . MLM &amp; NSP 방식 . . 3. Fine-tuning(Classification Layer) . Pre-training된 모델을 불러와 NLP task를 하는데 사용할 수 있음. 크게 4가지 유형으로 구분된다. . 모델은 pre-trained parameters로 초기화 되고 각각 downstream task 별로 다른 모델이 만들어진다. 문맥을 이해하는 Language Model위에 fine-tuning 방식으로 학습되므로 적은 label 데이터로도 높은 정확도를 갖는 모델을 생성할 수 있다. . . (a), (b)는 sequence-level task, (c)와 (d)는 token-level task로 각각의 특징은 다음과 같음. . (a)는 2개 sentence를 넣고 그 관계(pair classification)를 분류하는 task임. 문장의 유사 관계 분류 등에 사용되며 (b)는 1개 sentence를 넣고 감성과 같은 문장의 특성을 분류함 . (c)는 질문(Question)과 본문(Paragraph)을 넣고 문장 단위의 답을 호출함. paragraph에서 Q와 유사도가 높은 문장을 답으로 내놓기 위해, [SEP] token 이후의 00번째 token부터 00번째 token까지를 호출함. . d)는 1개 문장을 넣고, 문장 내 token의 정보를 분류함. NER(Named Entity Recognition, 개체명 인식)이나 형태소 분석과 같이 single sentence에서 각 token이 어떤 class를 갖는지 모두 classifier 적용하여 정답을 찾아냄. .",
            "url": "https://inspiringpeople.github.io/tech_blog/2020/09/10/BERT.html",
            "relUrl": "/2020/09/10/BERT.html",
            "date": " • Sep 10, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "# !pwd %cd /home/jovyan/AI_Master/Flex_NLP/ . /home/jovyan/AI_Master/Flex_NLP . from config import * import numpy as np import pandas as pd from pathlib import Path import os import torch import torch.optim as optim import random # fastai from fastai import * from fastai.text import * from fastai.callbacks import * from transformers import AdamW from functools import partial # transformers import fastai import transformers from flex_transformer import * from transformers import AdamW from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig from transformers import BertForSequenceClassification, BertTokenizer, BertConfig from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig from transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig from transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig from dataMgr import * &quot;&quot;&quot;# Model Configuration # model_class.pretrained_model_archive_map.keys() [bert] dict_keys([&#39;bert-base-uncased&#39;, &#39;bert-large-uncased&#39;, &#39;bert-base-cased&#39;, &#39;bert-large-cased&#39;, &#39;bert-base-multilingual-uncased&#39;, &#39;bert-base-multilingual-cased&#39;, &#39;bert-base-chinese&#39;, &#39;bert-base-german-cased&#39;, &#39;bert-large-uncased-whole-word-masking&#39;, &#39;bert-large-cased-whole-word-masking&#39;, &#39;bert-large-uncased-whole-word-masking-finetuned-squad&#39;, &#39;bert-large-cased-whole-word-masking-finetuned-squad&#39;, &#39;bert-base-cased-finetuned-mrpc&#39;, &#39;bert-base-german-dbmdz-cased&#39;, &#39;bert-base-german-dbmdz-uncased&#39;, &#39;bert-base-japanese&#39;, &#39;bert-base-japanese-whole-word-masking&#39;, &#39;bert-base-japanese-char&#39;, &#39;bert-base-japanese-char-whole-word-masking&#39;, &#39;bert-base-finnish-cased-v1&#39;, &#39;bert-base-finnish-uncased-v1&#39;, &#39;bert-base-dutch-cased&#39;]) [xlnet] dict_keys([&#39;xlnet-base-cased&#39;, &#39;xlnet-large-cased&#39;]) [xlm] dict_keys([&#39;xlm-mlm-en-2048&#39;, &#39;xlm-mlm-ende-1024&#39;, &#39;xlm-mlm-enfr-1024&#39;, &#39;xlm-mlm-enro-1024&#39;, &#39;xlm-mlm-tlm-xnli15-1024&#39;, &#39;xlm-mlm-xnli15-1024&#39;, &#39;xlm-clm-enfr-1024&#39;, &#39;xlm-clm-ende-1024&#39;, &#39;xlm-mlm-17-1280&#39;, &#39;xlm-mlm-100-1280&#39;]) [roberta] dict_keys([&#39;roberta-base&#39;, &#39;roberta-large&#39;, &#39;roberta-large-mnli&#39;, &#39;distilroberta-base&#39;, &#39;roberta-base-openai-detector&#39;, &#39;roberta-large-openai-detector&#39;]) [distilbert] dict_keys([&#39;distilbert-base-uncased&#39;, &#39;distilbert-base-uncased-distilled-squad&#39;, &#39;distilbert-base-cased&#39;, &#39;distilbert-base-cased-distilled-squad&#39;, &#39;distilbert-base-german-cased&#39;, &#39;distilbert-base-multilingual-cased&#39;, &#39;distilbert-base-uncased-finetuned-sst-2-english&#39;]) &quot;&quot;&quot; MODEL_CLASSES = { &#39;bert&#39;: (BertForSequenceClassification, BertTokenizer, BertConfig), # bert-base-uncased &#39;xlnet&#39;: (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig), # xlnet-base-cased&#39; &#39;xlm&#39;: (XLMForSequenceClassification, XLMTokenizer, XLMConfig), # xlm-mlm-en-2048 &#39;roberta&#39;: (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig), # distilroberta-base &#39;distilbert&#39;: (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig) # distilbert-base-uncased } def load_pretrained_model(model_type, pretrained_model_name): model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type] config = config_class.from_pretrained(pretrained_model_name) # config.num_labels = n_label config.num_labels = 5 transformer_model = model_class.from_pretrained(pretrained_model_name, config = config) # transformer_model = model_class.from_pretrained(pretrained_model_name, num_labels = 5) custom_transformer_model = CustomTransformerModel(transformer_model = transformer_model) return custom_transformer_model def fine_tuned_model(custom_transformer_model, databunch, model_path): CustomAdamW = partial(AdamW, correct_bias=False) learner = Learner(databunch, custom_transformer_model, opt_func = CustomAdamW, metrics=[accuracy, error_rate]) # For DistilBERT list_layers = [learner.model.transformer.distilbert.embeddings, learner.model.transformer.distilbert.transformer.layer[0], learner.model.transformer.distilbert.transformer.layer[1], learner.model.transformer.distilbert.transformer.layer[2], learner.model.transformer.distilbert.transformer.layer[3], learner.model.transformer.distilbert.transformer.layer[4], learner.model.transformer.distilbert.transformer.layer[5], learner.model.transformer.pre_classifier] learner.split(list_layers) num_groups = len(learner.layer_groups) print(&#39;Learner split in&#39;,num_groups,&#39;groups&#39;) print(learner.layer_groups) ## Train learner.save(&#39;untrain&#39;) seed_all(seed_val) learner.load(&#39;untrain&#39;) learner.freeze_to(-1) learner.summary() learner.lr_find() # learner.recorder.plot(skip_end=10,suggestion=True) learner.fit_one_cycle(1,max_lr=2e-03,moms=(0.8,0.7)) learner.save(&#39;first_cycle&#39;) seed_all(seed_val) learner.load(&#39;first_cycle&#39;) learner.freeze_to(-2) lr = 1e-5 learner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9)) learner.save(&#39;second_cycle&#39;) seed_all(seed_val) learner.load(&#39;second_cycle&#39;) learner.freeze_to(-3) learner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9)) learner.save(&#39;third_cycle&#39;) learner.export(file = model_path) return learner def get_model_info(): print(&quot;====================================== Model ======================================&quot;) print(&quot;분석하고자 하는 모델 정보를 입력해주세요.&quot;) # print(&quot;pre_trained 모델을 선택해주세요. n 1:distilbert 2:bert 3:roberta 4:.... (TBD)&quot;) model_i = 1 # print(&quot;분석 NLP Task를 선택해주세요. n 1: multi-label classification 2:ner 3:squad 4:..... (TBD)&quot;) task_i = 1 # print(&quot;Class Label 개수를 입력해주세요.&quot;) # n_label = int(input()) n_label = 5 model_info = [model_dict[model_i], task_dict[task_i], n_label] print(&quot;=================== 입력해주신 Model 정보입니다 ===================&quot;) print(&quot;[분석 모델, NLP Task, Label 개수]&quot;) print(model_info) return model_dict[model_i], task_dict[task_i], n_label def get_list_layers(learner, model_type): if(model_type == &quot;distilbert&quot;): # For DistilBERT list_layers = [learner.model.transformer.distilbert.embeddings, learner.model.transformer.distilbert.transformer.layer[0], learner.model.transformer.distilbert.transformer.layer[1], learner.model.transformer.distilbert.transformer.layer[2], learner.model.transformer.distilbert.transformer.layer[3], learner.model.transformer.distilbert.transformer.layer[4], learner.model.transformer.distilbert.transformer.layer[5], learner.model.transformer.pre_classifier] elif(model_type == &quot;bert&quot;): # # For BERT list_layers = [learner.model.transformer.bert.embeddings, learner.model.transformer.bert.encoder.layer[0], learner.model.transformer.bert.encoder.layer[1], learner.model.transformer.bert.encoder.layer[2], learner.model.transformer.bert.encoder.layer[3], learner.model.transformer.bert.encoder.layer[4], learner.model.transformer.bert.encoder.layer[5], learner.model.transformer.bert.encoder.layer[6], learner.model.transformer.bert.encoder.layer[7], learner.model.transformer.bert.encoder.layer[8], learner.model.transformer.bert.encoder.layer[9], learner.model.transformer.bert.encoder.layer[10], learner.model.transformer.bert.encoder.layer[11], learner.model.transformer.bert.pooler, learner.model.transformer.dropout, # learner.model.transformer.lin1, # learner.model.transformer.lin2, # learner.model.transformer.lin3, # learner.model.transformer.lin4, learner.model.transformer.classifier] # elif(model_type == &quot;xlnet&quot;): # # For XLNet # list_layers = [learner.model.transformer.xlnet.embeddings, # learner.model.transformer.xlnet.transformer.layer[0], # learner.model.transformer.xlnet.transformer.layer[1], # learner.model.transformer.xlnet.transformer.layer[2], # learner.model.transformer.xlnet.transformer.layer[3], # learner.model.transformer.xlnet.transformer.layer[4], # learner.model.transformer.xlnet.transformer.layer[5], # learner.model.transformer.xlnet.transformer.layer[6], # learner.model.transformer.xlnet.transformer.layer[7], # learner.model.transformer.xlnet.transformer.layer[8], # learner.model.transformer.xlnet.transformer.layer[9], # learner.model.transformer.xlnet.transformer.layer[10], # learner.model.transformer.xlnet.transformer.layer[11], # learner.model.transformer.pre_classifier] elif(model_type == &quot;roberta&quot;): # For Roberta list_layers = [learner.model.transformer.roberta.embeddings, learner.model.transformer.roberta.encoder.layer[0], learner.model.transformer.roberta.encoder.layer[1], learner.model.transformer.roberta.encoder.layer[2], learner.model.transformer.roberta.encoder.layer[3], learner.model.transformer.roberta.encoder.layer[4], learner.model.transformer.roberta.encoder.layer[5], learner.model.transformer.roberta.encoder.layer[6], learner.model.transformer.roberta.encoder.layer[7], learner.model.transformer.roberta.encoder.layer[8], learner.model.transformer.roberta.encoder.layer[9], learner.model.transformer.roberta.encoder.layer[10], learner.model.transformer.roberta.encoder.layer[11], learner.model.transformer.roberta.pooler] elif(model_type == &quot;distilroberta&quot;): # distilroberta # For Roberta list_layers = [learner.model.transformer.roberta.embeddings, learner.model.transformer.roberta.encoder.layer[0], learner.model.transformer.roberta.encoder.layer[1], learner.model.transformer.roberta.encoder.layer[2], learner.model.transformer.roberta.encoder.layer[3], learner.model.transformer.roberta.encoder.layer[4], learner.model.transformer.roberta.encoder.layer[5], learner.model.transformer.roberta.pooler] return list_layers if __name__ == &quot;__main__&quot;: data_name = &#39;movie_en_5label&#39; model_type = &#39;roberta&#39; pretrained_model_name = &#39;roberta-base&#39; # data_name = sys.argv[1] # model_type = sys.argv[2] # pretrained_model_name = sys.argv[3] finetuned_model_name = &#39;./models/&#39; + data_name + &#39;_&#39; + pretrained_model_name + &#39;.pkl&#39; train, test, n_label, doc_name, label_name = get_data(data_name) model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type] transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name) transformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type) fastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[]) # Custom Processor transformer_vocab = TransformersVocab(tokenizer = transformer_tokenizer) numericalize_processor = NumericalizeProcessor(vocab=transformer_vocab) tokenize_processor = TokenizeProcessor(tokenizer=fastai_tokenizer, include_bos=False, include_eos=False) transformer_processor = [tokenize_processor, numericalize_processor] # Setting up DatBunch pad_first = bool(model_type in [&#39;xlnet&#39;]) pad_idx = transformer_tokenizer.pad_token_id databunch = (TextList.from_df(train, cols=doc_name, processor=transformer_processor) .split_by_rand_pct(0.1,seed=seed_val) .label_from_df(cols= label_name) # .add_test(test) .databunch()) config = config_class.from_pretrained(pretrained_model_name) config.num_labels = n_label # get n_label config.use_bfloat16 = False # use_fp16 print(config) print(pretrained_model_name) # For Bert # transformer_model = BertForSequenceClassification.from_pretrained(pretrained_model_name, config = config) transformer_model = model_class.from_pretrained(pretrained_model_name, config = config) custom_transformer_model = CustomTransformerModel(transformer_model = transformer_model) # learner = fine_tuned_model(custom_transformer_model, train, &#39;./wm_ref_tv.pkl&#39;) CustomAdamW = partial(AdamW, correct_bias=False) learner = Learner(databunch, custom_transformer_model, opt_func = CustomAdamW, metrics=[accuracy, error_rate]) print(learner.model) list_layers = get_list_layers(learner, model_type) learner.split(list_layers) num_groups = len(learner.layer_groups) print(&#39;Learner split in&#39;,num_groups,&#39;groups&#39;) print(learner.layer_groups) ## Train learner.save(&#39;untrain&#39;) seed_all(seed_val) learner.load(&#39;untrain&#39;) learner.freeze_to(-1) . (124848, 4) get_data function took 0.20489811897277832 sec . RobertaConfig { &#34;_num_labels&#34;: 5, &#34;architectures&#34;: [ &#34;RobertaForMaskedLM&#34; ], &#34;attention_probs_dropout_prob&#34;: 0.1, &#34;bad_words_ids&#34;: null, &#34;bos_token_id&#34;: 0, &#34;decoder_start_token_id&#34;: null, &#34;do_sample&#34;: false, &#34;early_stopping&#34;: false, &#34;eos_token_id&#34;: 2, &#34;finetuning_task&#34;: null, &#34;hidden_act&#34;: &#34;gelu&#34;, &#34;hidden_dropout_prob&#34;: 0.1, &#34;hidden_size&#34;: 768, &#34;id2label&#34;: { &#34;0&#34;: &#34;LABEL_0&#34;, &#34;1&#34;: &#34;LABEL_1&#34;, &#34;2&#34;: &#34;LABEL_2&#34;, &#34;3&#34;: &#34;LABEL_3&#34;, &#34;4&#34;: &#34;LABEL_4&#34; }, &#34;initializer_range&#34;: 0.02, &#34;intermediate_size&#34;: 3072, &#34;is_decoder&#34;: false, &#34;is_encoder_decoder&#34;: false, &#34;label2id&#34;: { &#34;LABEL_0&#34;: 0, &#34;LABEL_1&#34;: 1, &#34;LABEL_2&#34;: 2, &#34;LABEL_3&#34;: 3, &#34;LABEL_4&#34;: 4 }, &#34;layer_norm_eps&#34;: 1e-05, &#34;length_penalty&#34;: 1.0, &#34;max_length&#34;: 20, &#34;max_position_embeddings&#34;: 514, &#34;min_length&#34;: 0, &#34;model_type&#34;: &#34;roberta&#34;, &#34;no_repeat_ngram_size&#34;: 0, &#34;num_attention_heads&#34;: 12, &#34;num_beams&#34;: 1, &#34;num_hidden_layers&#34;: 12, &#34;num_return_sequences&#34;: 1, &#34;output_attentions&#34;: false, &#34;output_hidden_states&#34;: false, &#34;output_past&#34;: true, &#34;pad_token_id&#34;: 1, &#34;prefix&#34;: null, &#34;pruned_heads&#34;: {}, &#34;repetition_penalty&#34;: 1.0, &#34;task_specific_params&#34;: null, &#34;temperature&#34;: 1.0, &#34;top_k&#34;: 50, &#34;top_p&#34;: 1.0, &#34;torchscript&#34;: false, &#34;type_vocab_size&#34;: 1, &#34;use_bfloat16&#34;: false, &#34;vocab_size&#34;: 50265 } roberta-base CustomTransformerModel( (transformer): RobertaForSequenceClassification( (roberta): RobertaModel( (embeddings): RobertaEmbeddings( (word_embeddings): Embedding(50265, 768, padding_idx=1) (position_embeddings): Embedding(514, 768, padding_idx=1) (token_type_embeddings): Embedding(1, 768) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (encoder): BertEncoder( (layer): ModuleList( (0): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (1): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (2): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (3): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (4): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (5): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (6): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (7): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (8): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (9): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (10): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (11): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) ) ) (pooler): BertPooler( (dense): Linear(in_features=768, out_features=768, bias=True) (activation): Tanh() ) ) (classifier): RobertaClassificationHead( (dense): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) (out_proj): Linear(in_features=768, out_features=5, bias=True) ) ) ) Learner split in 14 groups [Sequential( (0): Embedding(50265, 768, padding_idx=1) (1): Embedding(514, 768, padding_idx=1) (2): Embedding(1, 768) (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (4): Dropout(p=0.1, inplace=False) ), Sequential( (0): Linear(in_features=768, out_features=768, bias=True) (1): Linear(in_features=768, out_features=768, bias=True) (2): Linear(in_features=768, out_features=768, bias=True) (3): Dropout(p=0.1, inplace=False) (4): Linear(in_features=768, out_features=768, bias=True) (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (6): Dropout(p=0.1, inplace=False) (7): Linear(in_features=768, out_features=3072, bias=True) (8): Linear(in_features=3072, out_features=768, bias=True) (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (10): Dropout(p=0.1, inplace=False) ), Sequential( (0): Linear(in_features=768, out_features=768, bias=True) (1): Linear(in_features=768, out_features=768, bias=True) (2): Linear(in_features=768, out_features=768, bias=True) (3): Dropout(p=0.1, inplace=False) (4): Linear(in_features=768, out_features=768, bias=True) (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (6): Dropout(p=0.1, inplace=False) (7): Linear(in_features=768, out_features=3072, bias=True) (8): Linear(in_features=3072, out_features=768, bias=True) (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (10): Dropout(p=0.1, inplace=False) ), Sequential( (0): Linear(in_features=768, out_features=768, bias=True) (1): Linear(in_features=768, out_features=768, bias=True) (2): Linear(in_features=768, out_features=768, bias=True) (3): Dropout(p=0.1, inplace=False) (4): Linear(in_features=768, out_features=768, bias=True) (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (6): Dropout(p=0.1, inplace=False) (7): Linear(in_features=768, out_features=3072, bias=True) (8): Linear(in_features=3072, out_features=768, bias=True) (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (10): Dropout(p=0.1, inplace=False) ), Sequential( (0): Linear(in_features=768, out_features=768, bias=True) (1): Linear(in_features=768, out_features=768, bias=True) (2): Linear(in_features=768, out_features=768, bias=True) (3): Dropout(p=0.1, inplace=False) (4): Linear(in_features=768, out_features=768, bias=True) (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (6): Dropout(p=0.1, inplace=False) (7): Linear(in_features=768, out_features=3072, bias=True) (8): Linear(in_features=3072, out_features=768, bias=True) (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (10): Dropout(p=0.1, inplace=False) ), Sequential( (0): Linear(in_features=768, out_features=768, bias=True) (1): Linear(in_features=768, out_features=768, bias=True) (2): Linear(in_features=768, out_features=768, bias=True) (3): Dropout(p=0.1, inplace=False) (4): Linear(in_features=768, out_features=768, bias=True) (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (6): Dropout(p=0.1, inplace=False) (7): Linear(in_features=768, out_features=3072, bias=True) (8): Linear(in_features=3072, out_features=768, bias=True) (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (10): Dropout(p=0.1, inplace=False) ), Sequential( (0): Linear(in_features=768, out_features=768, bias=True) (1): Linear(in_features=768, out_features=768, bias=True) (2): Linear(in_features=768, out_features=768, bias=True) (3): Dropout(p=0.1, inplace=False) (4): Linear(in_features=768, out_features=768, bias=True) (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (6): Dropout(p=0.1, inplace=False) (7): Linear(in_features=768, out_features=3072, bias=True) (8): Linear(in_features=3072, out_features=768, bias=True) (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (10): Dropout(p=0.1, inplace=False) ), Sequential( (0): Linear(in_features=768, out_features=768, bias=True) (1): Linear(in_features=768, out_features=768, bias=True) (2): Linear(in_features=768, out_features=768, bias=True) (3): Dropout(p=0.1, inplace=False) (4): Linear(in_features=768, out_features=768, bias=True) (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (6): Dropout(p=0.1, inplace=False) (7): Linear(in_features=768, out_features=3072, bias=True) (8): Linear(in_features=3072, out_features=768, bias=True) (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (10): Dropout(p=0.1, inplace=False) ), Sequential( (0): Linear(in_features=768, out_features=768, bias=True) (1): Linear(in_features=768, out_features=768, bias=True) (2): Linear(in_features=768, out_features=768, bias=True) (3): Dropout(p=0.1, inplace=False) (4): Linear(in_features=768, out_features=768, bias=True) (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (6): Dropout(p=0.1, inplace=False) (7): Linear(in_features=768, out_features=3072, bias=True) (8): Linear(in_features=3072, out_features=768, bias=True) (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (10): Dropout(p=0.1, inplace=False) ), Sequential( (0): Linear(in_features=768, out_features=768, bias=True) (1): Linear(in_features=768, out_features=768, bias=True) (2): Linear(in_features=768, out_features=768, bias=True) (3): Dropout(p=0.1, inplace=False) (4): Linear(in_features=768, out_features=768, bias=True) (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (6): Dropout(p=0.1, inplace=False) (7): Linear(in_features=768, out_features=3072, bias=True) (8): Linear(in_features=3072, out_features=768, bias=True) (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (10): Dropout(p=0.1, inplace=False) ), Sequential( (0): Linear(in_features=768, out_features=768, bias=True) (1): Linear(in_features=768, out_features=768, bias=True) (2): Linear(in_features=768, out_features=768, bias=True) (3): Dropout(p=0.1, inplace=False) (4): Linear(in_features=768, out_features=768, bias=True) (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (6): Dropout(p=0.1, inplace=False) (7): Linear(in_features=768, out_features=3072, bias=True) (8): Linear(in_features=3072, out_features=768, bias=True) (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (10): Dropout(p=0.1, inplace=False) ), Sequential( (0): Linear(in_features=768, out_features=768, bias=True) (1): Linear(in_features=768, out_features=768, bias=True) (2): Linear(in_features=768, out_features=768, bias=True) (3): Dropout(p=0.1, inplace=False) (4): Linear(in_features=768, out_features=768, bias=True) (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (6): Dropout(p=0.1, inplace=False) (7): Linear(in_features=768, out_features=3072, bias=True) (8): Linear(in_features=3072, out_features=768, bias=True) (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (10): Dropout(p=0.1, inplace=False) ), Sequential( (0): Linear(in_features=768, out_features=768, bias=True) (1): Linear(in_features=768, out_features=768, bias=True) (2): Linear(in_features=768, out_features=768, bias=True) (3): Dropout(p=0.1, inplace=False) (4): Linear(in_features=768, out_features=768, bias=True) (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (6): Dropout(p=0.1, inplace=False) (7): Linear(in_features=768, out_features=3072, bias=True) (8): Linear(in_features=3072, out_features=768, bias=True) (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (10): Dropout(p=0.1, inplace=False) ), Sequential( (0): Linear(in_features=768, out_features=768, bias=True) (1): Tanh() (2): Linear(in_features=768, out_features=768, bias=True) (3): Dropout(p=0.1, inplace=False) (4): Linear(in_features=768, out_features=5, bias=True) )] . learner.summary() . CustomTransformerModel ====================================================================== Layer (type) Output Shape Param # Trainable ====================================================================== Embedding [75, 768] 38,603,520 False ______________________________________________________________________ Embedding [75, 768] 394,752 False ______________________________________________________________________ Embedding [75, 768] 768 False ______________________________________________________________________ LayerNorm [75, 768] 1,536 False ______________________________________________________________________ Dropout [75, 768] 0 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Dropout [12, 75, 75] 0 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ LayerNorm [75, 768] 1,536 False ______________________________________________________________________ Dropout [75, 768] 0 False ______________________________________________________________________ Linear [75, 3072] 2,362,368 False ______________________________________________________________________ Linear [75, 768] 2,360,064 False ______________________________________________________________________ LayerNorm [75, 768] 1,536 False ______________________________________________________________________ Dropout [75, 768] 0 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Dropout [12, 75, 75] 0 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ LayerNorm [75, 768] 1,536 False ______________________________________________________________________ Dropout [75, 768] 0 False ______________________________________________________________________ Linear [75, 3072] 2,362,368 False ______________________________________________________________________ Linear [75, 768] 2,360,064 False ______________________________________________________________________ LayerNorm [75, 768] 1,536 False ______________________________________________________________________ Dropout [75, 768] 0 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Dropout [12, 75, 75] 0 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ LayerNorm [75, 768] 1,536 False ______________________________________________________________________ Dropout [75, 768] 0 False ______________________________________________________________________ Linear [75, 3072] 2,362,368 False ______________________________________________________________________ Linear [75, 768] 2,360,064 False ______________________________________________________________________ LayerNorm [75, 768] 1,536 False ______________________________________________________________________ Dropout [75, 768] 0 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Dropout [12, 75, 75] 0 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ LayerNorm [75, 768] 1,536 False ______________________________________________________________________ Dropout [75, 768] 0 False ______________________________________________________________________ Linear [75, 3072] 2,362,368 False ______________________________________________________________________ Linear [75, 768] 2,360,064 False ______________________________________________________________________ LayerNorm [75, 768] 1,536 False ______________________________________________________________________ Dropout [75, 768] 0 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Dropout [12, 75, 75] 0 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ LayerNorm [75, 768] 1,536 False ______________________________________________________________________ Dropout [75, 768] 0 False ______________________________________________________________________ Linear [75, 3072] 2,362,368 False ______________________________________________________________________ Linear [75, 768] 2,360,064 False ______________________________________________________________________ LayerNorm [75, 768] 1,536 False ______________________________________________________________________ Dropout [75, 768] 0 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Dropout [12, 75, 75] 0 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ LayerNorm [75, 768] 1,536 False ______________________________________________________________________ Dropout [75, 768] 0 False ______________________________________________________________________ Linear [75, 3072] 2,362,368 False ______________________________________________________________________ Linear [75, 768] 2,360,064 False ______________________________________________________________________ LayerNorm [75, 768] 1,536 False ______________________________________________________________________ Dropout [75, 768] 0 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Dropout [12, 75, 75] 0 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ LayerNorm [75, 768] 1,536 False ______________________________________________________________________ Dropout [75, 768] 0 False ______________________________________________________________________ Linear [75, 3072] 2,362,368 False ______________________________________________________________________ Linear [75, 768] 2,360,064 False ______________________________________________________________________ LayerNorm [75, 768] 1,536 False ______________________________________________________________________ Dropout [75, 768] 0 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Dropout [12, 75, 75] 0 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ LayerNorm [75, 768] 1,536 False ______________________________________________________________________ Dropout [75, 768] 0 False ______________________________________________________________________ Linear [75, 3072] 2,362,368 False ______________________________________________________________________ Linear [75, 768] 2,360,064 False ______________________________________________________________________ LayerNorm [75, 768] 1,536 False ______________________________________________________________________ Dropout [75, 768] 0 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Dropout [12, 75, 75] 0 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ LayerNorm [75, 768] 1,536 False ______________________________________________________________________ Dropout [75, 768] 0 False ______________________________________________________________________ Linear [75, 3072] 2,362,368 False ______________________________________________________________________ Linear [75, 768] 2,360,064 False ______________________________________________________________________ LayerNorm [75, 768] 1,536 False ______________________________________________________________________ Dropout [75, 768] 0 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Dropout [12, 75, 75] 0 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ LayerNorm [75, 768] 1,536 False ______________________________________________________________________ Dropout [75, 768] 0 False ______________________________________________________________________ Linear [75, 3072] 2,362,368 False ______________________________________________________________________ Linear [75, 768] 2,360,064 False ______________________________________________________________________ LayerNorm [75, 768] 1,536 False ______________________________________________________________________ Dropout [75, 768] 0 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Dropout [12, 75, 75] 0 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ LayerNorm [75, 768] 1,536 False ______________________________________________________________________ Dropout [75, 768] 0 False ______________________________________________________________________ Linear [75, 3072] 2,362,368 False ______________________________________________________________________ Linear [75, 768] 2,360,064 False ______________________________________________________________________ LayerNorm [75, 768] 1,536 False ______________________________________________________________________ Dropout [75, 768] 0 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ Dropout [12, 75, 75] 0 False ______________________________________________________________________ Linear [75, 768] 590,592 False ______________________________________________________________________ LayerNorm [75, 768] 1,536 False ______________________________________________________________________ Dropout [75, 768] 0 False ______________________________________________________________________ Linear [75, 3072] 2,362,368 False ______________________________________________________________________ Linear [75, 768] 2,360,064 False ______________________________________________________________________ LayerNorm [75, 768] 1,536 False ______________________________________________________________________ Dropout [75, 768] 0 False ______________________________________________________________________ Linear [768] 590,592 True ______________________________________________________________________ Tanh [768] 0 False ______________________________________________________________________ Linear [768] 590,592 True ______________________________________________________________________ Dropout [768] 0 False ______________________________________________________________________ Linear [5] 3,845 True ______________________________________________________________________ Total params: 125,240,069 Total trainable params: 1,185,029 Total non-trainable params: 124,055,040 Optimized with &#39;transformers.optimization.AdamW&#39;, correct_bias=False Using true weight decay as discussed in https://www.fast.ai/2018/07/02/adam-weight-decay/ Loss function : FlattenedLoss ====================================================================== Callbacks functions applied . learner.lr_find() . &lt;progress value=&#39;0&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 0.00% [0/1 00:00&lt;00:00] epoch train_loss valid_loss accuracy error_rate time . &lt;progress value=&#39;71&#39; class=&#39;&#39; max=&#39;1755&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 4.05% [71/1755 00:07&lt;02:47 5.2353] &lt;/div&gt; &lt;/div&gt; LR Finder is complete, type {learner_name}.recorder.plot() to see the graph. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learner.recorder.plot(skip_end=10, suggestion=True) . Min numerical gradient: 1.91E-04 Min loss divided by 10: 1.74E-04 . # learner.fit_one_cycle(1,max_lr=2e-03,moms=(0.8,0.7)) learner.fit_one_cycle(1,max_lr=slice(1e-05, 1e-03)) learner.save(&#39;first_cycle&#39;) seed_all(seed_val) learner.load(&#39;first_cycle&#39;) . epoch train_loss valid_loss accuracy error_rate time . 0 | 1.199539 | 1.233606 | 0.510574 | 0.489426 | 01:02 | . Learner(data=TextClasDataBunch; Train: LabelList (112364 items) x: TextList &lt;s&gt; Ġunintentionally Ġ- RR B - &lt;/s&gt;,&lt;s&gt; Ġwill Ġneed Ġall Ġthe Ġluck Ġthey Ġcan Ġmuster Ġjust Ġfiguring Ġout Ġwho Ġ&#39; s Ġwho Ġin Ġthis Ġpret entious Ġmess &lt;/s&gt;,&lt;s&gt; Ġsomewhere Ġbetween ĠS ling ĠBlade Ġand ĠSouth Ġof ĠHeaven Ġ, ĠWest Ġof ĠHell &lt;/s&gt;,&lt;s&gt; Ġreminds Ġat Ġevery Ġturn Ġof ĠElizabeth ĠBerk ley Ġ&#39; s Ġflo pping Ġdolphin - g asm &lt;/s&gt;,&lt;s&gt; Ġthe ĠMag i &lt;/s&gt; y: CategoryList 2,0,2,1,2 Path: .; Valid: LabelList (12484 items) x: TextList &lt;s&gt; Ġk idd ie Ġentertainment &lt;/s&gt;,&lt;s&gt; Ġfinally Ġmakes ĠSex ĠWith ĠStr angers Ġ, Ġwhich Ġopens Ġtoday Ġin Ġthe ĠNew ĠYork Ġmetropolitan Ġarea Ġ, Ġso Ġdist ast eful &lt;/s&gt;,&lt;s&gt; Ġmuch Ġas Ġthey Ġlove Ġthemselves &lt;/s&gt;,&lt;s&gt; Ġupdate Ġher Ġbeloved Ġgenre &lt;/s&gt;,&lt;s&gt; Ġbeautiful Ġwomen &lt;/s&gt; y: CategoryList 2,0,2,2,2 Path: .; Test: None, model=CustomTransformerModel( (transformer): RobertaForSequenceClassification( (roberta): RobertaModel( (embeddings): RobertaEmbeddings( (word_embeddings): Embedding(50265, 768, padding_idx=1) (position_embeddings): Embedding(514, 768, padding_idx=1) (token_type_embeddings): Embedding(1, 768) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (encoder): BertEncoder( (layer): ModuleList( (0): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (1): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (2): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (3): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (4): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (5): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (6): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (7): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (8): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (9): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (10): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (11): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) ) ) (pooler): BertPooler( (dense): Linear(in_features=768, out_features=768, bias=True) (activation): Tanh() ) ) (classifier): RobertaClassificationHead( (dense): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) (out_proj): Linear(in_features=768, out_features=5, bias=True) ) ) ), opt_func=functools.partial(&lt;class &#39;transformers.optimization.AdamW&#39;&gt;, correct_bias=False), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[&lt;function accuracy at 0x7f8dfd183158&gt;, &lt;function error_rate at 0x7f8dfd183378&gt;], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath(&#39;.&#39;), model_dir=&#39;models&#39;, callback_fns=[functools.partial(&lt;class &#39;fastai.basic_train.Recorder&#39;&gt;, add_time=True, silent=False)], callbacks=[], layer_groups=[Sequential( (0): Embedding(50265, 768, padding_idx=1) (1): Embedding(514, 768, padding_idx=1) (2): Embedding(1, 768) (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (4): Dropout(p=0.1, inplace=False) ), Sequential( (0): Linear(in_features=768, out_features=768, bias=True) (1): Linear(in_features=768, out_features=768, bias=True) (2): Linear(in_features=768, out_features=768, bias=True) (3): Dropout(p=0.1, inplace=False) (4): Linear(in_features=768, out_features=768, bias=True) (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (6): Dropout(p=0.1, inplace=False) (7): Linear(in_features=768, out_features=3072, bias=True) (8): Linear(in_features=3072, out_features=768, bias=True) (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (10): Dropout(p=0.1, inplace=False) ), Sequential( (0): Linear(in_features=768, out_features=768, bias=True) (1): Linear(in_features=768, out_features=768, bias=True) (2): Linear(in_features=768, out_features=768, bias=True) (3): Dropout(p=0.1, inplace=False) (4): Linear(in_features=768, out_features=768, bias=True) (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (6): Dropout(p=0.1, inplace=False) (7): Linear(in_features=768, out_features=3072, bias=True) (8): Linear(in_features=3072, out_features=768, bias=True) (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (10): Dropout(p=0.1, inplace=False) ), Sequential( (0): Linear(in_features=768, out_features=768, bias=True) (1): Linear(in_features=768, out_features=768, bias=True) (2): Linear(in_features=768, out_features=768, bias=True) (3): Dropout(p=0.1, inplace=False) (4): Linear(in_features=768, out_features=768, bias=True) (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (6): Dropout(p=0.1, inplace=False) (7): Linear(in_features=768, out_features=3072, bias=True) (8): Linear(in_features=3072, out_features=768, bias=True) (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (10): Dropout(p=0.1, inplace=False) ), Sequential( (0): Linear(in_features=768, out_features=768, bias=True) (1): Linear(in_features=768, out_features=768, bias=True) (2): Linear(in_features=768, out_features=768, bias=True) (3): Dropout(p=0.1, inplace=False) (4): Linear(in_features=768, out_features=768, bias=True) (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (6): Dropout(p=0.1, inplace=False) (7): Linear(in_features=768, out_features=3072, bias=True) (8): Linear(in_features=3072, out_features=768, bias=True) (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (10): Dropout(p=0.1, inplace=False) ), Sequential( (0): Linear(in_features=768, out_features=768, bias=True) (1): Linear(in_features=768, out_features=768, bias=True) (2): Linear(in_features=768, out_features=768, bias=True) (3): Dropout(p=0.1, inplace=False) (4): Linear(in_features=768, out_features=768, bias=True) (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (6): Dropout(p=0.1, inplace=False) (7): Linear(in_features=768, out_features=3072, bias=True) (8): Linear(in_features=3072, out_features=768, bias=True) (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (10): Dropout(p=0.1, inplace=False) ), Sequential( (0): Linear(in_features=768, out_features=768, bias=True) (1): Linear(in_features=768, out_features=768, bias=True) (2): Linear(in_features=768, out_features=768, bias=True) (3): Dropout(p=0.1, inplace=False) (4): Linear(in_features=768, out_features=768, bias=True) (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (6): Dropout(p=0.1, inplace=False) (7): Linear(in_features=768, out_features=3072, bias=True) (8): Linear(in_features=3072, out_features=768, bias=True) (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (10): Dropout(p=0.1, inplace=False) ), Sequential( (0): Linear(in_features=768, out_features=768, bias=True) (1): Linear(in_features=768, out_features=768, bias=True) (2): Linear(in_features=768, out_features=768, bias=True) (3): Dropout(p=0.1, inplace=False) (4): Linear(in_features=768, out_features=768, bias=True) (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (6): Dropout(p=0.1, inplace=False) (7): Linear(in_features=768, out_features=3072, bias=True) (8): Linear(in_features=3072, out_features=768, bias=True) (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (10): Dropout(p=0.1, inplace=False) ), Sequential( (0): Linear(in_features=768, out_features=768, bias=True) (1): Linear(in_features=768, out_features=768, bias=True) (2): Linear(in_features=768, out_features=768, bias=True) (3): Dropout(p=0.1, inplace=False) (4): Linear(in_features=768, out_features=768, bias=True) (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (6): Dropout(p=0.1, inplace=False) (7): Linear(in_features=768, out_features=3072, bias=True) (8): Linear(in_features=3072, out_features=768, bias=True) (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (10): Dropout(p=0.1, inplace=False) ), Sequential( (0): Linear(in_features=768, out_features=768, bias=True) (1): Linear(in_features=768, out_features=768, bias=True) (2): Linear(in_features=768, out_features=768, bias=True) (3): Dropout(p=0.1, inplace=False) (4): Linear(in_features=768, out_features=768, bias=True) (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (6): Dropout(p=0.1, inplace=False) (7): Linear(in_features=768, out_features=3072, bias=True) (8): Linear(in_features=3072, out_features=768, bias=True) (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (10): Dropout(p=0.1, inplace=False) ), Sequential( (0): Linear(in_features=768, out_features=768, bias=True) (1): Linear(in_features=768, out_features=768, bias=True) (2): Linear(in_features=768, out_features=768, bias=True) (3): Dropout(p=0.1, inplace=False) (4): Linear(in_features=768, out_features=768, bias=True) (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (6): Dropout(p=0.1, inplace=False) (7): Linear(in_features=768, out_features=3072, bias=True) (8): Linear(in_features=3072, out_features=768, bias=True) (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (10): Dropout(p=0.1, inplace=False) ), Sequential( (0): Linear(in_features=768, out_features=768, bias=True) (1): Linear(in_features=768, out_features=768, bias=True) (2): Linear(in_features=768, out_features=768, bias=True) (3): Dropout(p=0.1, inplace=False) (4): Linear(in_features=768, out_features=768, bias=True) (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (6): Dropout(p=0.1, inplace=False) (7): Linear(in_features=768, out_features=3072, bias=True) (8): Linear(in_features=3072, out_features=768, bias=True) (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (10): Dropout(p=0.1, inplace=False) ), Sequential( (0): Linear(in_features=768, out_features=768, bias=True) (1): Linear(in_features=768, out_features=768, bias=True) (2): Linear(in_features=768, out_features=768, bias=True) (3): Dropout(p=0.1, inplace=False) (4): Linear(in_features=768, out_features=768, bias=True) (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (6): Dropout(p=0.1, inplace=False) (7): Linear(in_features=768, out_features=3072, bias=True) (8): Linear(in_features=3072, out_features=768, bias=True) (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (10): Dropout(p=0.1, inplace=False) ), Sequential( (0): Linear(in_features=768, out_features=768, bias=True) (1): Tanh() (2): Linear(in_features=768, out_features=768, bias=True) (3): Dropout(p=0.1, inplace=False) (4): Linear(in_features=768, out_features=5, bias=True) )], add_time=True, silent=False) . learner.lr_find() learner.recorder.plot(skip_end=10, suggestion=True) . &lt;progress value=&#39;0&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 0.00% [0/1 00:00&lt;00:00] epoch train_loss valid_loss accuracy error_rate time . &lt;progress value=&#39;77&#39; class=&#39;&#39; max=&#39;1755&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 4.39% [77/1755 00:07&lt;02:50 3.9110] &lt;/div&gt; &lt;/div&gt; LR Finder is complete, type {learner_name}.recorder.plot() to see the graph. Min numerical gradient: 9.12E-07 Min loss divided by 10: 1.32E-03 . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learner.freeze_to(-2) # lr = 1e-5 learner.fit_one_cycle(1, max_lr=slice(1e-06, 1e-04)) . epoch train_loss valid_loss accuracy error_rate time . 0 | 1.116296 | 1.071130 | 0.568728 | 0.431272 | 01:09 | . learner.callbacks.append(ShowGraph(learner)) print(learner.model) . CustomTransformerModel( (transformer): RobertaForSequenceClassification( (roberta): RobertaModel( (embeddings): RobertaEmbeddings( (word_embeddings): Embedding(50265, 768, padding_idx=1) (position_embeddings): Embedding(514, 768, padding_idx=1) (token_type_embeddings): Embedding(1, 768) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (encoder): BertEncoder( (layer): ModuleList( (0): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (1): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (2): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (3): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (4): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (5): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (6): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (7): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (8): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (9): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (10): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (11): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) ) ) (pooler): BertPooler( (dense): Linear(in_features=768, out_features=768, bias=True) (activation): Tanh() ) ) (classifier): RobertaClassificationHead( (dense): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) (out_proj): Linear(in_features=768, out_features=5, bias=True) ) ) ) . &lt;/div&gt; .",
            "url": "https://inspiringpeople.github.io/tech_blog/2020/09/09/lr_test.html",
            "relUrl": "/2020/09/09/lr_test.html",
            "date": " • Sep 9, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "fastcore: An Underrated Python Library",
            "content": ". Background . I recently embarked on a journey to sharpen my python skills: I wanted to learn advanced patterns, idioms, and techniques. I started with reading books on advanced Python, however, the information didn&#39;t seem to stick without having somewhere to apply it. I also wanted the ability to ask questions from an expert while I was learning -- which is an arrangement that is hard to find! That&#39;s when it occurred to me: What if I could find an open source project that has fairly advanced python code and write documentation and tests? I made a bet that if I did this it would force me to learn everything very deeply, and the maintainers would be appreciative of my work and be willing to answer my questions. . And that&#39;s exactly what I did over the past month! I&#39;m pleased to report that it has been the most efficient learning experience I&#39;ve ever experienced. I&#39;ve discovered that writing documentation forced me to deeply understand not just what the code does but also why the code works the way it does, and to explore edge cases while writing tests. Most importantly, I was able to ask questions when I was stuck, and maintainers were willing to devote extra time knowing that their mentorship was in service of making their code more accessible! It turns out the library I choose, fastcore is some of the most fascinating Python I have ever encountered as its purpose and goals are fairly unique. . For the uninitiated, fastcore is a library on top of which many fast.ai projects are built on. Most importantly, fastcore extends the python programming language and strives to eliminate boilerplate and add useful functionality for common tasks. In this blog post, I&#39;m going to highlight some of my favorite tools that fastcore provides, rather than sharing what I learned about python. My goal is to pique your interest in this library, and hopefully motivate you to check out the documentation after you are done to learn more! . Why fastcore is interesting . Get exposed to ideas from other languages without leaving python: I’ve always heard that it is beneficial to learn other languages in order to become a better programmer. From a pragmatic point of view, I’ve found it difficult to learn other languages because I could never use them at work. Fastcore extends python to include patterns found in languages as diverse as Julia, Ruby and Haskell. Now that I understand these tools I am motivated to learn other languages. | You get a new set of pragmatic tools: fastcore includes utilities that will allow you to write more concise expressive code, and perhaps solve new problems. | Learn more about the Python programming language: Because fastcore extends the python programming language, many advanced concepts are exposed during the process. For the motivated, this is a great way to see how many of the internals of python work. | A whirlwind tour through fastcore . Here are some things you can do with fastcore that immediately caught my attention. . . Making **kwargs transparent . Whenever I see a function that has the argument **kwargs, I cringe a little. This is because it means the API is obfuscated and I have to read the source code to figure out what valid parameters might be. Consider the below example: . def baz(a, b=2, c =3, d=4): return a + b + c def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, **kwargs)&gt; . Without reading the source code, it might be hard for me to know that foo also accepts and additional parameters b and d. We can fix this with delegates: . def baz(a, b=2, c =3, d=4): return a + b + c @delegates(baz) # this decorator will pass down keyword arguments from baz def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4)&gt; . You can customize the behavior of this decorator. For example, you can have your cake and eat it too by passing down your arguments and also keeping **kwargs: . @delegates(baz, keep=True) def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4, **kwargs)&gt; . You can also exclude arguments. For example, we exclude argument d from delegation: . def basefoo(a, b=2, c =3, d=4): pass @delegates(basefoo, but= [&#39;d&#39;]) # exclude `d` def foo(c, a, **kwargs): pass inspect.signature(foo) . &lt;Signature (c, a, b=2)&gt; . You can also delegate between classes: . class BaseFoo: def __init__(self, e, c=2): pass @delegates()# since no argument was passsed here we delegate to the superclass class Foo(BaseFoo): def __init__(self, a, b=1, **kwargs): super().__init__(**kwargs) inspect.signature(Foo) . &lt;Signature (a, b=1, c=2)&gt; . For more information, read the docs on delegates. . . Avoid boilerplate when setting instance attributes . Have you ever wondered if it was possible to avoid the boilerplate involved with setting attributes in __init__? . class Test: def __init__(self, a, b ,c): self.a, self.b, self.c = a, b, c . Ouch! That was painful. Look at all the repeated variable names. Do I really have to repeat myself like this when defining a class? Not Anymore! Checkout store_attr: . class Test: def __init__(self, a, b, c): store_attr() t = Test(5,4,3) assert t.b == 4 . You can also exclude certain attributes: . class Test: def __init__(self, a, b, c): store_attr(but=[&#39;c&#39;]) t = Test(5,4,3) assert t.b == 4 assert not hasattr(t, &#39;c&#39;) . There are many more ways of customizing and using store_attr than I highlighted here. Check out the docs for more detail. . . Avoiding subclassing boilerplate . One thing I hate about python is the __super__().__init__() boilerplate associated with subclassing. For example: . class ParentClass: def __init__(self): self.some_attr = &#39;hello&#39; class ChildClass(ParentClass): def __init__(self): super().__init__() cc = ChildClass() assert cc.some_attr == &#39;hello&#39; # only accessible b/c you used super . We can avoid this boilerplate by using the metaclass PrePostInitMeta. We define a new class called NewParent that is a wrapper around the ParentClass: . class NewParent(ParentClass, metaclass=PrePostInitMeta): def __pre_init__(self, *args, **kwargs): super().__init__() class ChildClass(NewParent): def __init__(self):pass sc = ChildClass() assert sc.some_attr == &#39;hello&#39; . . Type Dispatch . Type dispatch, or Multiple dispatch, allows you to change the way a function behaves based upon the input types it receives. This is a prominent feature in some programming languages like Julia. For example, this is a conceptual example of how multiple dispatch works in Julia, returning different values depending on the input types of x and y: . collide_with(x::Asteroid, y::Asteroid) = ... # deal with asteroid hitting asteroid collide_with(x::Asteroid, y::Spaceship) = ... # deal with asteroid hitting spaceship collide_with(x::Spaceship, y::Asteroid) = ... # deal with spaceship hitting asteroid collide_with(x::Spaceship, y::Spaceship) = ... # deal with spaceship hitting spaceship . Type dispatch can be especially useful in data science, where you might allow different input types (i.e. Numpy arrays and Pandas dataframes) to a function that processes data. Type dispatch allows you to have a common API for functions that do similar tasks. . Unfortunately, Python does not support this out-of-the box. Fortunately, there is the @typedispatch decorator to the rescue. This decorator relies upon type hints in order to route inputs the correct version of the function: . @typedispatch def f(x:str, y:str): return f&#39;{x}{y}&#39; @typedispatch def f(x:np.ndarray): return x.sum() @typedispatch def f(x:int, y:int): return x+y . Below is a demonstration of type dispatch at work for the function f: . f(&#39;Hello &#39;, &#39;World!&#39;) . &#39;Hello World!&#39; . f(2,3) . 5 . f(np.array([5,5,5,5])) . 20 . There are limitations of this feature, as well as other ways of using this functionality that you can read about here. In the process of learning about typed dispatch, I also found a python library called multipledispatch made by Mathhew Rocklin (the creator of Dask). . After using this feature, I am now motivated to learn languages like Julia to discover what other paradigms I might be missing. . . A better version of functools.partial . functools.partial is a great utility that creates functions from other functions that lets you set default values. Lets take this function for example that filters a list to only contain values &gt;= val: . test_input = [1,2,3,4,5,6] def f(arr, val): &quot;Filter a list to remove any values that are less than val.&quot; return [x for x in arr if x &gt;= val] f(test_input, 3) . [3, 4, 5, 6] . You can create a new function out of this function using partial that sets the default value to 5: . filter5 = partial(f, val=5) filter5(test_input) . [5, 6] . One problem with partial is that it removes the original docstring and replaces it with a generic docstring: . filter5.__doc__ . &#39;partial(func, *args, **keywords) - new function with partial application n of the given arguments and keywords. n&#39; . fastcore.utils.partialler fixes this, and makes sure the docstring is retained such that the new API is transparent: . filter5 = partialler(f, val=5) filter5.__doc__ . &#39;Filter a list to remove any values that are less than val.&#39; . . Composition of functions . A technique that is pervasive in functional programming languages is function composition, whereby you chain a bunch of functions together to achieve some kind of result. This is especially useful when applying various data transformations. Consider a toy example where I have three functions: (1) Removes elements of a list less than 5 (from the prior section) (2) adds 2 to each number (3) sums all the numbers: . def add(arr, val): return [x + val for x in arr] def arrsum(arr): return sum(arr) # See the previous section on partialler add2 = partialler(add, val=2) transform = compose(filter5, add2, arrsum) transform([1,2,3,4,5,6]) . 15 . But why is this useful? You might me thinking, I can accomplish the same thing with: . arrsum(add2(filter5([1,2,3,4,5,6]))) . You are not wrong! However, composition gives you a convenient interface in case you want to do something like the following: . def fit(x, transforms:list): &quot;fit a model after performing transformations&quot; x = compose(*transforms)(x) y = [np.mean(x)] * len(x) # its a dumb model. Don&#39;t judge me return y # filters out elements &lt; 5, adds 2, then predicts the mean fit(x=[1,2,3,4,5,6], transforms=[filter5, add2]) . [7.5, 7.5] . For more information about compose, read the docs. . . A more useful __repr__ . In python, __repr__ helps you get information about an object for logging and debugging. Below is what you get by default when you define a new class. (Note: we are using store_attr, which was discussed earlier). . class Test: def __init__(self, a, b=2, c=3): store_attr() # `store_attr` was discussed previously Test(1) . &lt;__main__.Test at 0x7fe0ab662790&gt; . We can use basic_repr to quickly give us a more sensible default: . class Test: def __init__(self, a, b=2, c=3): store_attr() __repr__ = basic_repr(&#39;a,b,c&#39;) Test(2) . Test(a=2, b=2, c=3) . . Monkey Patching With A Decorator . It can be convenient to monkey patch with a decorator, which is especially helpful when you want to patch an external library you are importing. We can use the decorator @patch from fastcore.foundation along with type hints like so: . class MyClass(int): pass @patch def func(self:MyClass, a): return self+a mc = MyClass(3) . Now, MyClass has an additional method named func: . mc.func(10) . 13 . Still not convinced? I&#39;ll show you another example of this kind of patching in the next section. . . A better pathlib.Path . When you see these extensions to pathlib.path you won&#39;t ever use vanilla pathlib again! A number of additional methods have been added to pathlib, such as: . Path.readlines: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.readlines() | Path.read: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.read() | Path.save: saves file as pickle | Path.load: loads pickle file | Path.ls: shows the contents of the path as a list. | etc. | . Read more about this here. Here is a demonstration of ls: . from pathlib import Path p = Path(&#39;../_notebooks&#39;) p.ls() # you don&#39;t get this with vanilla Pathlib.Path!! . (#21) [Path(&#39;../_notebooks/gpt2_simple_mask.jpg&#39;),Path(&#39;../_notebooks/bert_mac_small.jpg&#39;),Path(&#39;../_notebooks/causal_with_prefix.jpg&#39;),Path(&#39;../_notebooks/.DS_Store&#39;),Path(&#39;../_notebooks/2020-03-07-How_to_Create_an_Automatic_Code_Comment_Generator_using_Deep_Learning.ipynb&#39;),Path(&#39;../_notebooks/2020-09-01-fastcore.ipynb&#39;),Path(&#39;../_notebooks/2020-03-07-Syntax-Highlighting.ipynb&#39;),Path(&#39;../_notebooks/2020-03-06-bart.ipynb&#39;),Path(&#39;../_notebooks/README.md&#39;),Path(&#39;../_notebooks/2020-05-01-TrainDonkeyCar.ipynb&#39;)...] . Wait! What&#39;s going on here? We just imported pathlib.Path - why are we getting this new functionality? Thats because we imported the fastcore.foundation module, which patches this module via the @patch decorator discussed earlier. Just to drive the point home on why the @patch decorator is useful, I&#39;ll go ahead and add another method to Path right now: . @patch def fun(self:Path): return &quot;This is fun!&quot; p.fun() . &#39;This is fun!&#39; . That is magical, right? I know! That&#39;s why I&#39;m writing about it! . . An Even More Concise Way To Create Lambdas . Self, with an uppercase S, is an even more concise way to create lambdas that are calling methods on an object. For example, let&#39;s create a lambda for taking the sum of a Numpy array: . arr=np.array([5,4,3,2,1]) f = lambda a: a.sum() assert f(arr) == 15 . You can use Self in the same way: . f = Self.sum() assert f(arr) == 15 . Let&#39;s create a lambda that does a groupby and max of a Pandas dataframe: . import pandas as pd df=pd.DataFrame({&#39;Some Column&#39;: [&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, ], &#39;Another Column&#39;: [5, 7, 50, 70]}) f = Self.groupby(&#39;Some Column&#39;).mean() f(df) . Another Column . Some Column . a 6 | . b 60 | . Read more about Self in the docs). . . Notebook Functions . These are simple but handy, and allow you to know whether or not code is executing in a Jupyter Notebook, Colab, or an Ipython Shell: . in_notebook(), in_colab(), in_ipython() . (True, False, True) . This is useful if you are displaying certain types of visualizations, progress bars or animations in your code that you may want to modify or toggle depending on the environment. . . A Drop-In Replacement For List . You might be pretty happy with Python&#39;s list. This is one of those situations that you don&#39;t know you needed a better list until someone showed one to you. Enter L, a list like object with many extra goodies. . The best way I can describe L is to pretend that list and numpy had a pretty baby: . define a list (check out the nice __repr__ that shows the length of the list!) . L(1,2,3) . (#3) [1,2,3] . Shuffle a list: . p = L.range(20).shuffle() p . (#20) [2,0,18,6,15,17,14,8,12,1...] . Index into a list: . p[2,4,6] . (#3) [18,15,14] . L has sensible defaults, for example appending an element to a list: . 1 + L(2,3,4) . (#4) [1,2,3,4] . There is much more L has to offer. Read the docs to learn more. . But Wait ... There&#39;s More! . There are more things I would like to show you about fastcore, but there is no way they would reasonably fit into a blog post. Here is a list of some of my favorite things that I didn&#39;t demo in this blog post: . Utilities . The Utilites section contain many shortcuts to perform common tasks or provide an additional interface to what standard python provides. . mk_class: quickly add a bunch of attributes to a class | wrap_class: add new methods to a class with a simple decorator | groupby: similar to Scala&#39;s groupby | merge: merge dicts | fasttuple: a tuple on steroids | Infinite Lists: useful for padding and testing | chunked: for batching and organizing stuff | . Multiprocessing . The Multiprocessing section extends python&#39;s multiprocessing library by offering features like: . progress bars | ability to pause to mitigate race conditions with external services | processing things in batches on each worker, ex: if you have a vectorized operation to perform in chunks | . Functional Programming . The functional programming section is my favorite part of this library. . maps: a map that also composes functions | mapped: A more robust map | using_attr: compose a function that operates on an attribute | . Transforms . Transforms is a collection of utilities for creating data transformations and associated pipelines. These transformation utilities build upon many of the building blocks discussed in this blog post. . Further Reading . It should be noted that you should read the main page of the docs first, followed by the section on tests to fully understand the documentation. . The fastcore documentation site. | The fastcore GitHub repo. | Blog post on delegation. | . Shameless plug: fastpages . This blog post was written entirely in a Jupyter Notebook, which GitHub automatically converted into to a blog post! Sound interesting? Check out fastpages. .",
            "url": "https://inspiringpeople.github.io/tech_blog/fastcore/",
            "relUrl": "/fastcore/",
            "date": " • Sep 1, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://inspiringpeople.github.io/tech_blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://inspiringpeople.github.io/tech_blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "AI와 함께 작곡한다면?",
            "content": "AI를 통해 사람의 창의성을 발현 시키는 것은 가능한 일일까? . . 이것이 가능한지, 또 옳은일인지에 대해 다양한 의견이 있고 서로의 입장이 워낙 견고하다. 듣고 보는 것만으로는 그 정도를 가늠하기가 어려워서 이 질문을 나에게 적용해보기로 했다. . 예술가가 되기 - 인생의 어느 작은 구석에서라도 예술성을 실현하며 살기. . “인생의 어느 작은 구석에서라도 예술성을 실현하기”는 내 인생 모토 중 큰 부분을 차지하는 것이기도해서 좋은 실험 기회가 되었다. 내가 도전한 부분은 AI를 통해 작곡(편곡)하기였다. 타겟 곡은 평소 좋아하던 Pudditorium의 If I could meet again이라는 음악이다. 반복적인 멜로디에 여러 악기가 덧입혀지면서 분위기가 고조되고 독특한 분위기를 풍기는 곡인데, 피아노로 따라치면 그 분위기는 모두 없어지고 선율만 남아 아쉬움이 컸다. 내가 표현할 수 없었던 피아노 이외의 부분을 AI로 표현하면 좋을 것 같아 이 곡을 선정하게 되었다. . 원곡을 들어보자 (클릭하면 재생) . . 아름답다. 리더 김정범 씨가 어머니를 생각하며 만든 곡이라고 한다. . 이 훌륭한 곡을 잘 들어보면 아래 멜로디 라인이 반복됨을 알 수 있다. (이렇게 간단한 멜로디에 아름다운 음악이라니!) . . 이 멜로디를 토대로 구글 마젠타 스튜디오를 이용해서 내가 혼자 연주할 때보다 업그레이드 된 새로운 느낌의 곡을 만들어보았다. . 마젠타로 재현한 음악 (클릭하면 재생) . . 위 곡을 만들기 위해 사용한 Magenta Studio는 아래 사이트에 가면 확인해 볼 수 있다. Magenta Studio는 machine learning을 이용해서 music generation을 가능하게 하는 open source tool이다. 이것은 standalone으로도 동작하고 ableton DAW 상에서 plugin 방식으로도 사용된다. . 구글 마젠타 스튜디오 : https://magenta.tensorflow.org/studio . . 내가 위 곡을 만들기 위해 진행한 과정은 다음과 같다. . 8마디 피아노 + 8마디 스트링 (직접 연주) . 곡의 뼈대를 만들기 위해 위 악보에 채보되어 있는 멜로디 라인을 처음에는 피아노, 그 다음에는 스트링을 이용해서 직접 연주 . | Magenta Drumify : Drum bit Generation . 곡에 드럼라인을 입히기 위해 Drumify를 이용해서 비트 생성 . | Magenta Arpeggio Generation . https://codepen.io/teropa/full/ddqEwj/ . 위 사이트에서 제공하는 아르페지오 생성기를 사용 . | Magenta Continue : Bass Solo Generation . 멜로디를 input으로 하여 그에 맞는 bass solo 라인 생성 (2개 종류) . | Magenta Interpolate : Additional Bass Solo . 4번에서 생성된 2개 bass solo를 input으로 받아 이 둘 라인을 자연스럽게 연결해 줄 수 있는 additional bass solo 생성 . | . 단상 . 곡을 만들기 위해 Magenta Studio를 이용해서 여러 시도를 해보고 조합하는데에는 시간이 별로 들지 않았다. 15분 정도, 충분힌 시간을 가지고 다양한 시도를 한다면 더 멋진 곡이 나올수도. 하지만 Ableton DAW를 처음 접해보는지라, 기능 숙지하는데에 시간이 오히려 더 많이 걸렸다. 또한 사람이 작곡하는 경우에는 멜로디 라인에 따라 의도하는 화성, 감정선이 있을건데 Magenta를 이용해서 generation을 해서 끼워넣을 경우 그 감성이 깨지는 듯한 인상도 받았다. 따라서 짧은 멜로디 진행에 더 잘 어울릴 것 같다. 그 밖에 자신이 주로 연주하는 악기 외에도 다양한 악기를 나의 input에 맞게 생성해 볼 수 있으니 창작자에게 좋은 seed가 될 수 있을 것이라고 생각한다. 마젠타에 흥미로운 주제의 프로젝트가 많으니 시도해본다면 시간가는 줄 모를 것이다. . AI를 이용해 사람의 창의성을 발현 시키는 것이 가능한 일일까? 처음 했던 이 질문에 대한 대답을 한다면, Absolutely, Yes. 다만 그것을 인간이 허용할지 안할지에 대한 의사결정이 필요할 뿐… . (참고영상) . .",
            "url": "https://inspiringpeople.github.io/tech_blog/data%20analysis/2019/07/08/magenta_studio.html",
            "relUrl": "/data%20analysis/2019/07/08/magenta_studio.html",
            "date": " • Jul 8, 2019"
        }
        
    
  
    
        ,"post6": {
            "title": "Happy Faces",
            "content": "AI가 인간을 이해하는데 도움을 줄 수 있을까. 사회가 정의한 ‘행복’이라는 것은 무엇일까. 또한 그것은 우리가 볼 수 있는 유형의 것으로 표현될 수 있을까.라는 질문들로 시작된, Creative AI 첫번째 프로젝트이다. . . 우선 많은 사람들의 행복한 (웃는) 이미지들을 모아서 ‘Happy Eigen Face’를 만들었다. 그리고 새로운 얼굴 표정이 입력되면 이 얼굴이 ‘Happy Eigen Face’로 투영해가는 과정을 표현했다. 한 사람의 감정이 사회의 감정으로 투영해 가는 과정을 그린 것이다. 너와 내가 다르게 보일지라도 우리는 어떤 공통된 모습이 있고, 그 지점으로부터 모두 연결될 수 있다는 메시지를 주고 싶었다. 사람이 아니라서, 동떨어진 시선으로 볼 수 있는 AI라서, 그것으로부터 제공된 ‘공통요소, 동질감’이라는 것은 어쩌면 가끔 쿨하게 받아들여질 수 있지 않을까. 아래에 작업한 모델링 관련 소스 코드와 완성된 영상이 공유한다. 작업은 모두연 DLC 3기 전도희님, 곽현일님과 같이 작업하였고, 나는 모델링 부분을, 두 분은 전처리와 시각화 부분을 맡아주셨다.(시각화 부분 짱, 나도 저렇게 언젠간 interactive한 시각화를 해보고 싶다.) 어쩌면 작업 전, 후에 의도했던 결들이 조금씩 달라졌을지 모르지만, 사람을 돕는 AI라는 주제는 여전히 같다. 프로젝트 팀원분들 모두 적극적이고 아이디어가 샘솟아서 작업하면서 너무 즐거웠다. . . 우선 모델링 결과로 나온 이미지를 interactive하게 시각화한 데모 영상을 먼저 보자. 원래는 마우스 훨이나 키보드 입력에 따라 동적으로 움직이는데, 영상에는 일부만 표현됨 (클릭 시 동영상 재생). . . . 이제 모델링 부분 공유 시작. 사회가 정의한 행복을 표현하기 위해 우선 아래 링크의 Open dataset에서 얼굴 사진을 획득하고 웃는 얼굴만 사용하기 위해 Face_Emotion_Detecion이라는 Pre-trained Model을 사용하였다 . Dataset : scikit learn open data set 10명의 다양한 표정 데이터 400장 보유 . Pre-trained Model : Face_emotion_detection model . . 작업순서 . Pretraining 된 face emotion model을 이용하여 data set 중 happy로 라벨링된 표정 사진만 추출 | 10명의 총 400장 사진 중 108장이 추출됨 | 108장의 happy한 표정 데이터에 대한 eigen face를 도출하여 2차원으로 표현 (AI로 표현되는 Happy face) | Happy로 대표되는 eigen face에 나/나의 얼굴은 어떻게 투영될 것인가? | Input 이미지가 Eigen face에 투영되는 과정을 weight를 변화시켜가며 표현 | . # -*- coding: utf-8 -*- import cv2 import numpy as np from keras.models import load_model import sys ##Satart Section &#39;&#39;&#39; Keras took all GPU memory so to limit GPU usage, I have add those lines&#39;&#39;&#39; import tensorflow as tf from keras.backend.tensorflow_backend import set_session config = tf.ConfigProto() config.gpu_options.per_process_gpu_memory_fraction = 0.1 set_session(tf.Session(config=config)) &#39;&#39;&#39; Keras took all GPU memory so to limit GPU usage, I have add those lines&#39;&#39;&#39; ## End section import tensorflow.keras from keras.preprocessing import image from keras.applications.imagenet_utils import decode_predictions, preprocess_input from keras.models import Model . Using TensorFlow backend. . 데이터 셋 다운로드 . from sklearn.datasets import fetch_olivetti_faces from sklearn.decomposition import PCA import matplotlib.pyplot as plt import numpy as np faces_all = fetch_olivetti_faces() . Face_emotion_detection을 위해 pre-trained된 모델과 xml 파일 다운로드해서 colab에 업로드 &amp; 로딩 . faceCascade = cv2.CascadeClassifier(&#39;haarcascade_frontalface_alt2.xml&#39;) model = load_model(&#39;model_5-49-0.62.hdf5&#39;) . WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating: Colocations handled automatically by placer. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version. Instructions for updating: Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.cast instead. /usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:327: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer. warnings.warn(&#39;Error in loading the saved optimizer &#39; . model.summary() . _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 64, 48, 48) 640 _________________________________________________________________ dropout_1 (Dropout) (None, 64, 48, 48) 0 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 64, 24, 24) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 128, 24, 24) 204928 _________________________________________________________________ dropout_2 (Dropout) (None, 128, 24, 24) 0 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 128, 12, 12) 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 512, 12, 12) 590336 _________________________________________________________________ dropout_3 (Dropout) (None, 512, 12, 12) 0 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 512, 6, 6) 0 _________________________________________________________________ conv2d_4 (Conv2D) (None, 512, 6, 6) 2359808 _________________________________________________________________ dropout_4 (Dropout) (None, 512, 6, 6) 0 _________________________________________________________________ max_pooling2d_4 (MaxPooling2 (None, 512, 3, 3) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 4608) 0 _________________________________________________________________ dense_1 (Dense) (None, 256) 1179904 _________________________________________________________________ dropout_5 (Dropout) (None, 256) 0 _________________________________________________________________ dense_2 (Dense) (None, 512) 131584 _________________________________________________________________ dropout_6 (Dropout) (None, 512) 0 _________________________________________________________________ dense_3 (Dense) (None, 7) 3591 ================================================================= Total params: 4,470,791 Trainable params: 4,470,791 Non-trainable params: 0 _________________________________________________________________ . Pre-trained 모델을 사용하여 새로운 얼굴 사진이 들어오면 다음 7가지 감정 중 하나로 라벨링한다. . ‘angry’, ‘disgust’, ‘fear’, ‘happy’, ‘sad’, ‘surprise’ ,’neutral’ | . def test_image(addr): target = [&#39;angry&#39;,&#39;disgust&#39;,&#39;fear&#39;,&#39;happy&#39;,&#39;sad&#39;,&#39;surprise&#39;,&#39;neutral&#39;] font = cv2.FONT_HERSHEY_SIMPLEX faces = addr faces = cv2.resize(faces,(48,48)) faces = faces.reshape(1, 1,faces.shape[0],faces.shape[1]) result = target[np.argmax(model.predict(faces))] #print(result) return result . print(len(faces_all.images)) h_imgs=[] h_data=[] for i in range(len(faces_all.images)): if(test_image(faces_all.images[i])==&#39;happy&#39;): h_imgs.append(faces_all.images[i]) h_data.append(faces_all.data[i]) . 400 . print(len(h_imgs)) . 108 . 모델을 이용하여 happy로 라벨링 된 이미지 확인 전체 400장 얼굴 사진 중 108장의 얼굴이 Happy로 라벨링되었고, 이 사진들을 확인해본다. . N = 10 M = 10 fig = plt.figure(figsize=(10, 10)) plt.subplots_adjust(top=1, bottom=0, hspace=0, wspace=0.05) for i in range(N): for j in range(M): k = i * M + j ax = fig.add_subplot(N, M, k+1) ax.imshow(h_imgs[k], cmap=plt.cm.bone) ax.grid(False) ax.xaxis.set_ticks([]) ax.yaxis.set_ticks([]) plt.tight_layout() plt.show() . . 위 108장에 대한 Happy Face를 표현하는 주성분을 파악하기 위해 2개로 PCA 분석 진행한다 . from sklearn.decomposition import PCA pca3 = PCA(n_components=2) X3 = h_data W3 = pca3.fit_transform(X3) X32 = pca3.inverse_transform(W3) . 주성분으로 각 표정 데이터를 근사화 시키고 이미지를 저장함 . N = 10 M = 10 fig = plt.figure(figsize=(10, 10)) plt.subplots_adjust(top=1, bottom=0, hspace=0, wspace=0.05) for i in range(N): for j in range(M): k = i * M + j ax = fig.add_subplot(N, M, k+1) ax.imshow(X32[k].reshape(64, 64), cmap=plt.cm.bone) #cv2.imwrite(str(i)+&#39;_&#39;+str(j)+&#39;_happy_common.jpg&#39;, X32[k].reshape(64, 64)*255) ax.grid(False) ax.xaxis.set_ticks([]) ax.yaxis.set_ticks([]) #plt.suptitle(&quot;PCA : Happy faces&quot;) plt.tight_layout() plt.show() . . pca3.mean_.shape . (4096,) . 주성분으로 표현된 eigen face, 여기서는 100장의 happy face를 대표하는 얼굴과 각 주성분 PCA1, PCA2의 이미지를 확인한다 . face_mean = pca3.mean_.reshape(64, 64) face_p1 = pca3.components_[0].reshape(64, 64) face_p2 = pca3.components_[1].reshape(64, 64) plt.subplot(131) plt.imshow(face_mean, cmap=plt.cm.bone) plt.grid(False) plt.xticks([]) plt.yticks([]) plt.title(&quot;Mean Face&quot;) plt.subplot(132) plt.imshow(face_p1, cmap=plt.cm.bone) plt.grid(False) plt.xticks([]) plt.yticks([]) plt.title(&quot;PCA 1&quot;) plt.subplot(133) plt.imshow(face_p2, cmap=plt.cm.bone) plt.grid(False) plt.xticks([]) plt.yticks([]) plt.title(&quot;PCA 2&quot;) plt.show() . . mean_face, pca1_face, pca2_face 저장 . cv2.imwrite(&#39;mean_face.jpg&#39;, face_mean*255) cv2.imwrite(&#39;p1_face.jpg&#39;, face_p1*255) cv2.imwrite(&#39;p2_face.jpg&#39;, face_p2*255) . True . face_mean.shape . (64, 64) . 특정 인물의 사진을 happy eigen face로 투영해가는 과정 . happy face 얼굴 중 0번째 얼굴을 특정 인물이라고 가정 | 특정 인물 -&gt; happy eigen face 변화를 50개 레벨로 세분화해서 표현 | 각 이미지의 가로/세로 padding을 2씩 넣어서 저장 | . plt.imshow(h_imgs[0], cmap=plt.cm.bone) cv2.imwrite(&#39;org.jpg&#39;, h_imgs[0]*255) . True . . .",
            "url": "https://inspiringpeople.github.io/tech_blog/data%20analysis/2019/05/20/MyHappyfaces.html",
            "relUrl": "/data%20analysis/2019/05/20/MyHappyfaces.html",
            "date": " • May 20, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://inspiringpeople.github.io/tech_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://inspiringpeople.github.io/tech_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}