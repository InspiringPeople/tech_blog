{
  
    
        "post0": {
            "title": "Bert",
            "content": "BERT . By Google (2018.10 ë…¼ë¬¸ì´ ê³µê°œ, 11ì›”ì— ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ê³µê°œ ë¨) . | BERTëŠ” Bidirectional Encoder Representations from Transformers . | â€œAttention is all you needâ€ (2017ë…„ ë…¼ë¬¸)ì—ì„œ ì†Œê°œí•œ Transformerì˜ Encoder ë¶€ë¶„ ì‚¬ìš© . | Github : https://github.com/google-research/bert . | Paper : https://arxiv.org/abs/1810.04805 . | . . [Transformer Architecture] . 1) Transformer Encoder . 2) Transformer Decoder . Bertì—ì„œëŠ” Transformerì˜ Encoder Blockë§Œì„ ì‚¬ìš© . Bert base ëª¨ë¸ì€ Encoder Blockì„ 12ê°œ, Bert large ëª¨ë¸ì€ 24ê°œ ìŒ“ì•„ì„œ ë§Œë“¬. . (Transformerì˜ Decoder Blockì„ ì“°ëŠ” ëª¨ë¸ì€ GPT ê³„ì—´ ëª¨ë¸, Encoder / Decoder Blockì„ ëª¨ë‘ ì“°ëŠ” ëª¨ë¸ì€ T5 ë“±ì´ ìˆë‹¤) . ì£¼ìš” ê°œë… . Bertì˜ ìì„¸í•œ ë‚´ìš©ì„ ì‚´í´ë³´ê¸° ì „ì—, Bertë¥¼ ì´í•´í•˜ê¸° ìœ„í•œ ì£¼ìš” ê°œë…ë“¤ì— ëŒ€í•´ ì†Œê°œí•œë‹¤. . Transfer Learning . . íŒ¨ëŸ¬ë‹¤ì„ì˜ ì „í™˜! . BERTëŠ” ê¸°ë³¸ì ìœ¼ë¡œ, wikië‚˜ book dataì™€ ê°™ì€ ëŒ€ìš©ëŸ‰ unlabeled dataë¡œ ëª¨ë¸ì„ ë¯¸ë¦¬ í•™ìŠµ ì‹œí‚¨ í›„, íŠ¹ì • taskë¥¼ ê°€ì§€ê³  ìˆëŠ” labeled dataë¡œ transfer learningì„ í•˜ëŠ” ëª¨ë¸ì´ë‹¤. ì´ë•Œ unlabeled dataë¡œ í•™ìŠµí•˜ëŠ” ê³¼ì •ì„ pre-training (upstream task), labeled dataë¡œ í•™ìŠµí•˜ëŠ” ê³¼ì •ì„ fine-tuning (downstream task)ë¼ê³  ì§€ì¹­í•œë‹¤. | . . Bertì˜ Pre-training / Fine-tuning . BertëŠ” wiki, book corpus ë°ì´í„°ë¡œ pre-trainingì„ ìˆ˜í–‰í•˜ì—¬ General purposeì˜ ì–¸ì–´ëª¨ë¸ (Language Model)ì„ í•™ìŠµí•˜ê³ , ë‹¤ì–‘í•œ taskì— ë”°ë¼ (Glue, Super Glue ë“±) fine-tuning ëª¨ë¸ì„ ìƒì„±í•˜ëŠ” ë°©ì‹ì´ë‹¤. ì´ë¡œì¨ ì ì€ labeling dataë¡œë„ SOTA ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤. . Language Modelì€ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ë¬¸ë§¥ì„ í•™ìŠµí• ê¹Œ? (Transformer ê³„ì—´ ëª¨ë¸) . Attention . Attentionì´ë€? . ì¸ê°„ì€ ì •ë³´ì²˜ë¦¬ë¥¼ í•  ë•Œ ëª¨ë“  sequenceë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠìŒ. ì°¾ì•„ì•¼ í•˜ëŠ” ì£¼ìš” ì •ë³´ê°€ ë¬´ì—‡ì¸ì§€ ì•Œ ìˆ˜ ìˆë‹¤ë©´ ì´ì— ê°€ì¤‘ì¹˜ë¥¼ ì£¼ì–´, ì£¼ìš” ì •ë³´ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë§¥ë½ì„ íŒŒì•…í•  ìˆ˜ ìˆìŒ. Attentionì˜ ëª¨í‹°ë¸ŒëŠ” ìì‹ ì—ê²Œ ì˜ë¯¸ ìˆëŠ” featureë§Œ ê³¨ë¼ë‚´ì„œ ì¤‘ìš”í•˜ê²Œ íŒë‹¨í•˜ê² ë‹¤ëŠ” ê²ƒì„. . ë…¼ë¬¸ : Neural Machine Translation by Jointly Learning to Align and Translate (https://arxiv.org/abs/1409.0473), ì¡°ê²½í˜„ êµìˆ˜ë‹˜ . | ì–´í…ì…˜ ë§¤ì»¤ë‹ˆì¦˜ì€ ê¸°ê³„ë²ˆì—­(machine translation)ì„ ìœ„í•œ sequence-to-sequence ëª¨ë¸(S2S)ì— ì²˜ìŒ ë„ì… . | ë²ˆì—­ ì‹œ ì†ŒìŠ¤ë­ê·€ì§€ì™€ íƒ€ê²Ÿë­ê·€ì§€ì˜ ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆ ìˆ˜ë¡ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë‚˜ë¹ ì§, ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ëª¨ë¸ë¡œ í•˜ì—¬ê¸ˆ â€˜ì¤‘ìš”í•œ ë¶€ë¶„ë§Œ ì§‘ì¤‘(attention)í•˜ê²Œ ë§Œë“¤ìâ€™ëŠ” ê²ƒì´ ì–´í…ì…˜ ë§¤ì»¤ë‹ˆì¦˜ì˜ í•µì‹¬ ì•„ì´ë””ì–´ . | ì–´í…ì…˜ì€ weightsì˜ ì¤‘ìš”ë„ ë²¡í„°, ì´ë¯¸ì§€ì˜ í”½ì…€ê°’ì´ë‚˜ ë¬¸ì¥ì—ì„œ ë‹¨ì–´ ë“± ì–´ë–¤ ìš”ì†Œë¥¼ ì˜ˆì¸¡í•˜ê±°ë‚˜ ì¶”ì •í•˜ê¸° ìœ„í•´, ë‹¤ë¥¸ ìš”ì†Œë“¤ê³¼ ì–¼ë§ˆë‚˜ ê°•í•˜ê²Œ ì—°ê´€ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì–´í…ì…˜ ë°±í„°ë¡œ ê°€ì¤‘ í•©ì‚°ëœ ê°’ì˜ í•©ê³„ë¥¼ íƒ€ê²Ÿê°’ìœ¼ë¡œ ì¶”ì • . | . Input ì¤‘ì—ì„œ ë¬´ì—‡ì´ ì¤‘ìš”í•œì§€ëŠ” attention weightë¡œ í•™ìŠµì´ ê°€ëŠ¥í•˜ë©°, inputê³¼ outputì´ ì–´ë–»ê²Œ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ ì‹œê°í™”ê°€ ê°€ëŠ¥í•¨. (ì–´ë–¤ featureë“¤ì´ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ ì•Œ ìˆ˜ ìˆìŒ) . . ê¸°ì¡´ Attention ë°©ì‹ì˜ ë‹¨ì  . ë²ˆì—­ëª¨ë¸(Seq2Seq) Attention ë©”ì»¤ë‹ˆì¦˜ì˜ í•µì‹¬ì€ decoderì˜ íŠ¹ì • time-stepì˜ outputì´ encoderì˜ ëª¨ë“  time-stepì˜ output ì¤‘ ì–´ë–¤ time-stepê³¼ ê°€ì¥ ì—°ê´€ì´ ìˆëŠ”ê°€ì„. ì¼ë°˜ì ì¸ Seq2Seq-Attention ëª¨ë¸ì—ì„œì˜ ë²ˆì—­ íƒœìŠ¤í¬ì˜ ë¬¸ì œëŠ” ì›ë³¸ ì–¸ì–´(Source Language), ë²ˆì—­ëœ ì–¸ì–´(Target Language)ê°„ì˜ ëŒ€ì‘ ì—¬ë¶€ëŠ” Attentionì„ í†µí•´ ì°¾ì„ ìˆ˜ ìˆì—ˆìŒ. (input ë‚˜ëŠ” ì†Œë…„ì´ë‹¤ &lt;-&gt; output I am a boy ê°„ì— â€˜ë‚˜â€™ ì™€ â€˜Iâ€™, â€˜ì†Œë…„â€™ê³¼ â€˜boyâ€™ì˜ ëŒ€ì‘ê´€ê³„) . | ê·¸ëŸ¬ë‚˜ ê° ë¬¸ì¥ì—ì„œì˜ ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ì—†ìŒ. â€˜I love tiger but it is scareâ€™ì—ì„œ â€˜itâ€™ì´ ë¬´ì—‡ì„ ë‚˜íƒ€ë‚´ëŠ”ì§€ ì™€ ê°™ì€ ë¬¸ì œëŠ” ê¸°ì¡´ Encoder-Decoder ê¸°ë°˜ì˜ Attentionë©”ì»¤ë‹ˆì¦˜ì—ì„œëŠ” ì°¾ì„ ìˆ˜ ì—†ì—ˆìŒ. . | Attention ëª¨ë¸ì€ Seq2seqì˜ ì„±ëŠ¥ì„ ë†’ì˜€ìœ¼ë‚˜, RNNì´ ìˆœì°¨ë¡œ ì´ë¤„ì ¸ ì—°ì‚°ì´ ëŠë¦¬ë‹¤ëŠ” êµ¬ì¡°ì ì¸ ë¬¸ì œì ì´ ìˆìŒ. ë˜í•œ ë³‘ë ¬ì²˜ë¦¬ê°€ ë¶ˆê°€ëŠ¥í•¨. ì´ëŸ° ë‹¨ì ì„ í•´ê²°í•˜ê¸° ìœ„í•´ RNNì„ ì—†ì• ëŠ” ì•„ì´ë””ì–´ê°€ ë‚˜ì˜¤ê²Œ ë¨. . | . Self-attention . Self-attentionì´ë€? . 2017ë…„ êµ¬ê¸€ì´ ë°œí‘œí•œ ë…¼ë¬¸ì¸ â€œAttention is all you needâ€ (https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)ì—ì„œ ì œì•ˆëœ transformer modelì—ì„œ ì†Œê°œëœ ë°©ë²• . | ê¸°ì¡´ì˜ seq2seqì˜ êµ¬ì¡°ì¸ ì¸ì½”ë”-ë””ì½”ë” ë”°ë¥´ë©´ì„œë„ ìˆœí™˜ì‹ ê²½ë§Œ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì„±ëœ ê¸°ì¡´ ëª¨ë¸ê³¼ ë‹¤ë¥´ê²Œ ë‹¨ìˆœíˆ ì–´í…ì…˜ êµ¬ì¡°ë§Œìœ¼ë¡œ ì „ì²´ ëª¨ë¸ì„ ë§Œë“¤ì–´ ì–´í…ì…˜ ê¸°ë²•ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡° . | ë¬¸ì¥ ë‚´ì—ì„œ tokenë“¤ì´ ìŠ¤ìŠ¤ë¡œë¥¼ ì˜ í‘œí˜„í•˜ëŠ” context vectorë¥¼ ê°–ê²Œ ë§Œë“¦. . | . . ë°‘ì¤„ ì¹œ ë¶€ë¶„ì— ë”°ë¼ itì— ëŒ€í•œ ê°€ì¤‘ì¹˜ê°€ ë‹¬ë¼ì§ . Bert has multi-head attention : Bert is multi-headed beast! . Bert base ëª¨ë¸ì˜ ê²½ìš° Layer 12, attention head 12ê°œ architectureë¥¼ ê°€ì§€ê³  ìˆë‹¤. . | ë”°ë¼ì„œ 12*12 = 144, 144ê°œì˜ distinct attentionì„ ê°–ê²Œ ë¨ . | . . L2, H0: Attention to next word . . L6, H11: Attention to previous word . . L2, H1: Attention to other words predictive of word . ê¸°ì¡´ì˜ word2vec ê°™ì€ ê²½ìš° í•˜ë‚˜ì˜ ë‹¨ì–´ëŠ” í•˜ë‚˜ì˜ vector ê°’ë§Œì„ í‘œí˜„í•˜ì˜€ì§€ë§Œ, transformer ê³„ì—´ ëª¨ë¸ì˜ ê²½ìš° multi-headed attentionì„ ì´ìš©í•¨ìœ¼ë¡œì¨ í•˜ë‚˜ì˜ ë‹¨ì–´ê°€ ì—¬ëŸ¬ ê°œì˜ â€œrepresentation ê³µê°„â€ì„ ê°€ì§ˆ ìˆ˜ ìˆê²Œ ë¨. . 2. Pre-training . Bertì˜ pre-training ê³¼ì •ì€ í¬ê²Œ Input Embedding, MLM, NSP 3ê°€ì§€ ê³¼ì •ìœ¼ë¡œ ë‚˜ëˆ ì§„ë‹¤. . Pre-training data (BERT base) . BookCorpus (Zhu et al., 2015) (800M words) + English Wikipedia (2,500M words) . | Unsupervised Learningìœ¼ë¡œ ì§„í–‰ë¨ . | long contiguous sequenceë§Œì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ WikipediaëŠ” text passageë§Œ ì¶”ì¶œí•˜ì—¬ ì‚¬ìš© . | . . BERTì˜ Pre-trainingê³¼ Fine-tuning . BERTì˜ Pre-trainingì€ tokenizationì„ ìˆ˜í–‰í•˜ì—¬ embedding ëœ Sentence 2ê°œë¥¼ Input Dataë¡œ ì‚¬ìš©í•œë‹¤. (Input Data ìƒì„±ì— ëŒ€í•´ì„œëŠ” í›„ìˆ ) Input ê°’ì€ Transformer layerë¥¼ ê±°ì¹˜ë©° ë¬¸ì¥ ë‚´ì—ì„œ ë‹¨ì–´(token)ìŠ¤ìŠ¤ë¡œë¥¼ ê°€ì¥ ì˜ í‘œí˜„í•˜ëŠ” Self-attentionëœ token vector(Contextual representation of token), ê·¸ë¦¬ê³  í›„ìˆ í•  NSP(Next Sentence Prediction)ì™€ MLM(Masked Language Model)ì„ ê±°ì¹˜ë©° ì–¸ì–´ì˜ ë§¥ë½ ì •ë³´ë¥¼ ì €ì¥í•œ ëª¨ë¸ì„ êµ¬ì¶•í•¨. . ì´ í›„ Fine-tuning ê³¼ì •ì—ì„œëŠ” pre-training ëœ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬, MNLI, NER, SQuADì™€ ê°™ì€ ë‹¤ì–‘í•œ NLP Classification taskë¥¼ ìˆ˜í–‰í•˜ëŠ” labeled train dataë¥¼ í•™ìŠµì‹œì¼œ ë¶„ë¥˜ë¥¼ í•˜ê²Œ ë¨. . 2) Tokenization . Subword segmentation . ë‹¨ì–´ ë¶„ë¦¬(Subword segmenation) ì‘ì—…ì€ í•˜ë‚˜ì˜ ë‹¨ì–´ëŠ” (ë‹¨ì–´ë³´ë‹¤ ì‘ì€ ë‹¨ìœ„ì˜) ì˜ë¯¸ìˆëŠ” ì—¬ëŸ¬ ë‚´ë¶€ ë‹¨ì–´ë“¤(subwords)ì˜ ì¡°í•©ìœ¼ë¡œ êµ¬ì„±ëœ ê²½ìš°ê°€ ë§ê¸° ë•Œë¬¸ì—, í•˜ë‚˜ì˜ ë‹¨ì–´ë¥¼ ì—¬ëŸ¬ ë‚´ë¶€ ë‹¨ì–´ë¡œ ë¶„ë¦¬í•´ì„œ ë‹¨ì–´ë¥¼ ì´í•´í•´ë³´ê² ë‹¤ëŠ” ì˜ë„ë¥¼ ê°€ì§„ ì „ì²˜ë¦¬ ì‘ì—… . | ë” ì ì€ vocabìœ¼ë¡œ ë” ë§ì€ ë‹¨ì–´ë¥¼ í‘œí˜„í•˜ê³  out of vocabulary ë°©ì§€ . | . Bertì˜ Tokenizationì€ Subword segmentationì˜ í•˜ë‚˜ì¸ WPM(word piece model, by Google) ê¸°ë°˜ìœ¼ë¡œ tokenization ëœë‹¤. . Bert Tokenizer ëª¨ìŒ . Tokenization ê³¼ì • . Space ë‹¨ìœ„ë¡œ token ë¶„ë¦¬ . | Pretrained ëœ WPM tokenizer ê¸°ì¤€ìœ¼ë¡œ subwords segmentation . | ë¬¸ì¥ ì‹œì‘ì— [CLS], ë¬¸ì¥ ëì— [SEP], max_seq_lenì— ë”°ë¼ [PAD] special token ë¶™ì´ê¸° . | . 3) Input Embedding . Tokenizationëœ ë¬¸ì¥ì€ Token Embeddings, Sentence Embeddings, Positional Embedingsë¥¼ í•©ì³ ëª¨ë¸ì— ì…ë ¥í•  ìˆ˜ ìˆëŠ” Input Embedding ê°’ìœ¼ë¡œ ë³€í™˜ëœë‹¤. . . BERTëŠ” Input dataë¡œ 3 ê°€ì§€ì˜ vectorë“¤ì´ í•©ì³ì„œ ë§Œë“¤ì–´ì§ . 1) Token Embeddings : Inputì— ë“¤ì–´ê°ˆ 2ê°œì˜ ë¬¸ì¥ì„ ë‹¨ì–´ token ë‹¨ìœ„ë¡œ ìë¥´ê³  tokenì„ ìˆ˜í•™ì ìœ¼ë¡œ í‘œí˜„í•¨. ì´í›„ NSLì™€ MLMì„ ì‹¤ì‹œí•˜ê¸° ìœ„í•´, sentence ìˆœì„œë¥¼ 50% í™•ë¥ ë¡œ ë³€ê²½í•˜ê³ , 15%ì˜ tokenì„ maskingí•¨. Masked tokenì˜ 80%ëŠ” masked([MSK])ë˜ê³ , 10%ëŠ” ê·¸ëŒ€ë¡œ, 10%ëŠ” ë‹¤ë¥¸ tokenìœ¼ë¡œ ì¹˜í™˜í•˜ê²Œ ë¨. special tokenìœ¼ë¡œëŠ” ë¬¸ì¥ ê°€ì¥ ì•([CLS])ê³¼ ë¬¸ì¥ì´ ë¶„ë¦¬ë˜ëŠ” ë¶€ë¶„([SEP])ì— ë“¤ì–´ê°€ê²Œ ë¨. . 2) Segment Embeddings : ë¬¸ì¥ì˜ ìˆœì„œë¥¼ Embeddingí•¨. ê·¸ë¦¼ ìƒì—ì„œ A, Bë¡œ í‘œí˜„ë¨. . 3) Position Embeddings : ë‹¨ì–´ì˜ ìˆœì„œë¥¼ Embeddingí•¨. (TransformerëŠ” RNNì´ë‚˜ CNNì²˜ëŸ¼ dataë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì—°ì‚°í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ë‹¨ì–´ì™€ ë¬¸ì¥ì˜ ìˆœì„œì •ë³´ë¥¼ tokenì— ë³„ë„ë¡œ ë„£ì–´ì¤˜ì•¼ í•¨) ë˜í•œ ê°™ì€ ë‹¨ì–´ë¼ë„ ì“°ì¸ ìœ„ì¹˜ì— ë”°ë¼ ë‹¤ë¥¸ ì„ë² ë”© ê°’ì„ ê°€ì§ˆ ìˆ˜ ìˆìŒ . . 4 )MLM (Masked Language Model) . token embedding ê³¼ì •ì—ì„œ ê° ë¬¸ì¥ tokenì„ 15% í™•ë¥ ë¡œ masked í•˜ê²Œ ë˜ë©°, (ë¬¸ë§¥ì„ ê³„ì‚°í•˜ì—¬) masked tokenì´ ë¬´ì—‡ì¸ì§€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” í•™ìŠµì„ ì§„í–‰í•¨. . . ì…ë ¥ tokenì˜ 15%ë¥¼ ì„ íƒí•˜ì—¬ 80% [MASK], 10% ëœë¤í† í°ëŒ€ì²´, 10% ìœ ì§€ . . 5) NSP (Next Sentence Prediction) . NSPë¥¼ ìœ„í•´ corpusì—ì„œ ì‹¤ì œ next sentenceì™€ random sentenceë¥¼ 50% í™•ë¥ ë¡œ ê°€ì ¸ì˜¤ê²Œ ë˜ë©°, ì´ë¥¼ ê°€ì§€ê³  IsNextì¸ì§€ NotNextì¸ì§€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” í•™ìŠµì„ ì§„í–‰. (Segment embeddingìœ¼ë¡œ ë¬¸ì¥ì˜ ìˆœì„œ ì •ë³´ë¥¼ ë³´ì¡´) . sentence_a = â€œì‹ ìš© ëŒ€ì¶œì„ ì‹ ì²­ í•˜ê³  ì‹¶ì–´ìš”â€ . sentence_b = â€œëŒ€ì¶œ ê´€ë ¨ ì„œë¥˜ë¥¼ ì¤€ë¹„í•´ ì˜¤ì…¨ì–´ìš”?â€ . Label: IsNext . sentence_a = â€œì‹ ìš© ëŒ€ì¶œì„ ì‹ ì²­ í•˜ê³  ì‹¶ì–´ìš”â€ . sentence_b = â€ì–´ë–¤ íŒ€ì´ ì˜¬í•´ í”„ë¡œì•¼êµ¬ ìš°ìŠ¹ í–ˆë‚˜ìš”?â€ . Label: NotNext . | . MLM &amp; NSP ë°©ì‹ . . 3. Fine-tuning(Classification Layer) . Pre-trainingëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ NLP taskë¥¼ í•˜ëŠ”ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŒ. í¬ê²Œ 4ê°€ì§€ ìœ í˜•ìœ¼ë¡œ êµ¬ë¶„ëœë‹¤. . ëª¨ë¸ì€ pre-trained parametersë¡œ ì´ˆê¸°í™” ë˜ê³  ê°ê° downstream task ë³„ë¡œ ë‹¤ë¥¸ ëª¨ë¸ì´ ë§Œë“¤ì–´ì§„ë‹¤. ë¬¸ë§¥ì„ ì´í•´í•˜ëŠ” Language Modelìœ„ì— fine-tuning ë°©ì‹ìœ¼ë¡œ í•™ìŠµë˜ë¯€ë¡œ ì ì€ label ë°ì´í„°ë¡œë„ ë†’ì€ ì •í™•ë„ë¥¼ ê°–ëŠ” ëª¨ë¸ì„ ìƒì„±í•  ìˆ˜ ìˆë‹¤. . . (a), (b)ëŠ” sequence-level task, (c)ì™€ (d)ëŠ” token-level taskë¡œ ê°ê°ì˜ íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ìŒ. . (a)ëŠ” 2ê°œ sentenceë¥¼ ë„£ê³  ê·¸ ê´€ê³„(pair classification)ë¥¼ ë¶„ë¥˜í•˜ëŠ” taskì„. ë¬¸ì¥ì˜ ìœ ì‚¬ ê´€ê³„ ë¶„ë¥˜ ë“±ì— ì‚¬ìš©ë˜ë©° (b)ëŠ” 1ê°œ sentenceë¥¼ ë„£ê³  ê°ì„±ê³¼ ê°™ì€ ë¬¸ì¥ì˜ íŠ¹ì„±ì„ ë¶„ë¥˜í•¨ . (c)ëŠ” ì§ˆë¬¸(Question)ê³¼ ë³¸ë¬¸(Paragraph)ì„ ë„£ê³  ë¬¸ì¥ ë‹¨ìœ„ì˜ ë‹µì„ í˜¸ì¶œí•¨. paragraphì—ì„œ Qì™€ ìœ ì‚¬ë„ê°€ ë†’ì€ ë¬¸ì¥ì„ ë‹µìœ¼ë¡œ ë‚´ë†“ê¸° ìœ„í•´, [SEP] token ì´í›„ì˜ 00ë²ˆì§¸ tokenë¶€í„° 00ë²ˆì§¸ tokenê¹Œì§€ë¥¼ í˜¸ì¶œí•¨. . d)ëŠ” 1ê°œ ë¬¸ì¥ì„ ë„£ê³ , ë¬¸ì¥ ë‚´ tokenì˜ ì •ë³´ë¥¼ ë¶„ë¥˜í•¨. NER(Named Entity Recognition, ê°œì²´ëª… ì¸ì‹)ì´ë‚˜ í˜•íƒœì†Œ ë¶„ì„ê³¼ ê°™ì´ single sentenceì—ì„œ ê° tokenì´ ì–´ë–¤ classë¥¼ ê°–ëŠ”ì§€ ëª¨ë‘ classifier ì ìš©í•˜ì—¬ ì •ë‹µì„ ì°¾ì•„ëƒ„. .",
            "url": "https://inspiringpeople.github.io/tech_blog/2020/09/10/BERT.html",
            "relUrl": "/2020/09/10/BERT.html",
            "date": " â€¢ Sep 10, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Distilbert",
            "content": "distilbert . a distilled version of BERT: smaller, faster, cheaper and lighter . Hugging face, 2019.10 . ë…¼ë¬¸ : https://arxiv.org/abs/1910.01108 . github : https://github.com/huggingface/transformers/tree/master/examples/distillation . í•´ë‹¹ githubì—ì„œ ì œì•ˆ ë°©ë²•ìœ¼ë¡œ ìƒì„±í•œ ë‹¤ë¥¸ distil* ëª¨ë¸ ì†Œê°œ (DistilGPT2, DistilRoBERTa, DistilmBERT) . abstract . ìµœê·¼ pretrain -&gt; fine tuningìœ¼ë¡œ ê°€ëŠ” ë°©ë²•ì´ ë§ì•„ì§€ê³  í”í•´ì¡Œì§€ë§Œ, ëª¨ë¸ ìì²´ê°€ ë„ˆë¬´ í¬ê¸° ë•Œë¬¸ì— ì œí•œëœ í™˜ê²½ì—ì„œëŠ” êµ‰ì¥íˆ ì‚¬ìš©í•˜ê¸° í˜ë“¬ | huggingfaceì—ì„œ DistilBertë¼ëŠ” general purpose language representation model ì œì•ˆ | BERTë¥¼ 40% ì •ë„ ì¤„ì´ê³  60%ë‚˜ ë¹ ë¥´ê²Œ ì—°ì‚°í•˜ë©´ì„œ 97%ì˜ ì„±ëŠ¥ì„ ìœ ì§€ | . distilation (knowledge extraction) . Knowledge Distillation [Bucila et al., 2006, Hinton et al., 2015]ì€ larger model(teacher model)ë¡œë¶€í„° compact model(student model)ì„ ë§Œë“¤ì–´ë‚´ëŠ” ë°©ë²•ì´ë‹¤. ì´ê²Œ ì‘ì€ ëª¨ë¸ì„ ë°”ë¡œ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒë³´ë‹¤ ì˜ë¯¸ìˆëŠ” ì´ìœ ëŠ” near-zeroì¸ í™•ë¥ ë“¤ë„ í•™ìŠµí•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. . ëª¨ë¸ì´ ë§Œë“¤ì–´ë‚¸ ê²°ê³¼ ë¶„í¬ê°€ ì‹¤ì œ ì´ë¯¸ì§€ë¥¼ ë” í’ë¶€í•˜ê²Œ í‘œí˜„í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì¦‰, ì›ë˜ ì •ë‹µ ê´€ì ì—ì„œëŠ” ì •ë‹µ ì´ì™¸ì— ëŒ€í•œ ì •ë³´ê°€ ì—†ì§€ë§Œ, í•œë²ˆ ëª¨ë¸ì—ì„œ í’€ì–´ë‚˜ì˜¨ ê²°ê³¼ëŠ” ì •ë‹µ ì™¸ì—ë„ ë‹¤ë¥¸ ë¬¼ì²´ì— ëŒ€í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆê²Œ ë©ë‹ˆë‹¤. ì´ë ‡ê²Œ ì •ë³´ê°€ ë¬»ì–´ ë‚˜ì˜¤ëŠ” ê²ƒì´ ë§ˆì¹˜ ì„ìœ ì˜ ë¶€ì‚°ë¬¼ë“¤ì´ ì¦ë¥˜íƒ‘ì—ì„œ ë‚˜ì˜¤ëŠ” ì–‘ìƒê³¼ ê°™ê¸° ë•Œë¬¸ì— ì´ë¥¼ ì§€ì‹ ì¦ë¥˜ë¼ê³  ì¼ì»«ìŠµë‹ˆë‹¤. ì›ë˜ ëª¨ë¸ì´ ìƒê°í•˜ëŠ” ë°ì´í„°ì˜ ì •ë³´ê°€ í’€ì–´ë‚˜ì˜¨ ë°ì´í„°ë¡œ ìƒˆë¡œ í•™ìŠµì‹œí‚¤ê²Œ ë˜ë©´ ê°„ì ‘ì ìœ¼ë¡œ ì„ ìƒ ëª¨ë¸ì´ í•™ìŠµí•œ ë°”ë¥¼ ë°˜ì˜í•˜ê²Œ ë˜ë¯€ë¡œ ë” íš¨ìœ¨ì ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆìœ¼ë©°, ìƒˆë¡œ ë°°ìš°ëŠ” ëª¨ë¸(í•™ìƒ ëª¨ë¸)ì€ ìƒëŒ€ì ìœ¼ë¡œ ë” ì ì€ ê·œëª¨ë¡œ êµ¬ì„±ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì´ Hinton et al. ì´ ì œì‹œí•œ ì§€ì‹ ì¦ë¥˜ì˜ í•µì‹¬ì…ë‹ˆë‹¤. . . distilbert . architecture token-type embedding and the pooler are removed | layers is reduced by a factor of 2 variations on the last dimension of the tensor(hidden size dimension) have a smaller impact on computation efficiency) | . | . | initialization important to find the right initialization for the sub-network to converge | initialize the student from teacher by taking one layer out of two | . | distilation large batches (up to 4K) | dynamic masking without NSP | . | data and computer power same data with BERT | on 8 16GB V100 GPUs for approximately 90 hours | . | downstream benchmark 97% of performance with 40% fewer parameter comparing with BERT | . | . . distilation ë°©ë²• (github) prepare data binarize the data, i.e. tokenize the data and convert each token in an index in our modelâ€™s vocabulary | . . | training | . | . loss function ë¬´ì—‡? . Loss function . . Teacher ëª¨ë¸ì—ì„œ ë‚˜ì˜¨ class probabilitiesë¥¼ ë°”ë¡œ Student modelì˜ target (soft target) ì´ìš©, knowledgeë¥¼ íš¨ê³¼ì ìœ¼ë¡œ transferí•˜ëŠ” ë°©ì‹ | MNISTë¥¼ ì˜ˆë¥¼ ë“¤ì–´ 2ë¥¼ ë§ì¶œ ë•Œ ë‹®ì€ ìˆ«ìì¸ 3ê³¼ 7ë„ ë‚®ì€ í™•ë¥ ì´ì§€ë§Œ ê°’ì„ ë¶€ì—¬, ì´ëŸ¬í•œ ì •ë³´ë“¤ì€ dataì˜ structureì— ëŒ€í•œ ì •ë³´ê°€ ë“¤ì–´ìˆëŠ” ê°’ì´ê¸° ë•Œë¬¸ì— ì¤‘ìš” ì •ë³´ | êµ‰ì¥íˆ ë‚®ì€ ê°’ìœ¼ë¡œ í‘œí˜„ë˜ê¸° ë•Œë¬¸ì— temperature ê°œë…ì„ ë„ì… | . | softmax-temperature . . TëŠ” Temperatureì´ê³ , T=1ì´ë¼ë©´ ë³´í†µì˜ softmax ì‹ | Tê°€ ì»¤ì§€ë©´ í›¨ì”¬ softí•œ probability distribution | Tê°€ output distributionì˜ smoothnessë¥¼ ê²°ì •. training ë™ì•ˆì—ë§ŒÂ Të¥¼ ì¡°ì •í•˜ê³  inference ì‹œê°„ì—ëŠ” 1ë¡œ ì„¤ì •í•´ì„œ standard softmaxë¡œ ì‚¬ìš© | . | final training lossëŠ” distillation lossÂ Lceì™€ BERTì—ì„œ ì‚¬ìš©í•œÂ Lmlmì˜ linear combination | Softmax(ì†Œí”„íŠ¸ë§¥ìŠ¤) . softmaxëŠ” ì…ë ¥ë°›ì€ ê°’ì„ ì¶œë ¥ìœ¼ë¡œ 0~1ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ëª¨ë‘ ì •ê·œí™”í•˜ë©° ì¶œë ¥ ê°’ë“¤ì˜ ì´í•©ì€ í•­ìƒ 1ì´ ë˜ëŠ” íŠ¹ì„±ì„ ê°€ì§„ í•¨ìˆ˜ . | . distil* . distilbertëŠ” bert-base-uncased tokenizerì™€ ê°™ì€ tokenizer | teacher modelê³¼ ë¹„ìŠ·í•˜ê²Œ ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ì‹ (bert, gpt2, roberta, betlm..) | DistilBERT uncased:Â model = DistilBertModel.from_pretrained(&#39;distilbert-base-uncased&#39;) | DistilGPT2:Â model = GPT2Model.from_pretrained(&#39;distilgpt2&#39;) | DistilRoBERTa:Â model = RobertaModel.from_pretrained(&#39;distilroberta-base&#39;) | DistilmBERT:Â model = DistilBertModel.from_pretrained(&#39;distilbert-base-multilingual-cased&#39;) | . ì°¸ê³  . Transformer - Harder, Better, Faster, Stronger . ğŸ Smaller, faster, cheaper, lighter: Introducing DilBERT, a distilled version of BERT . Distilling the Knowledge in a Neural Network . ğŸ“ƒ Distilling the Knowledge in a Neural Network ë¦¬ë·° .",
            "url": "https://inspiringpeople.github.io/tech_blog/2020/08/05/distilbert.html",
            "relUrl": "/2020/08/05/distilbert.html",
            "date": " â€¢ Aug 5, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "fastcore: An Underrated Python Library",
            "content": ". Background . I recently embarked on a journey to sharpen my python skills: I wanted to learn advanced patterns, idioms, and techniques. I started with reading books on advanced Python, however, the information didn&#39;t seem to stick without having somewhere to apply it. I also wanted the ability to ask questions from an expert while I was learning -- which is an arrangement that is hard to find! That&#39;s when it occurred to me: What if I could find an open source project that has fairly advanced python code and write documentation and tests? I made a bet that if I did this it would force me to learn everything very deeply, and the maintainers would be appreciative of my work and be willing to answer my questions. . And that&#39;s exactly what I did over the past month! I&#39;m pleased to report that it has been the most efficient learning experience I&#39;ve ever experienced. I&#39;ve discovered that writing documentation forced me to deeply understand not just what the code does but also why the code works the way it does, and to explore edge cases while writing tests. Most importantly, I was able to ask questions when I was stuck, and maintainers were willing to devote extra time knowing that their mentorship was in service of making their code more accessible! It turns out the library I choose, fastcore is some of the most fascinating Python I have ever encountered as its purpose and goals are fairly unique. . For the uninitiated, fastcore is a library on top of which many fast.ai projects are built on. Most importantly, fastcore extends the python programming language and strives to eliminate boilerplate and add useful functionality for common tasks. In this blog post, I&#39;m going to highlight some of my favorite tools that fastcore provides, rather than sharing what I learned about python. My goal is to pique your interest in this library, and hopefully motivate you to check out the documentation after you are done to learn more! . Why fastcore is interesting . Get exposed to ideas from other languages without leaving python: Iâ€™ve always heard that it is beneficial to learn other languages in order to become a better programmer. From a pragmatic point of view, Iâ€™ve found it difficult to learn other languages because I could never use them at work. Fastcore extends python to include patterns found in languages as diverse as Julia, Ruby and Haskell. Now that I understand these tools I am motivated to learn other languages. | You get a new set of pragmatic tools: fastcore includes utilities that will allow you to write more concise expressive code, and perhaps solve new problems. | Learn more about the Python programming language: Because fastcore extends the python programming language, many advanced concepts are exposed during the process. For the motivated, this is a great way to see how many of the internals of python work. | A whirlwind tour through fastcore . Here are some things you can do with fastcore that immediately caught my attention. . . Making **kwargs transparent . Whenever I see a function that has the argument **kwargs, I cringe a little. This is because it means the API is obfuscated and I have to read the source code to figure out what valid parameters might be. Consider the below example: . def baz(a, b=2, c =3, d=4): return a + b + c def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, **kwargs)&gt; . Without reading the source code, it might be hard for me to know that foo also accepts and additional parameters b and d. We can fix this with delegates: . def baz(a, b=2, c =3, d=4): return a + b + c @delegates(baz) # this decorator will pass down keyword arguments from baz def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4)&gt; . You can customize the behavior of this decorator. For example, you can have your cake and eat it too by passing down your arguments and also keeping **kwargs: . @delegates(baz, keep=True) def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4, **kwargs)&gt; . You can also exclude arguments. For example, we exclude argument d from delegation: . def basefoo(a, b=2, c =3, d=4): pass @delegates(basefoo, but= [&#39;d&#39;]) # exclude `d` def foo(c, a, **kwargs): pass inspect.signature(foo) . &lt;Signature (c, a, b=2)&gt; . You can also delegate between classes: . class BaseFoo: def __init__(self, e, c=2): pass @delegates()# since no argument was passsed here we delegate to the superclass class Foo(BaseFoo): def __init__(self, a, b=1, **kwargs): super().__init__(**kwargs) inspect.signature(Foo) . &lt;Signature (a, b=1, c=2)&gt; . For more information, read the docs on delegates. . . Avoid boilerplate when setting instance attributes . Have you ever wondered if it was possible to avoid the boilerplate involved with setting attributes in __init__? . class Test: def __init__(self, a, b ,c): self.a, self.b, self.c = a, b, c . Ouch! That was painful. Look at all the repeated variable names. Do I really have to repeat myself like this when defining a class? Not Anymore! Checkout store_attr: . class Test: def __init__(self, a, b, c): store_attr() t = Test(5,4,3) assert t.b == 4 . You can also exclude certain attributes: . class Test: def __init__(self, a, b, c): store_attr(but=[&#39;c&#39;]) t = Test(5,4,3) assert t.b == 4 assert not hasattr(t, &#39;c&#39;) . There are many more ways of customizing and using store_attr than I highlighted here. Check out the docs for more detail. . . Avoiding subclassing boilerplate . One thing I hate about python is the __super__().__init__() boilerplate associated with subclassing. For example: . class ParentClass: def __init__(self): self.some_attr = &#39;hello&#39; class ChildClass(ParentClass): def __init__(self): super().__init__() cc = ChildClass() assert cc.some_attr == &#39;hello&#39; # only accessible b/c you used super . We can avoid this boilerplate by using the metaclass PrePostInitMeta. We define a new class called NewParent that is a wrapper around the ParentClass: . class NewParent(ParentClass, metaclass=PrePostInitMeta): def __pre_init__(self, *args, **kwargs): super().__init__() class ChildClass(NewParent): def __init__(self):pass sc = ChildClass() assert sc.some_attr == &#39;hello&#39; . . Type Dispatch . Type dispatch, or Multiple dispatch, allows you to change the way a function behaves based upon the input types it receives. This is a prominent feature in some programming languages like Julia. For example, this is a conceptual example of how multiple dispatch works in Julia, returning different values depending on the input types of x and y: . collide_with(x::Asteroid, y::Asteroid) = ... # deal with asteroid hitting asteroid collide_with(x::Asteroid, y::Spaceship) = ... # deal with asteroid hitting spaceship collide_with(x::Spaceship, y::Asteroid) = ... # deal with spaceship hitting asteroid collide_with(x::Spaceship, y::Spaceship) = ... # deal with spaceship hitting spaceship . Type dispatch can be especially useful in data science, where you might allow different input types (i.e. Numpy arrays and Pandas dataframes) to a function that processes data. Type dispatch allows you to have a common API for functions that do similar tasks. . Unfortunately, Python does not support this out-of-the box. Fortunately, there is the @typedispatch decorator to the rescue. This decorator relies upon type hints in order to route inputs the correct version of the function: . @typedispatch def f(x:str, y:str): return f&#39;{x}{y}&#39; @typedispatch def f(x:np.ndarray): return x.sum() @typedispatch def f(x:int, y:int): return x+y . Below is a demonstration of type dispatch at work for the function f: . f(&#39;Hello &#39;, &#39;World!&#39;) . &#39;Hello World!&#39; . f(2,3) . 5 . f(np.array([5,5,5,5])) . 20 . There are limitations of this feature, as well as other ways of using this functionality that you can read about here. In the process of learning about typed dispatch, I also found a python library called multipledispatch made by Mathhew Rocklin (the creator of Dask). . After using this feature, I am now motivated to learn languages like Julia to discover what other paradigms I might be missing. . . A better version of functools.partial . functools.partial is a great utility that creates functions from other functions that lets you set default values. Lets take this function for example that filters a list to only contain values &gt;= val: . test_input = [1,2,3,4,5,6] def f(arr, val): &quot;Filter a list to remove any values that are less than val.&quot; return [x for x in arr if x &gt;= val] f(test_input, 3) . [3, 4, 5, 6] . You can create a new function out of this function using partial that sets the default value to 5: . filter5 = partial(f, val=5) filter5(test_input) . [5, 6] . One problem with partial is that it removes the original docstring and replaces it with a generic docstring: . filter5.__doc__ . &#39;partial(func, *args, **keywords) - new function with partial application n of the given arguments and keywords. n&#39; . fastcore.utils.partialler fixes this, and makes sure the docstring is retained such that the new API is transparent: . filter5 = partialler(f, val=5) filter5.__doc__ . &#39;Filter a list to remove any values that are less than val.&#39; . . Composition of functions . A technique that is pervasive in functional programming languages is function composition, whereby you chain a bunch of functions together to achieve some kind of result. This is especially useful when applying various data transformations. Consider a toy example where I have three functions: (1) Removes elements of a list less than 5 (from the prior section) (2) adds 2 to each number (3) sums all the numbers: . def add(arr, val): return [x + val for x in arr] def arrsum(arr): return sum(arr) # See the previous section on partialler add2 = partialler(add, val=2) transform = compose(filter5, add2, arrsum) transform([1,2,3,4,5,6]) . 15 . But why is this useful? You might me thinking, I can accomplish the same thing with: . arrsum(add2(filter5([1,2,3,4,5,6]))) . You are not wrong! However, composition gives you a convenient interface in case you want to do something like the following: . def fit(x, transforms:list): &quot;fit a model after performing transformations&quot; x = compose(*transforms)(x) y = [np.mean(x)] * len(x) # its a dumb model. Don&#39;t judge me return y # filters out elements &lt; 5, adds 2, then predicts the mean fit(x=[1,2,3,4,5,6], transforms=[filter5, add2]) . [7.5, 7.5] . For more information about compose, read the docs. . . A more useful __repr__ . In python, __repr__ helps you get information about an object for logging and debugging. Below is what you get by default when you define a new class. (Note: we are using store_attr, which was discussed earlier). . class Test: def __init__(self, a, b=2, c=3): store_attr() # `store_attr` was discussed previously Test(1) . &lt;__main__.Test at 0x7fe0ab662790&gt; . We can use basic_repr to quickly give us a more sensible default: . class Test: def __init__(self, a, b=2, c=3): store_attr() __repr__ = basic_repr(&#39;a,b,c&#39;) Test(2) . Test(a=2, b=2, c=3) . . Monkey Patching With A Decorator . It can be convenient to monkey patch with a decorator, which is especially helpful when you want to patch an external library you are importing. We can use the decorator @patch from fastcore.foundation along with type hints like so: . class MyClass(int): pass @patch def func(self:MyClass, a): return self+a mc = MyClass(3) . Now, MyClass has an additional method named func: . mc.func(10) . 13 . Still not convinced? I&#39;ll show you another example of this kind of patching in the next section. . . A better pathlib.Path . When you see these extensions to pathlib.path you won&#39;t ever use vanilla pathlib again! A number of additional methods have been added to pathlib, such as: . Path.readlines: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.readlines() | Path.read: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.read() | Path.save: saves file as pickle | Path.load: loads pickle file | Path.ls: shows the contents of the path as a list. | etc. | . Read more about this here. Here is a demonstration of ls: . from pathlib import Path p = Path(&#39;../_notebooks&#39;) p.ls() # you don&#39;t get this with vanilla Pathlib.Path!! . (#21) [Path(&#39;../_notebooks/gpt2_simple_mask.jpg&#39;),Path(&#39;../_notebooks/bert_mac_small.jpg&#39;),Path(&#39;../_notebooks/causal_with_prefix.jpg&#39;),Path(&#39;../_notebooks/.DS_Store&#39;),Path(&#39;../_notebooks/2020-03-07-How_to_Create_an_Automatic_Code_Comment_Generator_using_Deep_Learning.ipynb&#39;),Path(&#39;../_notebooks/2020-09-01-fastcore.ipynb&#39;),Path(&#39;../_notebooks/2020-03-07-Syntax-Highlighting.ipynb&#39;),Path(&#39;../_notebooks/2020-03-06-bart.ipynb&#39;),Path(&#39;../_notebooks/README.md&#39;),Path(&#39;../_notebooks/2020-05-01-TrainDonkeyCar.ipynb&#39;)...] . Wait! What&#39;s going on here? We just imported pathlib.Path - why are we getting this new functionality? Thats because we imported the fastcore.foundation module, which patches this module via the @patch decorator discussed earlier. Just to drive the point home on why the @patch decorator is useful, I&#39;ll go ahead and add another method to Path right now: . @patch def fun(self:Path): return &quot;This is fun!&quot; p.fun() . &#39;This is fun!&#39; . That is magical, right? I know! That&#39;s why I&#39;m writing about it! . . An Even More Concise Way To Create Lambdas . Self, with an uppercase S, is an even more concise way to create lambdas that are calling methods on an object. For example, let&#39;s create a lambda for taking the sum of a Numpy array: . arr=np.array([5,4,3,2,1]) f = lambda a: a.sum() assert f(arr) == 15 . You can use Self in the same way: . f = Self.sum() assert f(arr) == 15 . Let&#39;s create a lambda that does a groupby and max of a Pandas dataframe: . import pandas as pd df=pd.DataFrame({&#39;Some Column&#39;: [&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, ], &#39;Another Column&#39;: [5, 7, 50, 70]}) f = Self.groupby(&#39;Some Column&#39;).mean() f(df) . Another Column . Some Column . a 6 | . b 60 | . Read more about Self in the docs). . . Notebook Functions . These are simple but handy, and allow you to know whether or not code is executing in a Jupyter Notebook, Colab, or an Ipython Shell: . in_notebook(), in_colab(), in_ipython() . (True, False, True) . This is useful if you are displaying certain types of visualizations, progress bars or animations in your code that you may want to modify or toggle depending on the environment. . . A Drop-In Replacement For List . You might be pretty happy with Python&#39;s list. This is one of those situations that you don&#39;t know you needed a better list until someone showed one to you. Enter L, a list like object with many extra goodies. . The best way I can describe L is to pretend that list and numpy had a pretty baby: . define a list (check out the nice __repr__ that shows the length of the list!) . L(1,2,3) . (#3) [1,2,3] . Shuffle a list: . p = L.range(20).shuffle() p . (#20) [2,0,18,6,15,17,14,8,12,1...] . Index into a list: . p[2,4,6] . (#3) [18,15,14] . L has sensible defaults, for example appending an element to a list: . 1 + L(2,3,4) . (#4) [1,2,3,4] . There is much more L has to offer. Read the docs to learn more. . But Wait ... There&#39;s More! . There are more things I would like to show you about fastcore, but there is no way they would reasonably fit into a blog post. Here is a list of some of my favorite things that I didn&#39;t demo in this blog post: . Utilities . The Utilites section contain many shortcuts to perform common tasks or provide an additional interface to what standard python provides. . mk_class: quickly add a bunch of attributes to a class | wrap_class: add new methods to a class with a simple decorator | groupby: similar to Scala&#39;s groupby | merge: merge dicts | fasttuple: a tuple on steroids | Infinite Lists: useful for padding and testing | chunked: for batching and organizing stuff | . Multiprocessing . The Multiprocessing section extends python&#39;s multiprocessing library by offering features like: . progress bars | ability to pause to mitigate race conditions with external services | processing things in batches on each worker, ex: if you have a vectorized operation to perform in chunks | . Functional Programming . The functional programming section is my favorite part of this library. . maps: a map that also composes functions | mapped: A more robust map | using_attr: compose a function that operates on an attribute | . Transforms . Transforms is a collection of utilities for creating data transformations and associated pipelines. These transformation utilities build upon many of the building blocks discussed in this blog post. . Further Reading . It should be noted that you should read the main page of the docs first, followed by the section on tests to fully understand the documentation. . The fastcore documentation site. | The fastcore GitHub repo. | Blog post on delegation. | . Shameless plug: fastpages . This blog post was written entirely in a Jupyter Notebook, which GitHub automatically converted into to a blog post! Sound interesting? Check out fastpages. .",
            "url": "https://inspiringpeople.github.io/tech_blog/fastcore/",
            "relUrl": "/fastcore/",
            "date": " â€¢ Jan 1, 2019"
        }
        
    
  
    
        ,"post3": {
            "title": "Ground Rules",
            "content": "facebook fast.ai ê·¸ë£¹ ëª‡ëª‡ ë¶„ë“¤ê³¼ í•¨ê»˜ fastbook ìŠ¤í„°ë””ë¥¼ í•˜ê²Œ ë˜ì—ˆë‹¤. fastbookì€ fast.aiì™€ pytorchë¥¼ ì´ìš©í•œ ë”¥ëŸ¬ë‹ ë‚´ìš©ê³¼ ì˜ˆì œê°€ ipynb í˜•ì‹ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. paperë„ êµ¬ë§¤ ê°€ëŠ¥í•˜ì§€ë§Œ draft fastbookë„ ê³µê°œë˜ì–´ ìˆë‹¤. . fast.aië¡œ transformer í”„ë¡œì íŠ¸ë¥¼ í•´ ë³¸ ê²°ê³¼ ë¹ ë¥´ê²Œ í”„ë¡œí† íƒ€ì´í•‘ í•´ë³¼ ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ê³¼ ì—°êµ¬ìë“¤ì´ í•­ìƒ ëª©ë§ë¼í•˜ë˜ ì—¬ëŸ¬ê°€ì§€ ì‹¤í—˜ì— ëŒ€í•´ ë‹¤ì–‘í•œ ì§€ì›ì„ í•´ì£¼ëŠ” ê²ƒì„ ë³´ê³  ê¹Šê²Œ ë“¤ì—¬ë‹¤ë³´ê³  ì‹¶ì€ ë§ˆìŒì´ ìˆì—ˆëŠ”ë°, ì´ë ‡ê²Œ ì¢‹ì€ ê³„ê¸°ê°€ ìƒê²¨ ìŠ¤í„°ë”” í›„ ê·¸ ê¸°ë¡ì„ ë¸”ë¡œê·¸ì— ë‚¨ê¸°ê¸°ë¡œ í•œë‹¤. . ì§€ê¸ˆ ê¸€ì„ ì“°ê³  ìˆëŠ” ì´ ë¸”ë¡œê·¸ í˜ì´ì§€ë„ fast.aiì˜ fastpagesë¼ëŠ” ê¸°ìˆ ë¡œ ë§Œë“  ê²ƒì¸ë°, ëª‡ ë¶„ì´ë©´ ëšë”± ë¸”ë¡œê·¸ë¥¼ ë§Œë“¤ ìˆ˜ ìˆê³ , technical writingê¹Œì§€ í¸í•˜ê²Œ ipynb, word, markdown ë“± ì—¬ëŸ¬ íƒ€ì…ì˜ ë¬¸ì„œë¥¼ í˜¸í™˜í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ìì„¸í•œ ì„¤ëª…ì€ ë°•ì°¬ì„±ë‹˜ì˜ ì´ ë§í¬ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë‹¤. . fastbook studyì— ëŒ€í•œ ë‚´ìš©ì„ ì•ìœ¼ë¡œ ì—…ë°ì´íŠ¸ í•  ì˜ˆì •ì¸ë°, ê¾¸ì¤€í•œ ê¸€ì“°ê¸°ë¥¼ ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ Grould rulesë¥¼ ë‘ì—ˆë‹¤. . ê° ë‚´ìš©ì—ì„œ ì¤‘ìš” ë¶€ë¶„ì€ bold í‘œì‹œ! | ëª¨ë“  ë‚´ìš©ì— ëŒ€í•œ ì„¤ëª…ì„ ì“°ê¸°ë³´ë‹¤ëŠ” (ì´ë¯¸ ìì„¸í•œ ì„¤ëª…ê³¼ ì˜ˆì œëŠ” fastbookì´ ê³µê°œë˜ì–´ ìˆìœ¼ë¯€ë¡œ..) ê° ì±•í„°ë¥¼ í†µí•´ ë°°ìš´ ë‚´ìš©, takeaway ë¶€ë¶„ì„ ìš”ì•½ì ìœ¼ë¡œ ì •ë¦¬í•œë‹¤. | ê° ë‚´ìš©ì— ëŒ€í•œ reference ë¶€ë¶„, try&amp;error ë“±ì„ ì¶”ê°€ì ìœ¼ë¡œ ê¸°ì…í•œë‹¤. | ê·¸ëŸ¼ ê¸°ë¡ ëª¨ë“œ ë‹¤ì‹œ on! Done is Better than Perfect! .",
            "url": "https://inspiringpeople.github.io/tech_blog/note/2018/08/30/rules.html",
            "relUrl": "/note/2018/08/30/rules.html",
            "date": " â€¢ Aug 30, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "ë‹¬ë¦¬ê¸°ë¥¼ ë§í•  ë•Œ ë‚´ê°€ í•˜ê³  ì‹¶ì€ ì´ì•¼ê¸° (ë¬´ë¼ì¹´ë¯¸í•˜ë£¨í‚¤) ì¤‘ì—ì„œ . [ë°œì·Œ] . ê°•ë¬¼ì„ ìƒê°í•˜ë ¤ í•œë‹¤. êµ¬ë¦„ì„ ìƒê°í•˜ë ¤ í•œë‹¤. ë‚˜ëŠ” ì†Œë°•í•˜ê³  ì•„ë‹´í•œ ê³µë°± ì†ì„, ì •ê²¨ìš´ ì¹¨ë¬µ ì†ì„ ê·¸ì € ê³„ì† ë‹¬ë ¤ê°€ê³  ìˆë‹¤. ê·¸ ëˆ„ê°€ ë­ë¼ê³  í•´ë„, ê·¸ê²ƒì€ ì—¬ê°„ ë©‹ì§„ ì¼ì´ ì•„ë‹ˆë‹¤. . ê³„ì† í•˜ëŠ” ê²ƒ-ë¦¬ë“¬ì„ ë‹¨ì ˆ í•˜ì§€ ì•ŠëŠ” ê²ƒ, ì¥ê¸°ì ì¸ ì‘ì—…ì„ í•˜ëŠ” ë°ì—ëŠ” ê·¸ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. ì¼ë‹¨ ë¦¬ë“¬ì´ ì„¤ì •ë˜ì–´ì§€ê¸°ë§Œ í•˜ë©´, ê·¸ ë’¤ëŠ” ì–´ë–»ê²Œë“  í’€ë ¤ ë‚˜ê°„ë‹¤. ê·¸ëŸ¬ë‚˜ íƒ„ë ¥ì„ ë°›ì€ ë°”í€´ê°€ ì¼ì •í•œ ì†ë„ë¡œ í™•ì‹¤í•˜ê²Œ ëŒì•„ê°€ê¸° ì‹œì‘í•  ë•Œê¹Œì§€ëŠ” ê³„ì† ê°€ì†í•˜ëŠ” í˜ì„ ë©ˆì¶”ì§€ ë§ì•„ì•¼ í•œë‹¤ëŠ” ê²ƒì€ ì•„ë¬´ë¦¬ ì£¼ì˜ë¥¼ ê¸°ìš¸ì¸ë‹¤ê³  í•´ë„ ì§€ë‚˜ì¹˜ì§€ ì•Šë‹¤. . ë‹¬ë¦¬ëŠ” ê²ƒì€, ë‚´ê°€ ì´ì œê¹Œì§€ì˜ ì¸ìƒì„ ì‚¬ëŠ” ê°€ìš´ë° í›„ì²œì ìœ¼ë¡œ ìµí˜”ë˜ ëª‡ ê°€ì§€ ìŠµê´€ ì¤‘ì—ì„œ ì•„ë§ˆë„ ê°€ì¥ ìœ ìµí•˜ê³  ì¤‘ìš”í•œ ì˜ë¯¸ë¥¼ ì§€ë‹Œ ê²ƒì´ë¼ê³  ìƒê°ëœë‹¤. â€¦ ë¬¼ë¡  ë‚˜ë¼ê³  í•´ì„œ ì§€ëŠ” ê±¸ ì¢‹ì•„í•  ë¦¬ëŠ” ì—†ë‹¤. ê·¸ëŸ¬ë‚˜ ì™ ì§€ ëª¨ë¥´ê²Œ ë‹¤ë¥¸ ì‚¬ëŒì„ ìƒëŒ€ë¡œ ì´ê¸°ê±°ë‚˜ ì§€ê±°ë‚˜ í•˜ëŠ” ê²½ê¸°ì— ëŒ€í•´ì„œëŠ” ì˜›ë‚ ë¶€í„° í•œê²°ê°™ì´ ê·¸ë‹¤ì§€ ì—°ì—°í•˜ì§€ ì•Šì•˜ë‹¤. ê·¸ëŸ¬í•œ ì„±í–¥ì€ ì–´ë¥¸ì´ ëœ ë’¤ì—ë„ ëŒ€ì²´ë¡œ ë³€í•˜ì§€ ì•ŠëŠ”ë‹¤. ì–´ë–¤ ì¼ì´ ëë“  ë‹¤ë¥¸ ì‚¬ëŒì„ ìƒëŒ€ë¡œ ì´ê¸°ë“  ì§€ë“  ê·¸ë‹¤ì§€ ì‹ ê²½ ì“°ì§€ ì•ŠëŠ”ë‹¤. ê·¸ë³´ë‹¤ëŠ” ë‚˜ ìì‹ ì´ ì„¤ì •í•œ ê¸°ì¤€ì„ ë§Œì¡±ì‹œí‚¬ ìˆ˜ ìˆëŠ”ê°€ ì—†ëŠ”ê°€ì— ë” ê´€ì‹¬ì´ ì ë¦°ë‹¤. ê·¸ëŸ° ì˜ë¯¸ì—ì„œ ì¥ê±°ë¦¬ë¥¼ ë‹¬ë¦¬ëŠ” ê²ƒì€ ë‚˜ì˜ ì„±ê²©ì— ì•„ì£¼ ì˜ ë§ëŠ” ìŠ¤í¬ì¸ ì˜€ë‹¤. â€¦ë‹¤ì‹œ ë§í•˜ë©´ ëê¹Œì§€ ë‹¬ë¦¬ê³  ë‚˜ì„œ ìì‹ ì— ëŒ€í•œ ìë¶€ì‹¬(í˜¹ì€ í”„ë¼ì´ë“œì™€ ë¹„ìŠ·í•œ ê²ƒ)ì„ ê°€ì§ˆ ìˆ˜ ìˆëŠ”ê°€ ì—†ëŠ”ê°€, ê·¸ê²ƒì´ ì¥ê±°ë¦¬ ëŸ¬ë„ˆì—ê²Œ ìˆì–´ì„œì˜ ì¤‘ìš”í•œ ê¸°ì¤€ì´ ëœë‹¤â€¦. ê·¸ëŸ° ì˜ë¯¸ì—ì„œ ì†Œì„¤ì„ ì“°ëŠ” ê²ƒì€ ë§ˆë¼í†¤ í’€ì½”ìŠ¤ë¥¼ ë›°ëŠ” ê²ƒê³¼ ë¹„ìŠ·í•˜ë‹¤. ê¸°ë³¸ì ì¸ ì›ì¹™ì„ ë§í•œë‹¤ë©´, ì°½ì‘ìì—ê²Œ ìˆì–´ ê·¸ ë™ê¸°ëŠ” ìì‹  ì•ˆì— ì¡°ìš©íˆ í™•ì‹¤í•˜ê²Œ ì¡´ì¬í•˜ëŠ” ê²ƒìœ¼ë¡œì„œ, ì™¸ë¶€ì—ì„œ ì–´ë–¤ í˜•íƒœë‚˜ ê¸°ì¤€ì„ ì°¾ì•„ì•¼ í•  ì¼ì€ ì•„ë‹ˆë‹¤. . ë‹¬ë¦°ë‹¤ëŠ” ê²ƒì€ ë‚˜ì—ê²Œ ìˆì–´ ìœ ìµí•œ ìš´ë™ì¸ ë™ì‹œì— ìœ íš¨í•œ ë©”íƒ€í¬ì´ê¸°ë„ í•˜ë‹¤. â€¦ ì–´ì œì˜ ìì‹ ì´ ì§€ë‹Œ ì•½ì ì„ ì¡°ê¸ˆì´ë¼ë„ ê·¹ë³µí•´ë‚˜ê°€ëŠ” ê²ƒ, ê·¸ê²ƒì´ ë” ì¤‘ìš”í•œ ê²ƒì´ë‹¤. ì¥ê±°ë¦¬ ë‹¬ë¦¬ê¸°ì— ìˆì–´ì„œ ì´ê²¨ë‚´ì•¼ í•  ìƒëŒ€ê°€ ìˆë‹¤ë©´, ê·¸ê²ƒì€ ë°”ë¡œ ê³¼ê±°ì˜ ìê¸° ìì‹ ì´ê¸° ë•Œë¬¸ì´ë‹¤. . ë‚˜ì´ë¥¼ ë¨¹ì–´ê°„ë‹¤ëŠ” ê²ƒì€ íƒœì–´ë‚˜ì„œ ì²˜ìŒìœ¼ë¡œ ì²´í—˜í•˜ëŠ” ê²ƒì´ê³ , ê±°ê¸°ì—ì„œ ëŠë¼ëŠ” ê°ì • ì—­ì‹œ ì²˜ìŒìœ¼ë¡œ ë§›ë³´ëŠ” ê°ì •ì¸ ê²ƒì´ë‹¤. â€¦ê·¸ëŸ¬ë‚˜ ë‚˜ì´ë¥¼ ë¨¹ì–´ê°ì— ë”°ë¼, ê·¸ì™€ ê°™ì€ ê´´ë¡œì›€ì´ë‚˜ ìƒì²˜ëŠ” ì¸ìƒì— ìˆì–´ ì–´ëŠ ì •ë„ í•„ìš”í•œ ê²ƒì´ë‹¤, ë¼ëŠ” ì ì„ ì¡°ê¸ˆì”© ê¹¨ë‹«ê²Œ ë˜ì—ˆë‹¤. ìƒê°í•´ë³´ë©´ íƒ€ì¸ê³¼ ì–¼ë§ˆê°„ì´ë‚˜ë§ˆ ì°¨ì´ê°€ ìˆëŠ” ê²ƒì´ì•¼ë§ë¡œ, ì‚¬ëŒì˜ ìì•„ë€ ê²ƒì„ í˜•ì„±í•˜ê²Œ ë˜ê³ , ìë¦½í•œ ì¸ê°„ìœ¼ë¡œì„œì˜ ëª¨ìŠµì„ ìœ ì§€í•´ê°ˆ ìˆ˜ ìˆë„ë¡ ë§Œë“œëŠ” ê²ƒì´ë‹¤. â€¦ ë‚´ê°€ ë‹¤ë¥¸ ëˆ„êµ¬ë„ ì•„ë‹Œ â€˜ë‚˜â€™ë¼ëŠ” ê²ƒì€, ë‚˜ì—ê²Œ ìˆì–´ í•˜ë‚˜ì˜ ì†Œì¤‘í•œ ìì‚°ì¸ ê²ƒì´ë‹¤. ë§ˆìŒì´ ë°›ê²Œ ë˜ëŠ” ì•„í”ˆ ìƒì²˜ëŠ” ê·¸ì™€ ê°™ì€ ì¸ê°„ì˜ ìë¦½ì„±ì´ ì„¸ê³„ì— ëŒ€í•´ ì§€ë¶ˆí•˜ì§€ ì•Šìœ¼ë©´ ì•ˆ ë  ë‹¹ì—°í•œ ëŒ€ê°€ì¸ê²ƒì´ë‹¤. . íƒ€ì¸ìœ¼ë¡œë¶€í„°ì˜ ê³ ë¦½ê³¼ ë‹¨ì ˆì€ ë³‘ì—ì„œ ìƒˆì–´ ë‚˜ì˜¨ ì‹ ì²˜ëŸ¼ ì•Œì§€ ëª»í•˜ëŠ” ì‚¬ì´ì— ì‚¬ëŒì˜ ë§ˆìŒì„ ê°‰ì•„ë¨¹ê³  ë…¹ì—¬ë²„ë¦°ë‹¤. ê·¸ê²ƒì€ ì–‘ë‚ ì˜ ê²€ê³¼ ê°™ì€ ê²ƒì´ë‹¤. ì‚¬ëŒì˜ ë§ˆìŒì„ ë³´í˜¸í•˜ëŠ” ë™ì‹œì—, ê·¸ ë‚´ë²½ì„ ëŠì„ì—†ì´ ìì˜í•˜ê²Œ ìƒì²˜ ë‚´ê¸°ë„ í•œë‹¤. â€¦ ë‚˜ëŠ” ì‹ ì²´ë¥¼ ëŠì„ì—†ì´ ë¬¼ë¦¬ì ìœ¼ë¡œ ì›€ì§ì—¬ ë‚˜ê°ìœ¼ë¡œì¨, ì–´ë–¤ ê²½ìš°ì—ëŠ” ê·¹í•œìœ¼ë¡œê¹Œì§€ ëª°ì•„ê°ìœ¼ë¡œì¨, ë‚´ë©´ì— ì•ˆê³  ìˆëŠ” ê³ ë¦½ê³¼ ë‹¨ì ˆì˜ ëŠë‚Œì„ ì¹˜ìœ í•˜ê³  ê°ê´€í™”í•´ ë‚˜ê°€ì•¼ í–ˆë˜ ê²ƒì´ë‹¤. ì˜ë„ì ì´ë¼ê¸°ë³´ë‹¤ëŠ” ì˜¤íˆë ¤ ì§ê°ì ìœ¼ë¡œ. .",
          "url": "https://inspiringpeople.github.io/tech_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://inspiringpeople.github.io/tech_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}