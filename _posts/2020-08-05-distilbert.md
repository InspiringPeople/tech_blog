# distilbert

a distilled version of BERT: smaller, faster, cheaper and lighter

Hugging face, 2019.10

ë…¼ë¬¸ : [https://arxiv.org/abs/1910.01108](https://arxiv.org/abs/1910.01108)

github : [https://github.com/huggingface/transformers/tree/master/examples/distillation](https://github.com/huggingface/transformers/tree/master/examples/distillation)

í•´ë‹¹ githubì—ì„œ ì œì•ˆ ë°©ë²•ìœ¼ë¡œ ìƒì„±í•œ ë‹¤ë¥¸ distil* ëª¨ë¸ ì†Œê°œ (DistilGPT2, DistilRoBERTa, DistilmBERT)

abstract

- ìµœê·¼ pretrain -> fine tuningìœ¼ë¡œ ê°€ëŠ” ë°©ë²•ì´ ë§ì•„ì§€ê³  í”í•´ì¡Œì§€ë§Œ, ëª¨ë¸ ìì²´ê°€ ë„ˆë¬´ í¬ê¸° ë•Œë¬¸ì— ì œí•œëœ í™˜ê²½ì—ì„œëŠ” êµ‰ì¥íˆ ì‚¬ìš©í•˜ê¸° í˜ë“¬
- huggingfaceì—ì„œ DistilBertë¼ëŠ” general purpose language representation model ì œì•ˆ
- BERTë¥¼ 40% ì •ë„ ì¤„ì´ê³  60%ë‚˜ ë¹ ë¥´ê²Œ ì—°ì‚°í•˜ë©´ì„œ 97%ì˜ ì„±ëŠ¥ì„ ìœ ì§€

distilation (knowledge extraction)

Knowledge Distillation [Bucila et al., 2006, Hinton et al., 2015]ì€ larger model(teacher model)ë¡œë¶€í„° compact model(student model)ì„ ë§Œë“¤ì–´ë‚´ëŠ” ë°©ë²•ì´ë‹¤. ì´ê²Œ ì‘ì€ ëª¨ë¸ì„ ë°”ë¡œ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒë³´ë‹¤ ì˜ë¯¸ìˆëŠ” ì´ìœ ëŠ” near-zeroì¸ í™•ë¥ ë“¤ë„ í•™ìŠµí•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤.

ëª¨ë¸ì´ ë§Œë“¤ì–´ë‚¸ ê²°ê³¼ ë¶„í¬ê°€ ì‹¤ì œ ì´ë¯¸ì§€ë¥¼ ë” í’ë¶€í•˜ê²Œ í‘œí˜„í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì¦‰, ì›ë˜ ì •ë‹µ ê´€ì ì—ì„œëŠ” ì •ë‹µ ì´ì™¸ì— ëŒ€í•œ ì •ë³´ê°€ ì—†ì§€ë§Œ, í•œë²ˆ ëª¨ë¸ì—ì„œ í’€ì–´ë‚˜ì˜¨ ê²°ê³¼ëŠ” ì •ë‹µ ì™¸ì—ë„ ë‹¤ë¥¸ ë¬¼ì²´ì— ëŒ€í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆê²Œ ë©ë‹ˆë‹¤. ì´ë ‡ê²Œ ì •ë³´ê°€ ë¬»ì–´ ë‚˜ì˜¤ëŠ” ê²ƒì´ ë§ˆì¹˜ ì„ìœ ì˜ ë¶€ì‚°ë¬¼ë“¤ì´ ì¦ë¥˜íƒ‘ì—ì„œ ë‚˜ì˜¤ëŠ” ì–‘ìƒê³¼ ê°™ê¸° ë•Œë¬¸ì— ì´ë¥¼ ì§€ì‹ ì¦ë¥˜ë¼ê³  ì¼ì»«ìŠµë‹ˆë‹¤. ì›ë˜ ëª¨ë¸ì´ ìƒê°í•˜ëŠ” ë°ì´í„°ì˜ ì •ë³´ê°€ í’€ì–´ë‚˜ì˜¨ ë°ì´í„°ë¡œ ìƒˆë¡œ í•™ìŠµì‹œí‚¤ê²Œ ë˜ë©´ ê°„ì ‘ì ìœ¼ë¡œ ì„ ìƒ ëª¨ë¸ì´ í•™ìŠµí•œ ë°”ë¥¼ ë°˜ì˜í•˜ê²Œ ë˜ë¯€ë¡œ ë” íš¨ìœ¨ì ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆìœ¼ë©°, ìƒˆë¡œ ë°°ìš°ëŠ” ëª¨ë¸(í•™ìƒ ëª¨ë¸)ì€ ìƒëŒ€ì ìœ¼ë¡œ ë” ì ì€ ê·œëª¨ë¡œ êµ¬ì„±ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì´ Hinton et al. ì´ ì œì‹œí•œ ì§€ì‹ ì¦ë¥˜ì˜ í•µì‹¬ì…ë‹ˆë‹¤.

![distilbert%204df0326400104a55af9ef2076f01cb42/Untitled.png](distilbert%204df0326400104a55af9ef2076f01cb42/Untitled.png)

distilbert

- architecture
    - token-type embedding and the pooler are removed
    - layers is reduced by a factor of 2
        - variations on the last dimension of the tensor(hidden size dimension) have a smaller impact on computation efficiency)
- initialization
    - important to find the right initialization for the sub-network to converge
    - initialize the student from teacher by taking one layer out of two
- distilation
    - large batches (up to 4K)
    - dynamic masking without NSP
- data and computer power
    - same data with BERT
    - on 8 16GB V100 GPUs for approximately 90 hours
- downstream benchmark
    - 97% of performance with 40% fewer parameter comparing with BERT

![distilbert%204df0326400104a55af9ef2076f01cb42/Untitled%201.png](distilbert%204df0326400104a55af9ef2076f01cb42/Untitled%201.png)

- distilation ë°©ë²• (github)
    - prepare data
        - binarize the data, i.e. tokenize the data and convert each token in an index in our model's vocabulary

        ```python

        ```

    - training

loss function ë¬´ì—‡?

- Loss function

    ![distilbert%204df0326400104a55af9ef2076f01cb42/Untitled%202.png](distilbert%204df0326400104a55af9ef2076f01cb42/Untitled%202.png)

    - Teacher ëª¨ë¸ì—ì„œ ë‚˜ì˜¨ class probabilitiesë¥¼ ë°”ë¡œ Student modelì˜ target (soft target) ì´ìš©, knowledgeë¥¼ íš¨ê³¼ì ìœ¼ë¡œ transferí•˜ëŠ” ë°©ì‹
    - MNISTë¥¼ ì˜ˆë¥¼ ë“¤ì–´ 2ë¥¼ ë§ì¶œ ë•Œ ë‹®ì€ ìˆ«ìì¸ 3ê³¼ 7ë„ ë‚®ì€ í™•ë¥ ì´ì§€ë§Œ ê°’ì„ ë¶€ì—¬, ì´ëŸ¬í•œ ì •ë³´ë“¤ì€ dataì˜ structureì— ëŒ€í•œ ì •ë³´ê°€ ë“¤ì–´ìˆëŠ” ê°’ì´ê¸° ë•Œë¬¸ì— ì¤‘ìš” ì •ë³´
    - êµ‰ì¥íˆ ë‚®ì€ ê°’ìœ¼ë¡œ í‘œí˜„ë˜ê¸° ë•Œë¬¸ì— temperature ê°œë…ì„ ë„ì…
- softmax-temperature

    ![distilbert%204df0326400104a55af9ef2076f01cb42/Untitled%203.png](distilbert%204df0326400104a55af9ef2076f01cb42/Untitled%203.png)

    - TëŠ” Temperatureì´ê³ , T=1ì´ë¼ë©´ ë³´í†µì˜ softmax ì‹
    - Tê°€ ì»¤ì§€ë©´ í›¨ì”¬ softí•œ probability distribution
    - Tê°€ output distributionì˜ smoothnessë¥¼ ê²°ì •. training ë™ì•ˆì—ë§ŒÂ Të¥¼ ì¡°ì •í•˜ê³  inference ì‹œê°„ì—ëŠ” 1ë¡œ ì„¤ì •í•´ì„œ standard softmaxë¡œ ì‚¬ìš©
- final training lossëŠ” distillation lossÂ Lceì™€ BERTì—ì„œ ì‚¬ìš©í•œÂ Lmlmì˜ linear combination
- Softmax(ì†Œí”„íŠ¸ë§¥ìŠ¤)

    softmaxëŠ” ì…ë ¥ë°›ì€ ê°’ì„ ì¶œë ¥ìœ¼ë¡œ 0~1ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ëª¨ë‘ ì •ê·œí™”í•˜ë©° ì¶œë ¥ ê°’ë“¤ì˜ ì´í•©ì€ í•­ìƒ 1ì´ ë˜ëŠ” íŠ¹ì„±ì„ ê°€ì§„ í•¨ìˆ˜

distil*

- distilbertëŠ” bert-base-uncased tokenizerì™€ ê°™ì€ tokenizer
- teacher modelê³¼ ë¹„ìŠ·í•˜ê²Œ ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ì‹ (bert, gpt2, roberta, betlm..)
- DistilBERT uncased:Â `model = DistilBertModel.from_pretrained('distilbert-base-uncased')`
- DistilGPT2:Â `model = GPT2Model.from_pretrained('distilgpt2')`
- DistilRoBERTa:Â `model = RobertaModel.from_pretrained('distilroberta-base')`
- DistilmBERT:Â `model = DistilBertModel.from_pretrained('distilbert-base-multilingual-cased')`

ì°¸ê³ 

[Transformer - Harder, Better, Faster, Stronger](https://blog.pingpong.us/transformer-review/#distilbert-knowledge-distillation)

[ğŸ Smaller, faster, cheaper, lighter: Introducing DilBERT, a distilled version of BERT](https://medium.com/huggingface/distilbert-8cf3380435b5)

[Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)

[ğŸ“ƒ Distilling the Knowledge in a Neural Network ë¦¬ë·°](https://jeongukjae.github.io/posts/distilling-the-knowledge-in-a-neural-network/)