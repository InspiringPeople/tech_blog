<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Title | fastpages</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Title" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="An easy to use blogging platform with support for Jupyter Notebooks." />
<meta property="og:description" content="An easy to use blogging platform with support for Jupyter Notebooks." />
<link rel="canonical" href="https://inspiringpeople.github.io/tech_blog/2020/09/09/lr_test.html" />
<meta property="og:url" content="https://inspiringpeople.github.io/tech_blog/2020/09/09/lr_test.html" />
<meta property="og:site_name" content="fastpages" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-09-09T00:00:00-05:00" />
<script type="application/ld+json">
{"dateModified":"2020-09-09T00:00:00-05:00","datePublished":"2020-09-09T00:00:00-05:00","description":"An easy to use blogging platform with support for Jupyter Notebooks.","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://inspiringpeople.github.io/tech_blog/2020/09/09/lr_test.html"},"url":"https://inspiringpeople.github.io/tech_blog/2020/09/09/lr_test.html","headline":"Title","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/tech_blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://inspiringpeople.github.io/tech_blog/feed.xml" title="fastpages" /><link rel="shortcut icon" type="image/x-icon" href="/tech_blog/images/favicon.ico"><link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css">

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/tech_blog/">fastpages</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/tech_blog/about/">About Me</a><a class="page-link" href="/tech_blog/search/">Search</a><a class="page-link" href="/tech_blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Title</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-09-09T00:00:00-05:00" itemprop="datePublished">
        Sep 9, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      30 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/InspiringPeople/tech_blog/tree/master/_notebooks/2020-09-09-lr_test.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/tech_blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/InspiringPeople/tech_blog/master?filepath=_notebooks%2F2020-09-09-lr_test.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/tech_blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/InspiringPeople/tech_blog/blob/master/_notebooks/2020-09-09-lr_test.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/tech_blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-09-09-lr_test.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># !pwd</span>
<span class="o">%</span><span class="k">cd</span> /home/jovyan/AI_Master/Flex_NLP/
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>/home/jovyan/AI_Master/Flex_NLP
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">config</span> <span class="kn">import</span> <span class="o">*</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span> 
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span> 
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span> 

<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="kn">import</span> <span class="nn">random</span> 

<span class="c1"># fastai</span>
<span class="kn">from</span> <span class="nn">fastai</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">fastai.text</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">fastai.callbacks</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AdamW</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>



<span class="c1"># transformers</span>
<span class="kn">import</span> <span class="nn">fastai</span>
<span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">from</span> <span class="nn">flex_transformer</span> <span class="kn">import</span> <span class="o">*</span>  
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AdamW</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">PreTrainedModel</span><span class="p">,</span> <span class="n">PreTrainedTokenizer</span><span class="p">,</span> <span class="n">PretrainedConfig</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertForSequenceClassification</span><span class="p">,</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertConfig</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DistilBertForSequenceClassification</span><span class="p">,</span> <span class="n">DistilBertTokenizer</span><span class="p">,</span> <span class="n">DistilBertConfig</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">RobertaForSequenceClassification</span><span class="p">,</span> <span class="n">RobertaTokenizer</span><span class="p">,</span> <span class="n">RobertaConfig</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">XLNetForSequenceClassification</span><span class="p">,</span> <span class="n">XLNetTokenizer</span><span class="p">,</span> <span class="n">XLNetConfig</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">XLMForSequenceClassification</span><span class="p">,</span> <span class="n">XLMTokenizer</span><span class="p">,</span> <span class="n">XLMConfig</span>

<span class="kn">from</span> <span class="nn">dataMgr</span> <span class="kn">import</span> <span class="o">*</span>

<span class="sd">&quot;&quot;&quot;# Model Configuration</span>
<span class="sd"># model_class.pretrained_model_archive_map.keys()</span>

<span class="sd">[bert]</span>
<span class="sd">dict_keys([&#39;bert-base-uncased&#39;, &#39;bert-large-uncased&#39;, &#39;bert-base-cased&#39;, </span>
<span class="sd">&#39;bert-large-cased&#39;, &#39;bert-base-multilingual-uncased&#39;, &#39;bert-base-multilingual-cased&#39;, </span>
<span class="sd">&#39;bert-base-chinese&#39;, &#39;bert-base-german-cased&#39;, &#39;bert-large-uncased-whole-word-masking&#39;, </span>
<span class="sd">&#39;bert-large-cased-whole-word-masking&#39;, &#39;bert-large-uncased-whole-word-masking-finetuned-squad&#39;, </span>
<span class="sd">&#39;bert-large-cased-whole-word-masking-finetuned-squad&#39;, &#39;bert-base-cased-finetuned-mrpc&#39;, </span>
<span class="sd">&#39;bert-base-german-dbmdz-cased&#39;, &#39;bert-base-german-dbmdz-uncased&#39;, &#39;bert-base-japanese&#39;, </span>
<span class="sd">&#39;bert-base-japanese-whole-word-masking&#39;, &#39;bert-base-japanese-char&#39;, &#39;bert-base-japanese-char-whole-word-masking&#39;, </span>
<span class="sd">&#39;bert-base-finnish-cased-v1&#39;, &#39;bert-base-finnish-uncased-v1&#39;, &#39;bert-base-dutch-cased&#39;])</span>

<span class="sd">[xlnet]</span>
<span class="sd">dict_keys([&#39;xlnet-base-cased&#39;, &#39;xlnet-large-cased&#39;])</span>

<span class="sd">[xlm]</span>
<span class="sd">dict_keys([&#39;xlm-mlm-en-2048&#39;, &#39;xlm-mlm-ende-1024&#39;, &#39;xlm-mlm-enfr-1024&#39;, &#39;xlm-mlm-enro-1024&#39;, </span>
<span class="sd">&#39;xlm-mlm-tlm-xnli15-1024&#39;, &#39;xlm-mlm-xnli15-1024&#39;, &#39;xlm-clm-enfr-1024&#39;, &#39;xlm-clm-ende-1024&#39;, </span>
<span class="sd">&#39;xlm-mlm-17-1280&#39;, &#39;xlm-mlm-100-1280&#39;])</span>

<span class="sd">[roberta]</span>
<span class="sd">dict_keys([&#39;roberta-base&#39;, &#39;roberta-large&#39;, &#39;roberta-large-mnli&#39;, &#39;distilroberta-base&#39;, </span>
<span class="sd">&#39;roberta-base-openai-detector&#39;, &#39;roberta-large-openai-detector&#39;])</span>

<span class="sd">[distilbert]</span>
<span class="sd">dict_keys([&#39;distilbert-base-uncased&#39;, &#39;distilbert-base-uncased-distilled-squad&#39;, &#39;distilbert-base-cased&#39;, </span>
<span class="sd">&#39;distilbert-base-cased-distilled-squad&#39;, &#39;distilbert-base-german-cased&#39;, &#39;distilbert-base-multilingual-cased&#39;, </span>
<span class="sd">&#39;distilbert-base-uncased-finetuned-sst-2-english&#39;])</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">MODEL_CLASSES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;bert&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">BertForSequenceClassification</span><span class="p">,</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertConfig</span><span class="p">),</span> <span class="c1"># bert-base-uncased</span>
    <span class="s1">&#39;xlnet&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">XLNetForSequenceClassification</span><span class="p">,</span> <span class="n">XLNetTokenizer</span><span class="p">,</span> <span class="n">XLNetConfig</span><span class="p">),</span> <span class="c1"># xlnet-base-cased&#39;</span>
    <span class="s1">&#39;xlm&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">XLMForSequenceClassification</span><span class="p">,</span> <span class="n">XLMTokenizer</span><span class="p">,</span> <span class="n">XLMConfig</span><span class="p">),</span> <span class="c1"># xlm-mlm-en-2048</span>
    <span class="s1">&#39;roberta&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">RobertaForSequenceClassification</span><span class="p">,</span> <span class="n">RobertaTokenizer</span><span class="p">,</span> <span class="n">RobertaConfig</span><span class="p">),</span> <span class="c1"># distilroberta-base</span>
    <span class="s1">&#39;distilbert&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">DistilBertForSequenceClassification</span><span class="p">,</span> <span class="n">DistilBertTokenizer</span><span class="p">,</span> <span class="n">DistilBertConfig</span><span class="p">)</span> <span class="c1"># distilbert-base-uncased</span>
<span class="p">}</span>



<span class="k">def</span> <span class="nf">load_pretrained_model</span><span class="p">(</span><span class="n">model_type</span><span class="p">,</span> <span class="n">pretrained_model_name</span><span class="p">):</span>
    <span class="n">model_class</span><span class="p">,</span> <span class="n">tokenizer_class</span><span class="p">,</span> <span class="n">config_class</span> <span class="o">=</span> <span class="n">MODEL_CLASSES</span><span class="p">[</span><span class="n">model_type</span><span class="p">]</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">config_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_model_name</span><span class="p">)</span>
    <span class="c1"># config.num_labels = n_label</span>
    <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">=</span> <span class="mi">5</span>

    <span class="n">transformer_model</span> <span class="o">=</span> <span class="n">model_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_model_name</span><span class="p">,</span> <span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="p">)</span>
    <span class="c1"># transformer_model = model_class.from_pretrained(pretrained_model_name, num_labels = 5)</span>
    <span class="n">custom_transformer_model</span> <span class="o">=</span> <span class="n">CustomTransformerModel</span><span class="p">(</span><span class="n">transformer_model</span> <span class="o">=</span> <span class="n">transformer_model</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">custom_transformer_model</span>


<span class="k">def</span> <span class="nf">fine_tuned_model</span><span class="p">(</span><span class="n">custom_transformer_model</span><span class="p">,</span> <span class="n">databunch</span><span class="p">,</span> <span class="n">model_path</span><span class="p">):</span>

    <span class="n">CustomAdamW</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">AdamW</span><span class="p">,</span> <span class="n">correct_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    
    <span class="n">learner</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">databunch</span><span class="p">,</span> 
                <span class="n">custom_transformer_model</span><span class="p">,</span> 
                <span class="n">opt_func</span> <span class="o">=</span> <span class="n">CustomAdamW</span><span class="p">,</span> 
                <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">error_rate</span><span class="p">])</span>

        
    <span class="c1"># For DistilBERT</span>
    <span class="n">list_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">distilbert</span><span class="o">.</span><span class="n">embeddings</span><span class="p">,</span>
                <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">distilbert</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">distilbert</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">distilbert</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">distilbert</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>
                <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">distilbert</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span>
                <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">distilbert</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span>
                <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">pre_classifier</span><span class="p">]</span>

    <span class="n">learner</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">list_layers</span><span class="p">)</span>
    <span class="n">num_groups</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">layer_groups</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Learner split in&#39;</span><span class="p">,</span><span class="n">num_groups</span><span class="p">,</span><span class="s1">&#39;groups&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">layer_groups</span><span class="p">)</span>


    <span class="c1">## Train</span>
    <span class="n">learner</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;untrain&#39;</span><span class="p">)</span>

    <span class="n">seed_all</span><span class="p">(</span><span class="n">seed_val</span><span class="p">)</span>
    <span class="n">learner</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;untrain&#39;</span><span class="p">)</span>

    <span class="n">learner</span><span class="o">.</span><span class="n">freeze_to</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">learner</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

    <span class="n">learner</span><span class="o">.</span><span class="n">lr_find</span><span class="p">()</span>
    <span class="c1"># learner.recorder.plot(skip_end=10,suggestion=True)</span>

    <span class="n">learner</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">max_lr</span><span class="o">=</span><span class="mf">2e-03</span><span class="p">,</span><span class="n">moms</span><span class="o">=</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span><span class="mf">0.7</span><span class="p">))</span>
    <span class="n">learner</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;first_cycle&#39;</span><span class="p">)</span>

    <span class="n">seed_all</span><span class="p">(</span><span class="n">seed_val</span><span class="p">)</span>
    <span class="n">learner</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;first_cycle&#39;</span><span class="p">)</span>

    <span class="n">learner</span><span class="o">.</span><span class="n">freeze_to</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="mf">1e-5</span>

    <span class="n">learner</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_lr</span><span class="o">=</span><span class="nb">slice</span><span class="p">(</span><span class="n">lr</span><span class="o">*</span><span class="mf">0.95</span><span class="o">**</span><span class="n">num_groups</span><span class="p">,</span> <span class="n">lr</span><span class="p">),</span> <span class="n">moms</span><span class="o">=</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">))</span>
    <span class="n">learner</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;second_cycle&#39;</span><span class="p">)</span>

    <span class="n">seed_all</span><span class="p">(</span><span class="n">seed_val</span><span class="p">)</span>
    <span class="n">learner</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;second_cycle&#39;</span><span class="p">)</span>

    <span class="n">learner</span><span class="o">.</span><span class="n">freeze_to</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">learner</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_lr</span><span class="o">=</span><span class="nb">slice</span><span class="p">(</span><span class="n">lr</span><span class="o">*</span><span class="mf">0.95</span><span class="o">**</span><span class="n">num_groups</span><span class="p">,</span> <span class="n">lr</span><span class="p">),</span> <span class="n">moms</span><span class="o">=</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">))</span>

    <span class="n">learner</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;third_cycle&#39;</span><span class="p">)</span>

    <span class="n">learner</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">file</span> <span class="o">=</span> <span class="n">model_path</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">learner</span>




<span class="k">def</span> <span class="nf">get_model_info</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;====================================== Model ======================================&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;분석하고자 하는 모델 정보를 입력해주세요.&quot;</span><span class="p">)</span>
    <span class="c1"># print(&quot;pre_trained 모델을 선택해주세요. \n 1:distilbert 2:bert 3:roberta 4:.... (TBD)&quot;)</span>
    <span class="n">model_i</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="c1"># print(&quot;분석 NLP Task를 선택해주세요. \n 1: multi-label classification 2:ner 3:squad 4:..... (TBD)&quot;)</span>
    <span class="n">task_i</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="c1"># print(&quot;Class Label 개수를 입력해주세요.&quot;)</span>
    <span class="c1"># n_label = int(input())</span>
    <span class="n">n_label</span> <span class="o">=</span> <span class="mi">5</span>

    <span class="n">model_info</span> <span class="o">=</span> <span class="p">[</span><span class="n">model_dict</span><span class="p">[</span><span class="n">model_i</span><span class="p">],</span> <span class="n">task_dict</span><span class="p">[</span><span class="n">task_i</span><span class="p">],</span> <span class="n">n_label</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=================== 입력해주신 Model 정보입니다 ===================&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[분석 모델, NLP Task, Label 개수]&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">model_info</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model_dict</span><span class="p">[</span><span class="n">model_i</span><span class="p">],</span> <span class="n">task_dict</span><span class="p">[</span><span class="n">task_i</span><span class="p">],</span> <span class="n">n_label</span>

<span class="k">def</span> <span class="nf">get_list_layers</span><span class="p">(</span><span class="n">learner</span><span class="p">,</span> <span class="n">model_type</span><span class="p">):</span>
    <span class="k">if</span><span class="p">(</span><span class="n">model_type</span> <span class="o">==</span> <span class="s2">&quot;distilbert&quot;</span><span class="p">):</span>
        <span class="c1"># For DistilBERT</span>
        <span class="n">list_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">distilbert</span><span class="o">.</span><span class="n">embeddings</span><span class="p">,</span>
                    <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">distilbert</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                    <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">distilbert</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                    <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">distilbert</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                    <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">distilbert</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>
                    <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">distilbert</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span>
                    <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">distilbert</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span>
                    <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">pre_classifier</span><span class="p">]</span>
    <span class="k">elif</span><span class="p">(</span><span class="n">model_type</span> <span class="o">==</span> <span class="s2">&quot;bert&quot;</span><span class="p">):</span>
        <span class="c1"># # For BERT</span>
        <span class="n">list_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">bert</span><span class="o">.</span><span class="n">embeddings</span><span class="p">,</span>
                    <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">bert</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                    <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">bert</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                    <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">bert</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                    <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">bert</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>
                    <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">bert</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span>
                    <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">bert</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span>
                    <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">bert</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">6</span><span class="p">],</span>
                    <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">bert</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">7</span><span class="p">],</span>
                    <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">bert</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">8</span><span class="p">],</span>
                    <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">bert</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">9</span><span class="p">],</span>
                    <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">bert</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">10</span><span class="p">],</span>
                    <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">bert</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">11</span><span class="p">],</span>
                    <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">bert</span><span class="o">.</span><span class="n">pooler</span><span class="p">,</span>
                    <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span>
                    <span class="c1"># learner.model.transformer.lin1,</span>
                    <span class="c1"># learner.model.transformer.lin2,</span>
                    <span class="c1"># learner.model.transformer.lin3,</span>
                    <span class="c1"># learner.model.transformer.lin4,</span>
                    <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">classifier</span><span class="p">]</span>
    <span class="c1"># elif(model_type == &quot;xlnet&quot;):</span>
        <span class="c1"># # For XLNet</span>
        <span class="c1"># list_layers = [learner.model.transformer.xlnet.embeddings,</span>
        <span class="c1">#             learner.model.transformer.xlnet.transformer.layer[0],</span>
        <span class="c1">#             learner.model.transformer.xlnet.transformer.layer[1],</span>
        <span class="c1">#             learner.model.transformer.xlnet.transformer.layer[2],</span>
        <span class="c1">#             learner.model.transformer.xlnet.transformer.layer[3],</span>
        <span class="c1">#             learner.model.transformer.xlnet.transformer.layer[4],</span>
        <span class="c1">#             learner.model.transformer.xlnet.transformer.layer[5],</span>
        <span class="c1">#             learner.model.transformer.xlnet.transformer.layer[6],</span>
        <span class="c1">#             learner.model.transformer.xlnet.transformer.layer[7],</span>
        <span class="c1">#             learner.model.transformer.xlnet.transformer.layer[8],</span>
        <span class="c1">#             learner.model.transformer.xlnet.transformer.layer[9],</span>
        <span class="c1">#             learner.model.transformer.xlnet.transformer.layer[10],</span>
        <span class="c1">#             learner.model.transformer.xlnet.transformer.layer[11],</span>
        <span class="c1">#             learner.model.transformer.pre_classifier]</span>
    <span class="k">elif</span><span class="p">(</span><span class="n">model_type</span> <span class="o">==</span> <span class="s2">&quot;roberta&quot;</span><span class="p">):</span>
        <span class="c1"># For Roberta</span>
        <span class="n">list_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">roberta</span><span class="o">.</span><span class="n">embeddings</span><span class="p">,</span>
                <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">roberta</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">roberta</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">roberta</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">roberta</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>
                <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">roberta</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span>
                <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">roberta</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span>
                <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">roberta</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">6</span><span class="p">],</span>
                <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">roberta</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">7</span><span class="p">],</span>
                <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">roberta</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">8</span><span class="p">],</span>
                <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">roberta</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">9</span><span class="p">],</span>
                <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">roberta</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">10</span><span class="p">],</span>
                <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">roberta</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">11</span><span class="p">],</span>
                <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">roberta</span><span class="o">.</span><span class="n">pooler</span><span class="p">]</span>

    <span class="k">elif</span><span class="p">(</span><span class="n">model_type</span> <span class="o">==</span> <span class="s2">&quot;distilroberta&quot;</span><span class="p">):</span> <span class="c1"># distilroberta</span>
            <span class="c1"># For Roberta</span>
        <span class="n">list_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">roberta</span><span class="o">.</span><span class="n">embeddings</span><span class="p">,</span>
                <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">roberta</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">roberta</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">roberta</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">roberta</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>
                <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">roberta</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span>
                <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">roberta</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span>
                <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">roberta</span><span class="o">.</span><span class="n">pooler</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">list_layers</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>

    <span class="n">data_name</span> <span class="o">=</span> <span class="s1">&#39;movie_en_5label&#39;</span>
    <span class="n">model_type</span> <span class="o">=</span> <span class="s1">&#39;roberta&#39;</span>
    <span class="n">pretrained_model_name</span> <span class="o">=</span> <span class="s1">&#39;roberta-base&#39;</span>

    <span class="c1"># data_name = sys.argv[1]</span>
    <span class="c1"># model_type = sys.argv[2]</span>
    <span class="c1"># pretrained_model_name = sys.argv[3]</span>
    <span class="n">finetuned_model_name</span> <span class="o">=</span> <span class="s1">&#39;./models/&#39;</span> <span class="o">+</span> <span class="n">data_name</span> <span class="o">+</span> <span class="s1">&#39;_&#39;</span> <span class="o">+</span> <span class="n">pretrained_model_name</span> <span class="o">+</span> <span class="s1">&#39;.pkl&#39;</span>
        
    <span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">,</span> <span class="n">n_label</span><span class="p">,</span> <span class="n">doc_name</span><span class="p">,</span> <span class="n">label_name</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">(</span><span class="n">data_name</span><span class="p">)</span>
    <span class="n">model_class</span><span class="p">,</span> <span class="n">tokenizer_class</span><span class="p">,</span> <span class="n">config_class</span> <span class="o">=</span> <span class="n">MODEL_CLASSES</span><span class="p">[</span><span class="n">model_type</span><span class="p">]</span>

    <span class="n">transformer_tokenizer</span> <span class="o">=</span> <span class="n">tokenizer_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_model_name</span><span class="p">)</span>
    <span class="n">transformer_base_tokenizer</span> <span class="o">=</span> <span class="n">TransformersBaseTokenizer</span><span class="p">(</span><span class="n">pretrained_tokenizer</span> <span class="o">=</span> <span class="n">transformer_tokenizer</span><span class="p">,</span> <span class="n">model_type</span> <span class="o">=</span> <span class="n">model_type</span><span class="p">)</span>
    <span class="n">fastai_tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">tok_func</span> <span class="o">=</span> <span class="n">transformer_base_tokenizer</span><span class="p">,</span> <span class="n">pre_rules</span><span class="o">=</span><span class="p">[],</span> <span class="n">post_rules</span><span class="o">=</span><span class="p">[])</span>

    <span class="c1"># Custom Processor</span>
    <span class="n">transformer_vocab</span> <span class="o">=</span>  <span class="n">TransformersVocab</span><span class="p">(</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">transformer_tokenizer</span><span class="p">)</span>
    <span class="n">numericalize_processor</span> <span class="o">=</span> <span class="n">NumericalizeProcessor</span><span class="p">(</span><span class="n">vocab</span><span class="o">=</span><span class="n">transformer_vocab</span><span class="p">)</span>
    <span class="n">tokenize_processor</span> <span class="o">=</span> <span class="n">TokenizeProcessor</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">fastai_tokenizer</span><span class="p">,</span> <span class="n">include_bos</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">include_eos</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">transformer_processor</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenize_processor</span><span class="p">,</span> <span class="n">numericalize_processor</span><span class="p">]</span>


    <span class="c1"># Setting up DatBunch</span>
    <span class="n">pad_first</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="n">model_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;xlnet&#39;</span><span class="p">])</span>
    <span class="n">pad_idx</span> <span class="o">=</span> <span class="n">transformer_tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span>

    <span class="n">databunch</span> <span class="o">=</span> <span class="p">(</span><span class="n">TextList</span><span class="o">.</span><span class="n">from_df</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">cols</span><span class="o">=</span><span class="n">doc_name</span><span class="p">,</span> <span class="n">processor</span><span class="o">=</span><span class="n">transformer_processor</span><span class="p">)</span>
                <span class="o">.</span><span class="n">split_by_rand_pct</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span><span class="n">seed</span><span class="o">=</span><span class="n">seed_val</span><span class="p">)</span>
                <span class="o">.</span><span class="n">label_from_df</span><span class="p">(</span><span class="n">cols</span><span class="o">=</span> <span class="n">label_name</span><span class="p">)</span>
                <span class="c1"># .add_test(test)</span>
                <span class="o">.</span><span class="n">databunch</span><span class="p">())</span>

    <span class="n">config</span> <span class="o">=</span> <span class="n">config_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_model_name</span><span class="p">)</span>
    <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">=</span> <span class="n">n_label</span> <span class="c1"># get n_label</span>
    <span class="n">config</span><span class="o">.</span><span class="n">use_bfloat16</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># use_fp16</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">pretrained_model_name</span><span class="p">)</span>

    <span class="c1"># For Bert</span>
    <span class="c1"># transformer_model = BertForSequenceClassification.from_pretrained(pretrained_model_name, config = config)</span>
    <span class="n">transformer_model</span> <span class="o">=</span> <span class="n">model_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_model_name</span><span class="p">,</span> <span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="p">)</span>

    <span class="n">custom_transformer_model</span> <span class="o">=</span> <span class="n">CustomTransformerModel</span><span class="p">(</span><span class="n">transformer_model</span> <span class="o">=</span> <span class="n">transformer_model</span><span class="p">)</span>

    <span class="c1"># learner = fine_tuned_model(custom_transformer_model, train, &#39;./wm_ref_tv.pkl&#39;)</span>

    <span class="n">CustomAdamW</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">AdamW</span><span class="p">,</span> <span class="n">correct_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    
    <span class="n">learner</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">databunch</span><span class="p">,</span> 
                <span class="n">custom_transformer_model</span><span class="p">,</span> 
                <span class="n">opt_func</span> <span class="o">=</span> <span class="n">CustomAdamW</span><span class="p">,</span> 
                <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">error_rate</span><span class="p">])</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
    
    <span class="n">list_layers</span> <span class="o">=</span> <span class="n">get_list_layers</span><span class="p">(</span><span class="n">learner</span><span class="p">,</span> <span class="n">model_type</span><span class="p">)</span>

    <span class="n">learner</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">list_layers</span><span class="p">)</span>
    <span class="n">num_groups</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">layer_groups</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Learner split in&#39;</span><span class="p">,</span><span class="n">num_groups</span><span class="p">,</span><span class="s1">&#39;groups&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">layer_groups</span><span class="p">)</span>


    <span class="c1">## Train</span>
    <span class="n">learner</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;untrain&#39;</span><span class="p">)</span>

    <span class="n">seed_all</span><span class="p">(</span><span class="n">seed_val</span><span class="p">)</span>
    <span class="n">learner</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;untrain&#39;</span><span class="p">)</span>

    <span class="n">learner</span><span class="o">.</span><span class="n">freeze_to</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>(124848, 4)
get_data function took 0.20489811897277832 sec
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>RobertaConfig {
  &#34;_num_labels&#34;: 5,
  &#34;architectures&#34;: [
    &#34;RobertaForMaskedLM&#34;
  ],
  &#34;attention_probs_dropout_prob&#34;: 0.1,
  &#34;bad_words_ids&#34;: null,
  &#34;bos_token_id&#34;: 0,
  &#34;decoder_start_token_id&#34;: null,
  &#34;do_sample&#34;: false,
  &#34;early_stopping&#34;: false,
  &#34;eos_token_id&#34;: 2,
  &#34;finetuning_task&#34;: null,
  &#34;hidden_act&#34;: &#34;gelu&#34;,
  &#34;hidden_dropout_prob&#34;: 0.1,
  &#34;hidden_size&#34;: 768,
  &#34;id2label&#34;: {
    &#34;0&#34;: &#34;LABEL_0&#34;,
    &#34;1&#34;: &#34;LABEL_1&#34;,
    &#34;2&#34;: &#34;LABEL_2&#34;,
    &#34;3&#34;: &#34;LABEL_3&#34;,
    &#34;4&#34;: &#34;LABEL_4&#34;
  },
  &#34;initializer_range&#34;: 0.02,
  &#34;intermediate_size&#34;: 3072,
  &#34;is_decoder&#34;: false,
  &#34;is_encoder_decoder&#34;: false,
  &#34;label2id&#34;: {
    &#34;LABEL_0&#34;: 0,
    &#34;LABEL_1&#34;: 1,
    &#34;LABEL_2&#34;: 2,
    &#34;LABEL_3&#34;: 3,
    &#34;LABEL_4&#34;: 4
  },
  &#34;layer_norm_eps&#34;: 1e-05,
  &#34;length_penalty&#34;: 1.0,
  &#34;max_length&#34;: 20,
  &#34;max_position_embeddings&#34;: 514,
  &#34;min_length&#34;: 0,
  &#34;model_type&#34;: &#34;roberta&#34;,
  &#34;no_repeat_ngram_size&#34;: 0,
  &#34;num_attention_heads&#34;: 12,
  &#34;num_beams&#34;: 1,
  &#34;num_hidden_layers&#34;: 12,
  &#34;num_return_sequences&#34;: 1,
  &#34;output_attentions&#34;: false,
  &#34;output_hidden_states&#34;: false,
  &#34;output_past&#34;: true,
  &#34;pad_token_id&#34;: 1,
  &#34;prefix&#34;: null,
  &#34;pruned_heads&#34;: {},
  &#34;repetition_penalty&#34;: 1.0,
  &#34;task_specific_params&#34;: null,
  &#34;temperature&#34;: 1.0,
  &#34;top_k&#34;: 50,
  &#34;top_p&#34;: 1.0,
  &#34;torchscript&#34;: false,
  &#34;type_vocab_size&#34;: 1,
  &#34;use_bfloat16&#34;: false,
  &#34;vocab_size&#34;: 50265
}

roberta-base
CustomTransformerModel(
  (transformer): RobertaForSequenceClassification(
    (roberta): RobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(50265, 768, padding_idx=1)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (classifier): RobertaClassificationHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): Linear(in_features=768, out_features=5, bias=True)
    )
  )
)
Learner split in 14 groups
[Sequential(
  (0): Embedding(50265, 768, padding_idx=1)
  (1): Embedding(514, 768, padding_idx=1)
  (2): Embedding(1, 768)
  (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (4): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Tanh()
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=5, bias=True)
)]
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learner</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>CustomTransformerModel
======================================================================
Layer (type)         Output Shape         Param #    Trainable 
======================================================================
Embedding            [75, 768]            38,603,520 False     
______________________________________________________________________
Embedding            [75, 768]            394,752    False     
______________________________________________________________________
Embedding            [75, 768]            768        False     
______________________________________________________________________
LayerNorm            [75, 768]            1,536      False     
______________________________________________________________________
Dropout              [75, 768]            0          False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Dropout              [12, 75, 75]         0          False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
LayerNorm            [75, 768]            1,536      False     
______________________________________________________________________
Dropout              [75, 768]            0          False     
______________________________________________________________________
Linear               [75, 3072]           2,362,368  False     
______________________________________________________________________
Linear               [75, 768]            2,360,064  False     
______________________________________________________________________
LayerNorm            [75, 768]            1,536      False     
______________________________________________________________________
Dropout              [75, 768]            0          False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Dropout              [12, 75, 75]         0          False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
LayerNorm            [75, 768]            1,536      False     
______________________________________________________________________
Dropout              [75, 768]            0          False     
______________________________________________________________________
Linear               [75, 3072]           2,362,368  False     
______________________________________________________________________
Linear               [75, 768]            2,360,064  False     
______________________________________________________________________
LayerNorm            [75, 768]            1,536      False     
______________________________________________________________________
Dropout              [75, 768]            0          False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Dropout              [12, 75, 75]         0          False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
LayerNorm            [75, 768]            1,536      False     
______________________________________________________________________
Dropout              [75, 768]            0          False     
______________________________________________________________________
Linear               [75, 3072]           2,362,368  False     
______________________________________________________________________
Linear               [75, 768]            2,360,064  False     
______________________________________________________________________
LayerNorm            [75, 768]            1,536      False     
______________________________________________________________________
Dropout              [75, 768]            0          False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Dropout              [12, 75, 75]         0          False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
LayerNorm            [75, 768]            1,536      False     
______________________________________________________________________
Dropout              [75, 768]            0          False     
______________________________________________________________________
Linear               [75, 3072]           2,362,368  False     
______________________________________________________________________
Linear               [75, 768]            2,360,064  False     
______________________________________________________________________
LayerNorm            [75, 768]            1,536      False     
______________________________________________________________________
Dropout              [75, 768]            0          False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Dropout              [12, 75, 75]         0          False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
LayerNorm            [75, 768]            1,536      False     
______________________________________________________________________
Dropout              [75, 768]            0          False     
______________________________________________________________________
Linear               [75, 3072]           2,362,368  False     
______________________________________________________________________
Linear               [75, 768]            2,360,064  False     
______________________________________________________________________
LayerNorm            [75, 768]            1,536      False     
______________________________________________________________________
Dropout              [75, 768]            0          False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Dropout              [12, 75, 75]         0          False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
LayerNorm            [75, 768]            1,536      False     
______________________________________________________________________
Dropout              [75, 768]            0          False     
______________________________________________________________________
Linear               [75, 3072]           2,362,368  False     
______________________________________________________________________
Linear               [75, 768]            2,360,064  False     
______________________________________________________________________
LayerNorm            [75, 768]            1,536      False     
______________________________________________________________________
Dropout              [75, 768]            0          False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Dropout              [12, 75, 75]         0          False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
LayerNorm            [75, 768]            1,536      False     
______________________________________________________________________
Dropout              [75, 768]            0          False     
______________________________________________________________________
Linear               [75, 3072]           2,362,368  False     
______________________________________________________________________
Linear               [75, 768]            2,360,064  False     
______________________________________________________________________
LayerNorm            [75, 768]            1,536      False     
______________________________________________________________________
Dropout              [75, 768]            0          False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Dropout              [12, 75, 75]         0          False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
LayerNorm            [75, 768]            1,536      False     
______________________________________________________________________
Dropout              [75, 768]            0          False     
______________________________________________________________________
Linear               [75, 3072]           2,362,368  False     
______________________________________________________________________
Linear               [75, 768]            2,360,064  False     
______________________________________________________________________
LayerNorm            [75, 768]            1,536      False     
______________________________________________________________________
Dropout              [75, 768]            0          False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Dropout              [12, 75, 75]         0          False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
LayerNorm            [75, 768]            1,536      False     
______________________________________________________________________
Dropout              [75, 768]            0          False     
______________________________________________________________________
Linear               [75, 3072]           2,362,368  False     
______________________________________________________________________
Linear               [75, 768]            2,360,064  False     
______________________________________________________________________
LayerNorm            [75, 768]            1,536      False     
______________________________________________________________________
Dropout              [75, 768]            0          False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Dropout              [12, 75, 75]         0          False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
LayerNorm            [75, 768]            1,536      False     
______________________________________________________________________
Dropout              [75, 768]            0          False     
______________________________________________________________________
Linear               [75, 3072]           2,362,368  False     
______________________________________________________________________
Linear               [75, 768]            2,360,064  False     
______________________________________________________________________
LayerNorm            [75, 768]            1,536      False     
______________________________________________________________________
Dropout              [75, 768]            0          False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Dropout              [12, 75, 75]         0          False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
LayerNorm            [75, 768]            1,536      False     
______________________________________________________________________
Dropout              [75, 768]            0          False     
______________________________________________________________________
Linear               [75, 3072]           2,362,368  False     
______________________________________________________________________
Linear               [75, 768]            2,360,064  False     
______________________________________________________________________
LayerNorm            [75, 768]            1,536      False     
______________________________________________________________________
Dropout              [75, 768]            0          False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
Dropout              [12, 75, 75]         0          False     
______________________________________________________________________
Linear               [75, 768]            590,592    False     
______________________________________________________________________
LayerNorm            [75, 768]            1,536      False     
______________________________________________________________________
Dropout              [75, 768]            0          False     
______________________________________________________________________
Linear               [75, 3072]           2,362,368  False     
______________________________________________________________________
Linear               [75, 768]            2,360,064  False     
______________________________________________________________________
LayerNorm            [75, 768]            1,536      False     
______________________________________________________________________
Dropout              [75, 768]            0          False     
______________________________________________________________________
Linear               [768]                590,592    True      
______________________________________________________________________
Tanh                 [768]                0          False     
______________________________________________________________________
Linear               [768]                590,592    True      
______________________________________________________________________
Dropout              [768]                0          False     
______________________________________________________________________
Linear               [5]                  3,845      True      
______________________________________________________________________

Total params: 125,240,069
Total trainable params: 1,185,029
Total non-trainable params: 124,055,040
Optimized with &#39;transformers.optimization.AdamW&#39;, correct_bias=False
Using true weight decay as discussed in https://www.fast.ai/2018/07/02/adam-weight-decay/ 
Loss function : FlattenedLoss
======================================================================
Callbacks functions applied </pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learner</span><span class="o">.</span><span class="n">lr_find</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
                background: #F44336;
            }
        </style>
      &lt;progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'&gt;&lt;/progress&gt;
      0.00% [0/1 00:00&lt;00:00]
    </div>
    
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
  </tbody>
</table><p>

    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
                background: #F44336;
            }
        </style>
      &lt;progress value='71' class='' max='1755', style='width:300px; height:20px; vertical-align: middle;'&gt;&lt;/progress&gt;
      4.05% [71/1755 00:07&lt;02:47 5.2353]
    </div>
    
&lt;/div&gt;

&lt;/div&gt;

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
</pre>
</div>
</div>

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learner</span><span class="o">.</span><span class="n">recorder</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">skip_end</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">suggestion</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Min numerical gradient: 1.91E-04
Min loss divided by 10: 1.74E-04
</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># learner.fit_one_cycle(1,max_lr=2e-03,moms=(0.8,0.7))</span>
<span class="n">learner</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">max_lr</span><span class="o">=</span><span class="nb">slice</span><span class="p">(</span><span class="mf">1e-05</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">))</span>
<span class="n">learner</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;first_cycle&#39;</span><span class="p">)</span>

<span class="n">seed_all</span><span class="p">(</span><span class="n">seed_val</span><span class="p">)</span>
<span class="n">learner</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;first_cycle&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.199539</td>
      <td>1.233606</td>
      <td>0.510574</td>
      <td>0.489426</td>
      <td>01:02</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Learner(data=TextClasDataBunch;

Train: LabelList (112364 items)
x: TextList
&lt;s&gt; Ġunintentionally Ġ- RR B - &lt;/s&gt;,&lt;s&gt; Ġwill Ġneed Ġall Ġthe Ġluck Ġthey Ġcan Ġmuster Ġjust Ġfiguring Ġout Ġwho Ġ&#39; s Ġwho Ġin Ġthis Ġpret entious Ġmess &lt;/s&gt;,&lt;s&gt; Ġsomewhere Ġbetween ĠS ling ĠBlade Ġand ĠSouth Ġof ĠHeaven Ġ, ĠWest Ġof ĠHell &lt;/s&gt;,&lt;s&gt; Ġreminds Ġat Ġevery Ġturn Ġof ĠElizabeth ĠBerk ley Ġ&#39; s Ġflo pping Ġdolphin - g asm &lt;/s&gt;,&lt;s&gt; Ġthe ĠMag i &lt;/s&gt;
y: CategoryList
2,0,2,1,2
Path: .;

Valid: LabelList (12484 items)
x: TextList
&lt;s&gt; Ġk idd ie Ġentertainment &lt;/s&gt;,&lt;s&gt; Ġfinally Ġmakes ĠSex ĠWith ĠStr angers Ġ, Ġwhich Ġopens Ġtoday Ġin Ġthe ĠNew ĠYork Ġmetropolitan Ġarea Ġ, Ġso Ġdist ast eful &lt;/s&gt;,&lt;s&gt; Ġmuch Ġas Ġthey Ġlove Ġthemselves &lt;/s&gt;,&lt;s&gt; Ġupdate Ġher Ġbeloved Ġgenre &lt;/s&gt;,&lt;s&gt; Ġbeautiful Ġwomen &lt;/s&gt;
y: CategoryList
2,0,2,2,2
Path: .;

Test: None, model=CustomTransformerModel(
  (transformer): RobertaForSequenceClassification(
    (roberta): RobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(50265, 768, padding_idx=1)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (classifier): RobertaClassificationHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): Linear(in_features=768, out_features=5, bias=True)
    )
  )
), opt_func=functools.partial(&lt;class &#39;transformers.optimization.AdamW&#39;&gt;, correct_bias=False), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[&lt;function accuracy at 0x7f8dfd183158&gt;, &lt;function error_rate at 0x7f8dfd183378&gt;], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath(&#39;.&#39;), model_dir=&#39;models&#39;, callback_fns=[functools.partial(&lt;class &#39;fastai.basic_train.Recorder&#39;&gt;, add_time=True, silent=False)], callbacks=[], layer_groups=[Sequential(
  (0): Embedding(50265, 768, padding_idx=1)
  (1): Embedding(514, 768, padding_idx=1)
  (2): Embedding(1, 768)
  (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (4): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Tanh()
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=5, bias=True)
)], add_time=True, silent=False)</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learner</span><span class="o">.</span><span class="n">lr_find</span><span class="p">()</span>
<span class="n">learner</span><span class="o">.</span><span class="n">recorder</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">skip_end</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">suggestion</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
                background: #F44336;
            }
        </style>
      &lt;progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'&gt;&lt;/progress&gt;
      0.00% [0/1 00:00&lt;00:00]
    </div>
    
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
  </tbody>
</table><p>

    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
                background: #F44336;
            }
        </style>
      &lt;progress value='77' class='' max='1755', style='width:300px; height:20px; vertical-align: middle;'&gt;&lt;/progress&gt;
      4.39% [77/1755 00:07&lt;02:50 3.9110]
    </div>
    
&lt;/div&gt;

&lt;/div&gt;

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
Min numerical gradient: 9.12E-07
Min loss divided by 10: 1.32E-03
</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learner</span><span class="o">.</span><span class="n">freeze_to</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># lr = 1e-5</span>
<span class="n">learner</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_lr</span><span class="o">=</span><span class="nb">slice</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="mf">1e-04</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.116296</td>
      <td>1.071130</td>
      <td>0.568728</td>
      <td>0.431272</td>
      <td>01:09</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learner</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ShowGraph</span><span class="p">(</span><span class="n">learner</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>CustomTransformerModel(
  (transformer): RobertaForSequenceClassification(
    (roberta): RobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(50265, 768, padding_idx=1)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (classifier): RobertaClassificationHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): Linear(in_features=768, out_features=5, bias=True)
    )
  )
)
</pre>
</div>
</div>

</div>
</div>

</div>
    

&lt;/div&gt;
 

</p></div></div></div></div></div></p></div></div></div></div></div></div>


  </div><a class="u-url" href="/tech_blog/2020/09/09/lr_test.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/tech_blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/tech_blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/tech_blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/tech_blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/tech_blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
