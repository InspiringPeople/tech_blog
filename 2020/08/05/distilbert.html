<h1 id="distilbert">distilbert</h1>

<p>a distilled version of BERT: smaller, faster, cheaper and lighter</p>

<p>Hugging face, 2019.10</p>

<p>ë…¼ë¬¸ : <a href="https://arxiv.org/abs/1910.01108">https://arxiv.org/abs/1910.01108</a></p>

<p>github : <a href="https://github.com/huggingface/transformers/tree/master/examples/distillation">https://github.com/huggingface/transformers/tree/master/examples/distillation</a></p>

<p>í•´ë‹¹ githubì—ì„œ ì œì•ˆ ë°©ë²•ìœ¼ë¡œ ìƒì„±í•œ ë‹¤ë¥¸ distil* ëª¨ë¸ ì†Œê°œ (DistilGPT2, DistilRoBERTa, DistilmBERT)</p>

<p>abstract</p>

<ul>
  <li>ìµœê·¼ pretrain -&gt; fine tuningìœ¼ë¡œ ê°€ëŠ” ë°©ë²•ì´ ë§ì•„ì§€ê³  í”í•´ì¡Œì§€ë§Œ, ëª¨ë¸ ìì²´ê°€ ë„ˆë¬´ í¬ê¸° ë•Œë¬¸ì— ì œí•œëœ í™˜ê²½ì—ì„œëŠ” êµ‰ì¥íˆ ì‚¬ìš©í•˜ê¸° í˜ë“¬</li>
  <li>huggingfaceì—ì„œ DistilBertë¼ëŠ” general purpose language representation model ì œì•ˆ</li>
  <li>BERTë¥¼ 40% ì •ë„ ì¤„ì´ê³  60%ë‚˜ ë¹ ë¥´ê²Œ ì—°ì‚°í•˜ë©´ì„œ 97%ì˜ ì„±ëŠ¥ì„ ìœ ì§€</li>
</ul>

<p>distilation (knowledge extraction)</p>

<p>Knowledge Distillation [Bucila et al., 2006, Hinton et al., 2015]ì€ larger model(teacher model)ë¡œë¶€í„° compact model(student model)ì„ ë§Œë“¤ì–´ë‚´ëŠ” ë°©ë²•ì´ë‹¤. ì´ê²Œ ì‘ì€ ëª¨ë¸ì„ ë°”ë¡œ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒë³´ë‹¤ ì˜ë¯¸ìˆëŠ” ì´ìœ ëŠ” near-zeroì¸ í™•ë¥ ë“¤ë„ í•™ìŠµí•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤.</p>

<p>ëª¨ë¸ì´ ë§Œë“¤ì–´ë‚¸ ê²°ê³¼ ë¶„í¬ê°€ ì‹¤ì œ ì´ë¯¸ì§€ë¥¼ ë” í’ë¶€í•˜ê²Œ í‘œí˜„í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì¦‰, ì›ë˜ ì •ë‹µ ê´€ì ì—ì„œëŠ” ì •ë‹µ ì´ì™¸ì— ëŒ€í•œ ì •ë³´ê°€ ì—†ì§€ë§Œ, í•œë²ˆ ëª¨ë¸ì—ì„œ í’€ì–´ë‚˜ì˜¨ ê²°ê³¼ëŠ” ì •ë‹µ ì™¸ì—ë„ ë‹¤ë¥¸ ë¬¼ì²´ì— ëŒ€í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆê²Œ ë©ë‹ˆë‹¤. ì´ë ‡ê²Œ ì •ë³´ê°€ ë¬»ì–´ ë‚˜ì˜¤ëŠ” ê²ƒì´ ë§ˆì¹˜ ì„ìœ ì˜ ë¶€ì‚°ë¬¼ë“¤ì´ ì¦ë¥˜íƒ‘ì—ì„œ ë‚˜ì˜¤ëŠ” ì–‘ìƒê³¼ ê°™ê¸° ë•Œë¬¸ì— ì´ë¥¼ ì§€ì‹ ì¦ë¥˜ë¼ê³  ì¼ì»«ìŠµë‹ˆë‹¤. ì›ë˜ ëª¨ë¸ì´ ìƒê°í•˜ëŠ” ë°ì´í„°ì˜ ì •ë³´ê°€ í’€ì–´ë‚˜ì˜¨ ë°ì´í„°ë¡œ ìƒˆë¡œ í•™ìŠµì‹œí‚¤ê²Œ ë˜ë©´ ê°„ì ‘ì ìœ¼ë¡œ ì„ ìƒ ëª¨ë¸ì´ í•™ìŠµí•œ ë°”ë¥¼ ë°˜ì˜í•˜ê²Œ ë˜ë¯€ë¡œ ë” íš¨ìœ¨ì ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆìœ¼ë©°, ìƒˆë¡œ ë°°ìš°ëŠ” ëª¨ë¸(í•™ìƒ ëª¨ë¸)ì€ ìƒëŒ€ì ìœ¼ë¡œ ë” ì ì€ ê·œëª¨ë¡œ êµ¬ì„±ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì´ Hinton et al. ì´ ì œì‹œí•œ ì§€ì‹ ì¦ë¥˜ì˜ í•µì‹¬ì…ë‹ˆë‹¤.</p>

<p><img src="distilbert%204df0326400104a55af9ef2076f01cb42/Untitled.png" alt="distilbert%204df0326400104a55af9ef2076f01cb42/Untitled.png" /></p>

<p>distilbert</p>

<ul>
  <li>architecture
    <ul>
      <li>token-type embedding and the pooler are removed</li>
      <li>layers is reduced by a factor of 2
        <ul>
          <li>variations on the last dimension of the tensor(hidden size dimension) have a smaller impact on computation efficiency)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>initialization
    <ul>
      <li>important to find the right initialization for the sub-network to converge</li>
      <li>initialize the student from teacher by taking one layer out of two</li>
    </ul>
  </li>
  <li>distilation
    <ul>
      <li>large batches (up to 4K)</li>
      <li>dynamic masking without NSP</li>
    </ul>
  </li>
  <li>data and computer power
    <ul>
      <li>same data with BERT</li>
      <li>on 8 16GB V100 GPUs for approximately 90 hours</li>
    </ul>
  </li>
  <li>downstream benchmark
    <ul>
      <li>97% of performance with 40% fewer parameter comparing with BERT</li>
    </ul>
  </li>
</ul>

<p><img src="distilbert%204df0326400104a55af9ef2076f01cb42/Untitled%201.png" alt="distilbert%204df0326400104a55af9ef2076f01cb42/Untitled%201.png" /></p>

<ul>
  <li>distilation ë°©ë²• (github)
    <ul>
      <li>prepare data
        <ul>
          <li>binarize the data, i.e. tokenize the data and convert each token in an index in our modelâ€™s vocabulary</li>
        </ul>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div>        </div>
      </li>
      <li>training</li>
    </ul>
  </li>
</ul>

<p>loss function ë¬´ì—‡?</p>

<ul>
  <li>
    <p>Loss function</p>

    <p><img src="distilbert%204df0326400104a55af9ef2076f01cb42/Untitled%202.png" alt="distilbert%204df0326400104a55af9ef2076f01cb42/Untitled%202.png" /></p>

    <ul>
      <li>Teacher ëª¨ë¸ì—ì„œ ë‚˜ì˜¨ class probabilitiesë¥¼ ë°”ë¡œ Student modelì˜ target (soft target) ì´ìš©, knowledgeë¥¼ íš¨ê³¼ì ìœ¼ë¡œ transferí•˜ëŠ” ë°©ì‹</li>
      <li>MNISTë¥¼ ì˜ˆë¥¼ ë“¤ì–´ 2ë¥¼ ë§ì¶œ ë•Œ ë‹®ì€ ìˆ«ìì¸ 3ê³¼ 7ë„ ë‚®ì€ í™•ë¥ ì´ì§€ë§Œ ê°’ì„ ë¶€ì—¬, ì´ëŸ¬í•œ ì •ë³´ë“¤ì€ dataì˜ structureì— ëŒ€í•œ ì •ë³´ê°€ ë“¤ì–´ìˆëŠ” ê°’ì´ê¸° ë•Œë¬¸ì— ì¤‘ìš” ì •ë³´</li>
      <li>êµ‰ì¥íˆ ë‚®ì€ ê°’ìœ¼ë¡œ í‘œí˜„ë˜ê¸° ë•Œë¬¸ì— temperature ê°œë…ì„ ë„ì…</li>
    </ul>
  </li>
  <li>
    <p>softmax-temperature</p>

    <p><img src="distilbert%204df0326400104a55af9ef2076f01cb42/Untitled%203.png" alt="distilbert%204df0326400104a55af9ef2076f01cb42/Untitled%203.png" /></p>

    <ul>
      <li>TëŠ” Temperatureì´ê³ , T=1ì´ë¼ë©´ ë³´í†µì˜ softmax ì‹</li>
      <li>Tê°€ ì»¤ì§€ë©´ í›¨ì”¬ softí•œ probability distribution</li>
      <li>Tê°€ output distributionì˜ smoothnessë¥¼ ê²°ì •. training ë™ì•ˆì—ë§ŒÂ Të¥¼ ì¡°ì •í•˜ê³  inference ì‹œê°„ì—ëŠ” 1ë¡œ ì„¤ì •í•´ì„œ standard softmaxë¡œ ì‚¬ìš©</li>
    </ul>
  </li>
  <li>final training lossëŠ” distillation lossÂ Lceì™€ BERTì—ì„œ ì‚¬ìš©í•œÂ Lmlmì˜ linear combination</li>
  <li>
    <p>Softmax(ì†Œí”„íŠ¸ë§¥ìŠ¤)</p>

    <p>softmaxëŠ” ì…ë ¥ë°›ì€ ê°’ì„ ì¶œë ¥ìœ¼ë¡œ 0~1ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ëª¨ë‘ ì •ê·œí™”í•˜ë©° ì¶œë ¥ ê°’ë“¤ì˜ ì´í•©ì€ í•­ìƒ 1ì´ ë˜ëŠ” íŠ¹ì„±ì„ ê°€ì§„ í•¨ìˆ˜</p>
  </li>
</ul>

<p>distil*</p>

<ul>
  <li>distilbertëŠ” bert-base-uncased tokenizerì™€ ê°™ì€ tokenizer</li>
  <li>teacher modelê³¼ ë¹„ìŠ·í•˜ê²Œ ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ì‹ (bert, gpt2, roberta, betlm..)</li>
  <li>DistilBERT uncased:Â <code class="highlighter-rouge"><span class="k">model</span> <span class="p">=</span> <span class="n">DistilBertModel</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilbert-base-uncased'</span><span class="p">)</span></code></li>
  <li>DistilGPT2:Â <code class="highlighter-rouge"><span class="k">model</span> <span class="p">=</span> <span class="n">GPT2Model</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilgpt2'</span><span class="p">)</span></code></li>
  <li>DistilRoBERTa:Â <code class="highlighter-rouge"><span class="k">model</span> <span class="p">=</span> <span class="n">RobertaModel</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilroberta-base'</span><span class="p">)</span></code></li>
  <li>DistilmBERT:Â <code class="highlighter-rouge"><span class="k">model</span> <span class="p">=</span> <span class="n">DistilBertModel</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilbert-base-multilingual-cased'</span><span class="p">)</span></code></li>
</ul>

<p>ì°¸ê³ </p>

<p><a href="https://blog.pingpong.us/transformer-review/#distilbert-knowledge-distillation">Transformer - Harder, Better, Faster, Stronger</a></p>

<p><a href="https://medium.com/huggingface/distilbert-8cf3380435b5">ğŸ Smaller, faster, cheaper, lighter: Introducing DilBERT, a distilled version of BERT</a></p>

<p><a href="https://arxiv.org/abs/1503.02531">Distilling the Knowledge in a Neural Network</a></p>

<p><a href="https://jeongukjae.github.io/posts/distilling-the-knowledge-in-a-neural-network/">ğŸ“ƒ Distilling the Knowledge in a Neural Network ë¦¬ë·°</a></p>
